{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9bd9eb-ccd3-43b3-aa5c-cb4f846e909d",
   "metadata": {},
   "source": [
    "# Advanced transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef2dd9-c3ce-4982-92cd-514aed69273d",
   "metadata": {},
   "source": [
    "## How to Transform Numerical and Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f65bf6-9f1b-4b6f-b674-5cae71a14a4c",
   "metadata": {},
   "source": [
    "üîß **Why Data Transformation Is Needed**\n",
    "- Essential for preparing raw data before model training.\n",
    "- Ensures machine learning algorithms can understand and learn from the data structure.\n",
    "- Different types of data (numerical vs. categorical) require different preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed919b17-3556-43ad-ace7-ec481cd29552",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Challenges with Mixed Data Types**\n",
    "- Mixed-type datasets need tailored preprocessing **per column type**.\n",
    "- Manual separation and transformation of columns is inefficient and error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8f0ba-3d19-40a9-96ec-b07913e3a818",
   "metadata": {},
   "source": [
    "üß∞ **Solution: ColumnTransformer (scikit-learn)**\n",
    "- Enables selective transformation of specific columns.\n",
    "- Automatically applies different transforms to numerical vs. categorical data.\n",
    "- Avoids manual column management and recombination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acada5c-8e1c-4ded-9684-7bb6c5702ced",
   "metadata": {},
   "source": [
    "üî® **How to Use ColumnTransformer**\n",
    "- Define a list of transformers: **Each as a tuple: (\"name\", transform_object, columns)**\n",
    "- **Example usage**:<br>\n",
    "o\tApply **OneHotEncoder** to **categorical columns** (e.g., columns 0 & 1).<br>\n",
    "o\tApply **SimpleImputer(median)** to **numerica**l columns.<br>\n",
    "- **Remainder options**:<br>\n",
    "o\t**drop** (default): drops unspecified columns.<br>\n",
    "o\t**passthrough**: passes through untouched columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a98e05-7665-4c13-822a-92ef8fd5f533",
   "metadata": {},
   "source": [
    "üîÅ **Integrating ColumnTransformer in a Pipeline**\n",
    "- Combine ColumnTransformer with model in a Pipeline.\n",
    "- Ensures consistent preprocessing for:<br>\n",
    "o\t**Training**<br>\n",
    "o\t**Cross-validation**<br>\n",
    "o\t**Future predictions**<br>\n",
    "- Enables automation and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05690e7f-690c-47f7-9768-37604fe94ced",
   "metadata": {},
   "source": [
    "üß† **Key Takeaways for Practice**\n",
    "- Mixed-type preprocessing is streamlined with **ColumnTransformer**.\n",
    "- Always evaluate with cross-validation for reliable metrics.\n",
    "- Modular and reusable pipelines increase efficiency and model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0873c6-d671-43c2-8863-944d8397c392",
   "metadata": {},
   "source": [
    "#### Data Preparation for the Abalone Regression Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17966147-33a3-4e84-ba4b-754e4c846cbb",
   "metadata": {},
   "source": [
    "The abalone dataset is a standard machine learning problem that involves predicting the age of an abalone given measurements of an abalone. The dataset has 4,177 examples, 8 input variables, and the target variable is an integer. A naive model can achieve a mean absolute error (MAE) of about 2.363 (std 0.092) by predicting the mean value, evaluated via 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcbb24-1777-4db5-a85c-d1c50e91cc01",
   "metadata": {},
   "source": [
    "We can see that the first column is categorical and the remainder of the columns are numerical. We may want to one hot encode the first column and normalize the remaining numerical columns, and this can be achieved using the ColumnTransformer. We can model this as a regression predictive modeling problem with a support vector machine model (SVR). First, we need to load the dataset. We can load the dataset directly from the file using the read csv() Pandas function, then split the data into two data frames: one for input and one for the output. The complete example of loading the dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566a677d-2668-4c6e-b8de-6cbf7dcbc154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 8) (4177,)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "from pandas import read_csv \n",
    "\n",
    "# load dataset\n",
    "path_abalone_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv  \"\n",
    "dataframe = read_csv(path_abalone_data, header=None) \n",
    "\n",
    "# split into inputs and outputs\n",
    "last_ix = len(dataframe.columns) - 1\n",
    "X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix] \n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6cbced-9fdc-4285-adc2-dc6df4be3677",
   "metadata": {},
   "source": [
    "Next, we can use the **select dtypes()** function to select the column indexes that match different data types. <br>We are interested in a list of columns that are numerical columns marked as float64 or int64 in Pandas, and a list of categorical columns, marked as object or bool type in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483dba4d-fb11-415b-b655-aa2e693e28f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 8) (4177,)\n",
      "MAE: 1.465 (0.047)\n"
     ]
    }
   ],
   "source": [
    "# example of using the ColumnTransformer for the Abalone dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# load dataset\n",
    "path_abalone_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv  \"\n",
    "dataframe = read_csv(path_abalone_data, header=None) \n",
    "\n",
    "# split into inputs and outputs\n",
    "last_ix = len(dataframe.columns) - 1\n",
    "X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix] \n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# determine categorical and numerical features\n",
    "numerical_ix = X.select_dtypes(include=['int64', 'float64']).columns \n",
    "categorical_ix = X.select_dtypes(include=['object', 'bool']).columns \n",
    "\n",
    "# define the data preparation for the columns\n",
    "t = [('cat', OneHotEncoder(), categorical_ix), ('num', MinMaxScaler(), numerical_ix)] \n",
    "col_transform = ColumnTransformer(transformers=t)\n",
    "\n",
    "# define the model\n",
    "model  =  SVR(kernel='rbf',gamma='scale',C=100)\n",
    "\n",
    "# define the data preparation and modeling pipeline\n",
    "pipeline = Pipeline(steps=[('prep',col_transform), ('m', model)]) \n",
    "\n",
    "# define the model cross-validation configuration\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# evaluate the pipeline using cross validation and calculate MAE\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# convert MAE scores to positive values \n",
    "scores = absolute(scores)\n",
    "\n",
    "# summarize the model performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b34f2-f27a-4901-be73-9d3fc7f7139a",
   "metadata": {},
   "source": [
    "You now have a template for using the ColumnTransformer on a dataset with mixed data types that you can use and adapt for your own projects in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8d9ba-6d40-4e42-b90d-53aecbf3ba3b",
   "metadata": {},
   "source": [
    "## How to Transform the Target in Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc021a-51a7-46b6-9d7f-3e24906df93c",
   "metadata": {},
   "source": [
    "üìå **Why Target Transformation Matters**\n",
    "-  Critical for **boosting model performance** in regression tasks.\n",
    "- Transforms like **scaling** improve models sensitive to value range (e.g., linear models, SVMs).\n",
    "- Often overlooked: **Target variable** (output) **needs scaling too**‚Äînot just input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e7313-7e59-4c83-a6fd-3504542561c9",
   "metadata": {},
   "source": [
    "üìè **Importance of Data Scaling**\n",
    "- Helps algorithms that rely on:<br>\n",
    "o\tWeighted sums (Linear models, Neural Nets)<br>\n",
    "o\tDistance measures (SVM, KNN)<br>\n",
    "- Common techniques:<br>\n",
    "o\t**MinMaxScaler** ‚Üí Normalize values to range [0,1]<br>\n",
    "o\t**PowerTransformer** ‚Üí Make data more Gaussian-like<br>\n",
    "- Improves both input and target representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29cdba-a44d-4453-97e2-c07f595743cc",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **Two Ways to Scale Target Variables**<br><br>\n",
    "1Ô∏è‚É£ Manual Scaling\n",
    "- Requires multiple steps:\n",
    "1.\tCreate and fit scaler (e.g., MinMaxScaler).\n",
    "2.\tTransform training and test target values.\n",
    "3.\tInvert transforms for predictions.\n",
    "- ‚ö†Ô∏è **Downside**:<br>\n",
    "o\tTedious<br>\n",
    "o\tDoesn‚Äôt integrate well with cross_val_score() or pipelines.<br><br>\n",
    "\n",
    "2Ô∏è‚É£ Automatic Scaling with **TransformedTargetRegressor**\n",
    "- Scikit-learn wrapper that:<br>\n",
    "o\tAutomatically transforms and inverse-transforms target variable.<br>\n",
    "o\tIntegrates with models and pipelines.<br>\n",
    "- Defined by:\n",
    "- **TransformedTargetRegressor(regressor=model, transformer=scaler)**\n",
    "- ‚úÖ Allows use of model evaluation functions like **cross_val_score()**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17584904-9daf-4883-9303-10c18f0a9785",
   "metadata": {},
   "source": [
    "üß™ **Real Example: Boston Housing Dataset**<br><br>\n",
    "üìä **Dataset Overview**<br>\n",
    "- Predict house prices based on 13 suburb features.\n",
    "- Regression task with 506 samples.<br><br>\n",
    "üõ†Ô∏è **Approach**<br>\n",
    "1.\tNormalize inputs using a Pipeline.\n",
    "2.\tWrap pipeline in a TransformedTargetRegressor to scale the target.\n",
    "3.\tEvaluate using repeated 10-fold cross-validation.<br><br>\n",
    "üìâ **Results**<br>\n",
    "- Naive model MAE: ~6.6\n",
    "- With scaling: ~3.2 MAE\n",
    "- With PowerTransformer (on input & target): ~2.9 MAE ‚Üí best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5784e3b-3af0-4da4-9346-2accbe37d822",
   "metadata": {},
   "source": [
    "üîÅ **PowerTransformer for Advanced Scaling**\n",
    "- Applies **Yeo-Johnson transform** ‚Üí Makes distribution more Gaussian-like.\n",
    "- Often used with **MinMaxScaler** to:<br>\n",
    "o\tEnsure positive values<br>\n",
    "o\tImprove inverse transform stability<br>\n",
    "- Ideal for improving **linear model performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a320002-0be6-436c-b795-849ff367b9b0",
   "metadata": {},
   "source": [
    "üß† **Key Takeaways**\n",
    "- Always **consider transforming target variables** in regression tasks.\n",
    "- Use TransformedTargetRegressor for:<br>\n",
    "o\t**Cleaner code**<br>\n",
    "o\t**Integration with cross-validation**<br>\n",
    "o\t**Avoiding manual transform/inverse headaches**<br>\n",
    "- Experiment with different transforms (MinMaxScaler, PowerTransformer) to boost accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b502f61-948a-493c-a485-0ad610db67b1",
   "metadata": {},
   "source": [
    "#### Example of Using the TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4365be-3c2d-441d-8ab3-6294b358de37",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the TransformedTargetRegressor on a real dataset. We will use the Boston housing regression problem that has 13 inputs and one numerical target and requires learning the relationship between suburb characteristics and house prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2ea4bc-bbe8-4a9b-bee9-2d66166cb20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset \n",
    "from numpy import loadtxt\n",
    "\n",
    "# load data\n",
    "boston_housing = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv \"\n",
    "dataset = loadtxt(boston_housing, delimiter=\",\") \n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = dataset[:, :-1], dataset[:, -1]\n",
    "\n",
    "# summarize dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068a87d0-e54b-4b9e-bb07-b00eac526352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 3.203\n"
     ]
    }
   ],
   "source": [
    "# example of normalizing input and output variables for regression. \n",
    "from numpy import mean\n",
    "from numpy import absolute \n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor \n",
    "\n",
    "# load data\n",
    "boston_housing = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv \"\n",
    "dataset = loadtxt(boston_housing, delimiter=\",\") \n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = dataset[:, :-1], dataset[:, -1] \n",
    "\n",
    "# prepare the model with input scaling\n",
    "pipeline = Pipeline(steps=[('normalize', MinMaxScaler()), ('model', HuberRegressor())]) \n",
    "\n",
    "# prepare the model with target scaling\n",
    "model  =  TransformedTargetRegressor(regressor=pipeline,  transformer=MinMaxScaler())\n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1) \n",
    "\n",
    "# convert scores to positive\n",
    "scores = absolute(scores) \n",
    "\n",
    "# summarize the result \n",
    "s_mean = mean(scores)\n",
    "print('Mean MAE: %.3f' % (s_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e11ca-e04e-4d1d-8e5c-9e4a9fef3834",
   "metadata": {},
   "source": [
    "Running the example evaluates the model with normalization of the input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9cc7e-86b1-493d-92b0-96bc387d97d9",
   "metadata": {},
   "source": [
    "In this case, we achieve a MAE of about 3.2, much better than a naive model that achieved about 6.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b1f36-4c83-46bf-a45f-d9c9d375a61f",
   "metadata": {},
   "source": [
    "We are not restricted to using scaling objects; for example, we can also explore using other data transforms on the target variable, such as the PowerTransformer, that can make each variable more-Gaussian-like (using the Yeo-Johnson transform) and improve the performance of linear models (introduced in Chapter 20). By default, the PowerTransformer also performs a standardization of each variable after performing the transform. It can also help to scale the values to the range 0-1 prior to applying a power transform, to avoid problems inverting the transform. We achieve this using the MinMaxScaler and defining a positive range (introduced in Chapter 17). The complete example of using a PowerTransformer on the input and target variables of the housing dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35a5141-df0e-4f56-ab3a-bea5558b7963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 2.972\n"
     ]
    }
   ],
   "source": [
    "#example of power transform input and output variables for regression. \n",
    "from numpy import mean\n",
    "from numpy import absolute \n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor \n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor \n",
    "\n",
    "# load data\n",
    "boston_housing = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv \"\n",
    "dataset = loadtxt(boston_housing, delimiter=\",\") \n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = dataset[:, :-1], dataset[:, -1]\n",
    "\n",
    "# prepare the model with input scaling and power transform \n",
    "steps = list()\n",
    "steps.append(('scale', MinMaxScaler(feature_range=(1e-5,1)))) \n",
    "steps.append(('power', PowerTransformer())) \n",
    "steps.append(('model', HuberRegressor()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# prepare the model with target scaling\n",
    "model  =  TransformedTargetRegressor(regressor=pipeline,  transformer=PowerTransformer()) \n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1) \n",
    "\n",
    "# convert scores to positive\n",
    "scores = absolute(scores) \n",
    "\n",
    "# summarize the result \n",
    "s_mean = mean(scores)\n",
    "print('Mean MAE: %.3f' % (s_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea93be-ace5-4278-987a-43aad4dcb314",
   "metadata": {},
   "source": [
    "Running the example evaluates the model with a power transform of the input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3342581-79e4-40ee-8569-bba2835771e0",
   "metadata": {},
   "source": [
    "In this case, we see further improvement to a MAE of about 2.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c6a47-7b98-437e-8ef0-291af0e4539d",
   "metadata": {},
   "source": [
    "## How to Save and Load Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76553ba-e63c-45c6-8428-e4f95c9214f8",
   "metadata": {},
   "source": [
    "üöß **Why It Matters**\n",
    "- ‚úÖ **Consistency is key**: Any transform applied during training must also be applied to **test or future data**.\n",
    "- ‚ùå Inconsistent data preparation ‚Üí **invalid predictions or degraded model performance**.\n",
    "- ‚úÖ Essential for **model deployment** and **real-world inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b95505-1332-4d32-a105-c565baca059a",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Challenge: Preparing New Data**\n",
    "- Input variables can vary in scale or units (e.g., inches, days, miles).<br><br>\n",
    "- **Models sensitive to input scale**:<br>\n",
    "o\tLogistic Regression<br>\n",
    "o\tNeural Networks<br>\n",
    "o\tKNN<br><br>\n",
    "- **Scaling ensures**:<br>\n",
    "o\tEqual contribution of all features.<br>\n",
    "o\tImproved model stability and convergence.<br><br>\n",
    "- üéØ Scaling must be:<br>\n",
    "o\tFit on **training data**<br>\n",
    "o\tApplied to **both train and test/new data**<br><br>\n",
    "- ‚úÖ Easy in-memory, ‚ùå Tricky when model is **saved for later use**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24b75b-fee5-4558-bd82-931e3fe45f4b",
   "metadata": {},
   "source": [
    "üí° **Solution: Save Data Preparation Objects**\n",
    "- Save both: <br>\n",
    "o\tüß† Trained model<br>\n",
    "o\t‚öôÔ∏è Data preparation object (e.g., MinMaxScaler, StandardScaler)<br><br>\n",
    "- üîß Use pickle (Python built-in) for serialization.\n",
    "- Two options:\n",
    "1.\tSave **entire objects** (easiest, most common).\n",
    "2.\tSave **parameters only** (for more control, expert use)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7ca86-8cd9-4239-b4b5-57e317801b92",
   "metadata": {},
   "source": [
    "üß™ **Worked Example: Step-by-Step**<br><br>\n",
    "üß∞ **1. Define and Split Dataset**\n",
    "- Use make_blobs() for a simple binary classification example.\n",
    "- Train/test split with fixed random state.\n",
    "- Observed:<br>\n",
    "o\tVariables have **different scales** in train vs. test.<br>\n",
    "    \n",
    "üìè **2. Scale Dataset**\n",
    "- Apply **MinMaxScaler**:<br>\n",
    "o\tFit on training set<br>\n",
    "o\tTransform both train and test sets<br>\n",
    "- Result: All features in [0,1] range for both sets.\n",
    "                                      \n",
    "üíΩ **3. Save Model & Scaler**\n",
    "- Train **LogisticRegression** on scaled data.\n",
    "- Save both:<br>\n",
    "o\tmodel.pkl (**trained model**)<br>\n",
    "o\tscaler.pkl (**fitted scaler**)<br>\n",
    "                                      \n",
    "üìÇ **4. Load and Use Later**\n",
    "- Load model and scaler from disk.\n",
    "- Apply saved scaler to **new/test data**.\n",
    "- Use model to make predictions.\n",
    "- ‚úÖ Scaler ensures data is in the same format as during training.\n",
    "- ‚úÖ 100% accuracy achieved on this trivial test case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed0a11-85f6-4cf5-b6f1-f3e77c276c8d",
   "metadata": {},
   "source": [
    "üß† **Key Takeaways**\n",
    "- Always **save the exact transform** used during training.\n",
    "- Use tools like **pickle** to:<br>\n",
    "o\tSerialize models and scalers.<br>\n",
    "o\tLoad them later for consistent preprocessing and inference.<br>\n",
    "- üîÅ Reuse this workflow in real-world machine learning projects to ensure **reliable and repeatable results**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98cafa-b384-4f08-8da7-ae6818ec7bce",
   "metadata": {},
   "source": [
    "### Define a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed648b9-1b4f-4efa-b085-c443f97d21fe",
   "metadata": {},
   "source": [
    "First, we need a dataset. We will use a synthetic dataset, specifically a binary classification problem with two input variables created randomly via the make blobs() function. The example below creates a test dataset with 100 examples, two input features, and two class labels (0 and 1). The dataset is then split into training and test sets and the min and max values of each variable are then reported.\n",
    "Importantly, the random state is set when creating the dataset and when splitting the data so that the same dataset is created and the same split of data is performed each time that the code is run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30707ca6-7ac5-48be-8ee4-4b276afa2303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0, train: min=-11.856, max=0.526, test: min=-11.270, max=0.085\n",
      ">1, train: min=-6.388, max=6.507, test: min=-5.581, max=5.926\n"
     ]
    }
   ],
   "source": [
    "# example of creating a test dataset and splitting it into train and test sets \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# summarize the scale of each input variable\n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' % (i, X_train[:, i].min(), X_train[:, i].max(),\n",
    "    X_test[:, i].min(), X_test[:, i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e5906-75f6-4707-a739-ffeae8ca34df",
   "metadata": {},
   "source": [
    "Running the example reports the min and max values for each variable in both the train and test datasets. We can see that each variable has a different scale, and that the scales differ between the train and test datasets. This is a realistic scenario that we may encounter with a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16320ee-a58d-4c3f-a592-497146972d3e",
   "metadata": {},
   "source": [
    "### Scale the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e264e5-630d-4ced-925e-ab78d0829dd0",
   "metadata": {},
   "source": [
    "Next, we can scale the dataset. We will use the MinMaxScaler to scale each input variable to the range 0-1 (introduced in Chapter 17). The best practice use of this scaler is to fit it on the training dataset and then apply the transform to the training dataset, and other datasets: in this case, the test dataset. The complete example of scaling the data and summarizing the effects is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85e7cde-8c2f-4430-b7ef-ec224ed0715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0, train: min=0.000, max=1.000, test: min=0.047, max=0.964\n",
      ">1, train: min=0.000, max=1.000, test: min=0.063, max=0.955\n"
     ]
    }
   ],
   "source": [
    "# example of scaling the dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit scaler on the training dataset \n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform both datasets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# summarize the scale of each input variable \n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' % (i, X_train_scaled[:, i].min(), X_train_scaled[:, i].max(),\n",
    "    X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198dadb-d811-4628-86f2-e567ad3547b1",
   "metadata": {},
   "source": [
    "Running the example prints the effect of the scaled data showing the min and max values for each variable in the train and test datasets. We can see that all variables in both datasets now have values in the desired range of 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2fc3e1-adec-4a1f-80f0-784262cc63d4",
   "metadata": {},
   "source": [
    "### Save Model and Data Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cb382-747a-4a68-9eda-aa8ece4ec417",
   "metadata": {},
   "source": [
    "Next, we can fit a model on the training dataset and save both the model and the scaler object to file. We will use a LogisticRegression model because the problem is a simple binary classification task. The training dataset is scaled as before, and in this case, we will assume the test dataset is currently not available. Once scaled, the dataset is used to fit a logistic regression model. We will use the pickle framework to save the LogisticRegression model to one file, and the MinMaxScaler to another file. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56eeb203-3aaa-469c-819f-b3296776b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of fitting a model on the scaled dataset \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from pickle import dump\n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit scaler on the training dataset \n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform the training dataset \n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "\n",
    "# define model\n",
    "model = LogisticRegression(solver='lbfgs') \n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# save the model\n",
    "model_path = \"../../data/models/model.pkl\"\n",
    "dump(model, open(model_path, 'wb')) \n",
    "\n",
    "# save the scaler\n",
    "scaler_path = \"../../data/scalers/scaler.pkl\"\n",
    "dump(scaler,  open(scaler_path,  'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ef8dc-a149-4c51-bfd3-d52601675490",
   "metadata": {},
   "source": [
    "Running the example scales the data, fits the model, and saves the model and scaler to files using pickle. <br>You should have two files in your current working directory:\n",
    "- The model object: model.pkl\n",
    "- The scaler object: scaler.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f3816-a97f-4936-ae0b-b0e92effd801",
   "metadata": {},
   "source": [
    "### Load Model and Data Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703c43f-af96-4c0e-8c23-ddf3a20614f5",
   "metadata": {},
   "source": [
    "Finally, we can load the model and the scaler object and make use of them. In this case, we will assume that the training dataset is not available, and that only new data or the test dataset is available. We will load the model and the scaler, then use the scaler to prepare the new data and use the model to make predictions. Because it is a test dataset, we have the expected target values, so we will compare the predictions to the expected target values and calculate the accuracy of the model. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce7ab2c-a724-456a-a451-85086b904cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw test set range\n",
      ">0, min=-11.270, max=0.085\n",
      ">1, min=-5.581, max=5.926\n",
      "\n",
      "Scaled test set range\n",
      ">0, min=0.047, max=0.964\n",
      ">1, min=0.063, max=0.955\n",
      "\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# load model and scaler and make predictions on new data\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from pickle import load \n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# load the model\n",
    "model_path = \"../../data/models/model.pkl\"\n",
    "model = load(open(model_path, 'rb'))\n",
    "\n",
    "# load the scaler\n",
    "scaler_path = \"../../data/scalers/scaler.pkl\"\n",
    "scaler = load(open(scaler_path, 'rb'))\n",
    "\n",
    "# check scale of the test set before scaling \n",
    "print('Raw test set range')\n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, min=%.3f, max=%.3f' % (i, X_test[:, i].min(), X_test[:, i].max())) \n",
    "\n",
    "# transform the test dataset\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "print('\\nScaled test set range')\n",
    "for i in range(X_test_scaled.shape[1]):\n",
    "    print('>%d, min=%.3f, max=%.3f' % (i, X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))\n",
    "    \n",
    "# make predictions on the test set \n",
    "yhat = model.predict(X_test_scaled) \n",
    "\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(y_test, yhat) \n",
    "\n",
    "print('\\nTest Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c15c0-ea3f-4471-975e-1daba7bb25a4",
   "metadata": {},
   "source": [
    "Running the example loads the model and scaler, then uses the scaler to prepare the test dataset correctly for the model, meeting the expectations of the model when it was trained. To confirm the scaler is having the desired effect, we report the min and max value for each input feature both before and after applying the scaling. The model then makes a prediction for the examples in the test set and the classification accuracy is calculated. In this case, as expected, the data set correctly normalized the model achieved 100 percent accuracy on the test set because the test problem is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95642509-1c52-4930-9faa-3f05feb67cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
