{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c5afd3-7bad-48a4-97dd-c26654478608",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f8e5bf-6852-40a9-9368-10a1c11bb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "## Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic cleaning\n",
    "## Variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e1e0a-4031-42da-bf67-dfaacab452d1",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f1fab-0668-457e-8af1-aa77241ba29f",
   "metadata": {},
   "source": [
    "### Basic data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a588348-8c2d-4195-914d-42877ef34d72",
   "metadata": {},
   "source": [
    "ðŸ” **Why Data Cleaning Matters**\n",
    "- Essential in every ML project; often skipped at a cost.\n",
    "- Prevents model failure and misleading performance results.\n",
    "- Helps prepare trustworthy, high-quality datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f073b7-3363-4f83-8fd9-54a221bba85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of oil_spill_data : (937, 50)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Shape of iris_data : (150, 5)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "path_oil_spill = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv\"\n",
    "oil_spill_data=pd.read_csv(path_oil_spill,header=None)\n",
    "display(f\"Shape of oil_spill_data : {oil_spill_data.shape}\")\n",
    "\n",
    "# Load Iris Flower dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_data['target'] = iris.target\n",
    "display(f\"Shape of iris_data : {iris_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8aa01a3-ce64-4f06-a21f-184f172ad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "oil_spill_data_copy = oil_spill_data.copy()\n",
    "iris_data_copy = iris_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9b989-8262-41d6-ac6e-72b6262f7742",
   "metadata": {},
   "source": [
    "#### Identify Columns That Contain a Single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0125e699-8acd-497f-8d45-3c7550d1d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summurize the number of unique values in each column\n",
    "oil_spill_data.nunique();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8de9c-389a-4c7a-9e41-791a2dd09767",
   "metadata": {},
   "source": [
    "#### Delete Columns That Contain a Single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb02183-8626-42f8-9b83-bdaa411c038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to drop columns that have a single value\n",
    "def drop_single_value_columns(df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "    Drop columns with a single unique value \n",
    "    Args:\n",
    "        df (DataFrame): Raw data\n",
    "    Returns:\n",
    "        df (DataFrame) with no single value columns\n",
    "    \"\"\" \n",
    "        print(f\"Shape BEFORE dropping single value columns: {df.shape}\") \n",
    "        \n",
    "        # get number of unique values for each column \n",
    "        counts = df.nunique()\n",
    "        \n",
    "        # record columns to delete (i.e columns that only contain a unique value)\n",
    "        to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "    \n",
    "        print(f\"Names of single value columns: {to_del}\") \n",
    "        \n",
    "        # drop useless columns \n",
    "        df.drop(to_del, axis=1, inplace=True) \n",
    "        \n",
    "        print(f\"Shape AFTER dropping single value columns: {df.shape}\") \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c8f26f-5d45-494e-a627-0164d3ee8239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape BEFORE dropping single value columns: (937, 50)\n",
      "Names of single value columns: [22]\n",
      "Shape AFTER dropping single value columns: (937, 49)\n"
     ]
    }
   ],
   "source": [
    "oil_spill_data = drop_single_value_columns(oil_spill_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e458c9-c680-4a6b-80f7-e01ef4c1b957",
   "metadata": {},
   "source": [
    "#### Consider Columns That Have Very Few Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98712724-1246-46e8-8d02-a3938db14a5c",
   "metadata": {},
   "source": [
    "This does not mean that these rows and columns should be deleted, but they require further attention. <br>\n",
    "For example: <br>\n",
    "â€¢\tPerhaps the unique values can be encoded as ordinal values? <br>\n",
    "â€¢\tPerhaps the unique values can be encoded as categorical values? <br>\n",
    "â€¢\tPerhaps compare model skill with each variable removed from the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8df85d-1727-41e4-9079-da0d90498d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve original data\n",
    "oil_spill_data = oil_spill_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a685b1f0-9669-4c1b-a979-f7bedc515fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that print index, number of unique values and percentage of unique values per column in regard to total number of rows per column\n",
    "def show_unique_values_percentage(df : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print index, number of unique values and percentage of unique values per column in regard to total number of rows per column\n",
    "    Args:\n",
    "        - df (DataFrame): Raw data\n",
    "    \"\"\"  \n",
    "    for i in range(df.shape[1]):\n",
    "        num = df.nunique().values[i]\n",
    "        percentage = float(num) / df.shape[0] * 100\n",
    "        print(f\"{i}, {num}, {round(percentage,1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b481ad6-069e-444e-8cf6-0b7c82c930a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 238, 25.4%\n",
      "1, 297, 31.7%\n",
      "2, 927, 98.9%\n",
      "3, 933, 99.6%\n",
      "4, 179, 19.1%\n",
      "5, 375, 40.0%\n",
      "6, 820, 87.5%\n",
      "7, 618, 66.0%\n",
      "8, 561, 59.9%\n",
      "9, 57, 6.1%\n",
      "10, 577, 61.6%\n",
      "11, 59, 6.3%\n",
      "12, 73, 7.8%\n",
      "13, 107, 11.4%\n",
      "14, 53, 5.7%\n",
      "15, 91, 9.7%\n",
      "16, 893, 95.3%\n",
      "17, 810, 86.4%\n",
      "18, 170, 18.1%\n",
      "19, 53, 5.7%\n",
      "20, 68, 7.3%\n",
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "23, 92, 9.8%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "27, 308, 32.9%\n",
      "28, 447, 47.7%\n",
      "29, 392, 41.8%\n",
      "30, 107, 11.4%\n",
      "31, 42, 4.5%\n",
      "32, 4, 0.4%\n",
      "33, 45, 4.8%\n",
      "34, 141, 15.0%\n",
      "35, 110, 11.7%\n",
      "36, 3, 0.3%\n",
      "37, 758, 80.9%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "40, 388, 41.4%\n",
      "41, 220, 23.5%\n",
      "42, 644, 68.7%\n",
      "43, 649, 69.3%\n",
      "44, 499, 53.3%\n",
      "45, 2, 0.2%\n",
      "46, 937, 100.0%\n",
      "47, 169, 18.0%\n",
      "48, 286, 30.5%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "show_unique_values_percentage(oil_spill_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d9649f-932d-4ae5-9733-95d4d4f90693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that print index, number of unique values and percentage of unique values per column in regard to total number of rows per column\n",
    "def show_unique_values_percentage_below_threshold(df : pd.DataFrame,threshold : float):\n",
    "    \"\"\"\n",
    "    Print index, number of unique values (below a threshold) \n",
    "    and percentage of unique values per column in regard to total \n",
    "    number of rows per column\n",
    "    Args:\n",
    "        - df (DataFrame): Raw data\n",
    "        - threshold (float) : percentage (%)\n",
    "    \"\"\"  \n",
    "    for i in range(df.shape[1]):\n",
    "        num = df.nunique().values[i]\n",
    "        percentage = float(num) / df.shape[0] * 100\n",
    "        if percentage < threshold:\n",
    "            print(f\"{i}, {num}, {round(percentage,1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab28cccb-cf5a-41a4-a4fa-1974bda91d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "32, 4, 0.4%\n",
      "36, 3, 0.3%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "45, 2, 0.2%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "show_unique_values_percentage_below_threshold(oil_spill_data, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc60885-5dab-413d-9db1-54a388b8d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get columns where number of unique values is less than X% of the rows of the entire considered dataset\n",
    "def get_columns_with_few_values_under_threshold(df : pd.DataFrame, threshold : float) -> list:\n",
    "    \"\"\"\n",
    "    Get columns where number of unique values is less than X% of the rows of the entire considered dataset\n",
    "    Args:\n",
    "        - df (DataFrame): Raw data\n",
    "        - threshold (float) : percentage (%)\n",
    "    Returns:\n",
    "        df (DataFrame) with no columns whose number of unique values are below threshold (in regard to total number of rows of considered dataset)\n",
    "    \"\"\"    \n",
    "    # get number of unique values for each column \n",
    "    counts = df.nunique() \n",
    "    \n",
    "    # record columns to delete \n",
    "    cols_with_few_values = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < threshold]\n",
    "\n",
    "    return cols_with_few_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5311ad7-9045-4b1b-a5bc-1518f1926699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_columns_with_few_values_under_threshold(oil_spill_data, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5e07e2-af6b-442e-88ed-c68da711d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to drop columns where number of unique values is less than X% of the rows of the entire considered dataset\n",
    "def drop_columns_with_few_values_under_threshold(df : pd.DataFrame, threshold : float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop columns where number of unique values is less than X% of the rows of the entire considered dataset\n",
    "    Args:\n",
    "        - df (DataFrame): Raw data\n",
    "        - threshold (float) : percentage (%)\n",
    "    Returns:\n",
    "        df (DataFrame) with no columns whose number of unique values are below threshold (in regard to total number of rows of considered dataset)\n",
    "    \"\"\" \n",
    "    print(f\"Shape BEFORE dropping few value columns: {df.shape}\")\n",
    "    \n",
    "    # get number of unique values for each column \n",
    "    counts = df.nunique() \n",
    "    \n",
    "    # record columns to delete \n",
    "    to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < threshold]\n",
    "\n",
    "    print(f\"Names of columns with few values: {to_del}\") \n",
    "    \n",
    "    # drop useless columns \n",
    "    df.drop(to_del, axis=1, inplace=True) \n",
    "    \n",
    "    print(f\"Shape AFTER dropping columns with few values: {df.shape}\") \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52eeb024-a749-46aa-b811-2b064dfc5825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape BEFORE dropping few value columns: (937, 50)\n",
      "Names of columns with few values: [21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]\n",
      "Shape AFTER dropping columns with few values: (937, 39)\n"
     ]
    }
   ],
   "source": [
    "oil_spill_data = drop_columns_with_few_values_under_threshold(oil_spill_data, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ce8ba-aebc-4bdd-98ce-8c4113b14be9",
   "metadata": {},
   "source": [
    "#### Remove columns that have a low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d4475e-c469-4252-a39f-3ed231f77e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the effect of the variance thresholds on the number of selected features from numpy import arange \n",
    "def explore_variance_threshold_effects(df : pd.DataFrame, range : np.arange):\n",
    "    \"\"\"\n",
    "    Explore the effect of the variance thresholds on the number of selected features from numpy import arange \n",
    "    Args:\n",
    "        - df (DataFrame): Raw data\n",
    "        - range (np.arange) : variance range to check\n",
    "    Returns:\n",
    "        - Print number of features whose variance is over the considered threshold\n",
    "        - Plot the number of features whose variance is over the considered threshold\n",
    "    \"\"\" \n",
    "    \n",
    "    # apply transform with each threshold \n",
    "    results = list() \n",
    "\n",
    "    for t in range: \n",
    "        # define the transform \n",
    "        transform = VarianceThreshold(threshold=t) \n",
    "        \n",
    "        # transform the input data \n",
    "        df_sel = transform.fit_transform(df) \n",
    "        \n",
    "        # determine the number of input features \n",
    "        n_features = df_sel.shape[1] \n",
    "        \n",
    "        print(f\">Threshold={round(t,2)}, Features={n_features}\")\n",
    "        \n",
    "        # store the result \n",
    "        results.append(n_features) \n",
    "        \n",
    "    # plot the threshold vs the number of selected features \n",
    "    plt.plot(range, results) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb3c3bd-8e47-4b1e-8229-da5fd4a3daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 38) (937,)\n",
      ">Threshold=0.0, Features=38\n",
      ">Threshold=0.05, Features=29\n",
      ">Threshold=0.1, Features=28\n",
      ">Threshold=0.15, Features=28\n",
      ">Threshold=0.2, Features=28\n",
      ">Threshold=0.25, Features=28\n",
      ">Threshold=0.3, Features=28\n",
      ">Threshold=0.35, Features=28\n",
      ">Threshold=0.4, Features=28\n",
      ">Threshold=0.45, Features=26\n",
      ">Threshold=0.5, Features=25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve original data\n",
    "oil_spill_data = oil_spill_data_copy\n",
    "\n",
    "# split data into inputs and outputs \n",
    "data = oil_spill_data.values \n",
    "X = data[:, :-1] \n",
    "y = data[:, -1] \n",
    "print(X.shape, y.shape) \n",
    "\n",
    "# define variance thresholds to check \n",
    "thresholds = np.arange(0.0, 0.55, 0.05) \n",
    "\n",
    "explore_variance_threshold_effects(X, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00e7d9f0-e00d-4431-b472-e1239c53ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 38)\n",
      "(937, 28)\n"
     ]
    }
   ],
   "source": [
    "# Remove features whose variance is lower than threshold\n",
    "threshold = 0.4\n",
    "\n",
    "# Retrieve original data\n",
    "oil_spill_data = oil_spill_data_copy\n",
    "\n",
    "# split data into inputs and outputs \n",
    "data = oil_spill_data.values \n",
    "X = data[:, :-1] \n",
    "y = data[:, -1] \n",
    "print(X.shape) \n",
    "\n",
    "# define the transform \n",
    "transform = VarianceThreshold(threshold=threshold) \n",
    "\n",
    "# transform the input data \n",
    "X_sel = transform.fit_transform(X) \n",
    "\n",
    "print(X_sel.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b700851-aa74-4fde-b513-1d722115ac8b",
   "metadata": {},
   "source": [
    "#### Identify rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2e3e05b-1e5c-45fe-9916-22a88fee8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve original data\n",
    "iris_data = iris_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2ef4987-6e48-4a0d-836f-3ab00ad04b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "142                5.8               2.7                5.1               1.9   \n",
      "\n",
      "     target  \n",
      "142       2  \n"
     ]
    }
   ],
   "source": [
    "# calculate duplicates \n",
    "dups = iris_data.duplicated() \n",
    "\n",
    "# report if there are any duplicates \n",
    "print(dups.any()) \n",
    "\n",
    "# list all duplicate rows \n",
    "print(iris_data[dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0d279-1093-4e1c-9a1e-da001eb40c1b",
   "metadata": {},
   "source": [
    "#### Delete rows thtat contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "749d3212-b6ba-4c59-851b-69c4e9eea65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    \"\"\"\n",
    "    Counts duplicated rows. If any, function drops duplicated rows.\n",
    "    Args:\n",
    "        df (DataFrame): Raw data\n",
    "    Returns:\n",
    "        - df (Dataframe) without duplicated rows, a string otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check number of rows before removing duplicates\n",
    "    print(f\"Number of rows : {len(df)}\")\n",
    "\n",
    "    # Compute the number of duplicated rows\n",
    "    num_dups = df.duplicated().sum()\n",
    "    \n",
    "    print(f\"Number of duplicated rows : {num_dups}\")\n",
    "\n",
    "    if df.duplicated().any():\n",
    "        # Remove duplicates\n",
    "        df_no_duplicates = df.drop_duplicates()\n",
    "        print(f\"{num_dups} duplicated row(s) removed\")\n",
    "        return df_no_duplicates\n",
    "    else:\n",
    "        return \"No duplicated rows found !\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe22e7fa-8cd6-4220-ae49-5614f3571375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 150\n",
      "Number of duplicated rows : 1\n",
      "1 duplicated row(s) removed\n"
     ]
    }
   ],
   "source": [
    "iris_data_no_dups = check_duplicates(iris_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7c6ee-09eb-4a60-9657-f57463cca397",
   "metadata": {},
   "source": [
    "### Outlier Identification and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "045b4a43-6f1a-42e5-acbe-095139d4155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=50.049 stdv=4.994\n"
     ]
    }
   ],
   "source": [
    "# generate gaussian data \n",
    "from numpy.random import seed \n",
    "from numpy.random import randn \n",
    "from numpy import mean \n",
    "from numpy import std \n",
    "# seed the random number generator \n",
    "seed(1) \n",
    "# generate univariate observations \n",
    "data = 5 * randn(10000) + 50 \n",
    "# summarize \n",
    "print('mean=%.3f stdv=%.3f' % (mean(data), std(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0996f25-24c7-4207-b976-8e88ec9d8d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot boxplot of data\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(data=data, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34eea0-b5a0-4dfa-995e-a05e14c18772",
   "metadata": {},
   "source": [
    "#### Standard Deviation Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005291e-3d9e-42c2-a185-fb63738a82ce",
   "metadata": {},
   "source": [
    "If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bbb802c-27bd-4eca-967b-b3df58f85fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If distribution Gaussian or Gaussian-like, we use the standard deviation of the sample as a cut-off for identifying outliers\n",
    "def identify_outliers_std_dev_method(data : np.ndarray):\n",
    "    \"\"\"\n",
    "    If distribution Gaussian or Gaussian-like, we use the standard deviation of the sample as a cut-off for identifying outliers\n",
    "    Args:\n",
    "        data (ndarray): Raw data\n",
    "    Returns:\n",
    "        - outliers (ndarray) : only identified outliers\n",
    "        - outliers_removed (ndarray) : data with outliers removed\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate summary statistics \n",
    "    data_mean, data_std = mean(data), std(data) \n",
    "\n",
    "    # define outliers \n",
    "    cut_off = data_std * 3 \n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off \n",
    "\n",
    "    # identify outliers \n",
    "    outliers = [x for x in data if x < lower or x > upper] \n",
    "    print(f\"Identified outliers: {len(outliers)}\") \n",
    "\n",
    "    # remove outliers\n",
    "    outliers_removed = [x for x in data if x >= lower and x <= upper] \n",
    "    print(f\"Non-outlier observations: {len(outliers_removed)}\")\n",
    "\n",
    "    return outliers, outliers_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd179fb6-f0a5-4a4c-a19c-c2a386b28e1d",
   "metadata": {},
   "source": [
    "So far we have only talked about univariate data with a Gaussian distribution, e.g. a single variable. You can use the same approach if you have multivariate data, e.g. data with multiple variables, each with a different Gaussian distribution. You can imagine bounds in two dimensions that would define an ellipse if you have two variables. Observations that fall outside of the ellipse would be considered outliers. In three dimensions, this would be an ellipsoid, and so on into higher dimensions. Alternately, if you knew more about the domain, perhaps an outlier may be identified by exceeding the limits on one or a subset of the data dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83542d41-07d8-4d3a-9089-fb78a3a8a5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified outliers: 29\n",
      "Non-outlier observations: 9971\n"
     ]
    }
   ],
   "source": [
    "# identify outliers with standard deviation \n",
    "from numpy.random import seed \n",
    "from numpy.random import randn \n",
    "from numpy import mean \n",
    "from numpy import std \n",
    "\n",
    "# seed the random number generator \n",
    "seed(1) \n",
    "\n",
    "# generate univariate observations \n",
    "data = 5 * randn(10000) + 50 \n",
    "\n",
    "outliers, outliers_removed = identify_outliers_std_dev_method(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c98136-f3d7-431c-92a5-16abaa7b33a7",
   "metadata": {},
   "source": [
    "#### Interquartile Range Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9e1c238-ce20-4235-9c37-0ce5d8f246c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers with interquartile range \n",
    "def identify_outliers_iqr_method(data : np.ndarray):\n",
    "    \"\"\"\n",
    "    A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short.\n",
    "    This function identifies outliers with interquartile range (iqr).\n",
    "    Args:\n",
    "        data (ndarray): Raw data\n",
    "    Returns:\n",
    "        - outliers (ndarray) : only identified outliers\n",
    "        - outliers_removed (ndarray) : data with outliers removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate interquartile range \n",
    "    q25, q75 = percentile(data, 25), percentile(data, 75) \n",
    "    iqr = q75 - q25 \n",
    "    print(f\"Percentiles: 25th={round(q25,3)}, 75th={round(q75,3)}, IQR={round(iqr,3)}\") \n",
    "    \n",
    "    # calculate the outlier cutoff \n",
    "    cut_off = iqr * 1.5 \n",
    "    lower, upper = q25 - cut_off, q75 + cut_off \n",
    "    \n",
    "    # identify outliers \n",
    "    outliers = [x for x in data if x < lower or x > upper] \n",
    "    print('Identified outliers: %d' % len(outliers)) \n",
    "    \n",
    "    # remove outliers \n",
    "    outliers_removed = [x for x in data if x >= lower and x <= upper] \n",
    "    print('Non-outlier observations: %d' % len(outliers_removed))\n",
    "    \n",
    "    return outliers, outliers_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c1fc81e-1e97-49c2-9f74-a492c04b653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles: 25th=46.685, 75th=53.359, IQR=6.674\n",
      "Identified outliers: 81\n",
      "Non-outlier observations: 9919\n"
     ]
    }
   ],
   "source": [
    "# identify outliers with interquartile range \n",
    "from numpy.random import seed \n",
    "from numpy.random import randn \n",
    "from numpy import percentile \n",
    "# seed the random number generator \n",
    "seed(1) \n",
    "# generate univariate observations \n",
    "data = 5 * randn(10000) + 50 \n",
    "\n",
    "outliers, outliers_removed = identify_outliers_iqr_method(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a4fc5-b0a2-49e1-ace9-7c90e8e7d83a",
   "metadata": {},
   "source": [
    "#### Automatic Outlier Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a2d54eb-6f2e-4a09-8b86-7adde1ab466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of boston_housing : (506, 14)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Boston Housing Dataset \n",
    "boston_housing = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv \"\n",
    "boston_housing=pd.read_csv(boston_housing,header=None)\n",
    "display(f\"Shape of boston_housing : {boston_housing.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "216478f6-1fe1-4cf5-82f1-dd01cbf2479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.417\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on the raw dataset \n",
    "from pandas import read_csv \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "# retrieve the array \n",
    "data = boston_housing.values \n",
    "\n",
    "# split into input and output elements \n",
    "X, y = data[:, :-1], data[:, -1] \n",
    "\n",
    "# split into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# fit the model \n",
    "model = LinearRegression() \n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions \n",
    "mae = mean_absolute_error(y_test, yhat) \n",
    "print(f\"MAE: {round(mae,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df815670-852b-418c-b703-828a68ca931b",
   "metadata": {},
   "source": [
    "##### LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c398e9a6-6a4a-40f7-93e7-28adb8984ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(305, 13) (305,)\n",
      "MAE: 3.356\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset with outliers removed \n",
    "from pandas import read_csv \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.neighbors import LocalOutlierFactor \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "# retrieve the array \n",
    "data = boston_housing.values \n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1] \n",
    "\n",
    "# split into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# summarize the shape of the training dataset \n",
    "print(X_train.shape, y_train.shape) \n",
    "\n",
    "# identify outliers in the training dataset \n",
    "lof = LocalOutlierFactor() \n",
    "yhat = lof.fit_predict(X_train) \n",
    "\n",
    "# select all rows that are not outliers \n",
    "mask = yhat != -1 \n",
    "X_train, y_train = X_train[mask, :], y_train[mask] \n",
    "\n",
    "# summarize the shape of the updated training dataset \n",
    "print(X_train.shape, y_train.shape) \n",
    "\n",
    "# fit the model \n",
    "model = LinearRegression() \n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# evaluate the model \n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions \n",
    "mae = mean_absolute_error(y_test, yhat) \n",
    "print(f\"MAE: {round(mae,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67c79f-5bfc-454c-8658-c7c4436d51e7",
   "metadata": {},
   "source": [
    "##### IsolationForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3571cbfb-e7f5-4ce4-826b-fa639e6fa60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(294, 13) (294,)\n",
      "MAE: 3.218\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset with outliers removed \n",
    "from pandas import read_csv \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "# retrieve the array \n",
    "data = boston_housing.values \n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1] \n",
    "\n",
    "# split into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# summarize the shape of the training dataset \n",
    "print(X_train.shape, y_train.shape) \n",
    "\n",
    "# identify outliers in the training dataset \n",
    "lof = IsolationForest() \n",
    "yhat = lof.fit_predict(X_train) \n",
    "\n",
    "# select all rows that are not outliers \n",
    "mask = yhat != -1 \n",
    "X_train, y_train = X_train[mask, :], y_train[mask] \n",
    "\n",
    "# summarize the shape of the updated training dataset \n",
    "print(X_train.shape, y_train.shape) \n",
    "\n",
    "# fit the model \n",
    "model = LinearRegression() \n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# evaluate the model \n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions \n",
    "mae = mean_absolute_error(y_test, yhat) \n",
    "print(f\"MAE: {round(mae,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cf012-b89d-43b0-8952-5162025a398b",
   "metadata": {},
   "source": [
    "### How to Mark and Remove Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef41c103-884b-415c-a8aa-e4d1d48b7360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of diabetes_data : (768, 9)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Load diabetes data\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "\n",
    "diabetes_data=pd.read_csv(path_diabetes,header=None)\n",
    "display(f\"Shape of diabetes_data : {diabetes_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d82b05b7-90db-4bd2-b279-49dcd0531563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "diabetes_data_copy = diabetes_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7d3a9-f50b-4d13-8d1a-7f396c7ed59b",
   "metadata": {},
   "source": [
    "#### Mark Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "762b2982-9112-4632-b17e-5ca71178fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Summarize the dataset from pandas import read_csv\n",
    "print(diabetes_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb2f05-bf60-48be-8183-0b836bc1d7b4",
   "metadata": {},
   "source": [
    "This is useful. We can see that there are columns that have a minimum value of zero (0). On some columns, a value of zero does not make sense and indicates an invalid or missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "508ef3c4-e46c-4d92-a959-0e0365a26618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# example of summarizing the number of missing values for each variable from pandas import read_csv\n",
    "# count the number of missing values for each column \n",
    "num_missing = (diabetes_data[[1,2,3,4,5]] == 0).sum()\n",
    "\n",
    "# report the results \n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177087fb-dc9c-4f16-8c86-ba0ba02e0263",
   "metadata": {},
   "source": [
    "In Python, specifically Pandas, NumPy and Scikit-Learn, we mark missing values as NaN. Values with a NaN value are ignored from operations like sum, count, etc. We can mark values as NaN easily with the Pandas DataFrame by using the replace() function on a subset of the columns we are interested in. After we have marked the missing values, we can use the isnull() function to mark all of the NaN values in the dataset as True and get a count of the missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f09d3d5d-662d-426c-a010-5a98761e4df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# example of marking missing values with nan values from numpy import nan\n",
    " # replace '0' values with 'nan'\n",
    "diabetes_data[[1,2,3,4,5]]  =  diabetes_data[[1,2,3,4,5]].replace(0,  np.nan) \n",
    "\n",
    "# count the number of nan values in each column \n",
    "print(diabetes_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c97df7-ab24-47f0-8782-fbc1b9ab278e",
   "metadata": {},
   "source": [
    "This is a useful summary, as we want to confirm that we have not fooled ourselves somehow.\n",
    "Below is the same example, except we print the first 20 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6e061dd-2f27-42ed-bbcf-bf1c99d3b3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3      4     5      6   7  8\n",
      "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
      "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
      "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
      "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
      "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
      "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
      "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
      "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
      "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
      "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
      "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
      "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
      "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
      "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
      "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
      "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
      "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
      "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
      "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
      "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "# example of review data with missing values marked with a nan from numpy import nan\n",
    "# replace '0' values with 'nan'\n",
    "diabetes_data[[1,2,3,4,5]]  =  diabetes_data[[1,2,3,4,5]].replace(0,  np.nan) \n",
    "\n",
    "# summarize the first 20 rows of data \n",
    "print(diabetes_data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b77366-6841-46e4-9e91-80ad5562ee48",
   "metadata": {},
   "source": [
    "#### Missing Values Cause Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db7a14-e017-45a8-8743-18169cfaeda8",
   "metadata": {},
   "source": [
    "Having missing values in a dataset can cause errors with some machine learning algorithms.\n",
    "In this section, we will try to evaluate the Linear Discriminant Analysis (LDA) algorithm on the dataset with missing values. This is an algorithm that does not work when there are missing values in the dataset. The example below marks the missing values in the dataset, as we did in the previous section, then attempts to evaluate LDA using 3-fold cross-validation and print the mean accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f5837f1-cf21-42ba-8b43-923e9b2e450c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # example where missing values cause errors from numpy import nan\n",
    "# from pandas import read_csv\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # load the dataset\n",
    "# diabetes_data = diabetes_data_copy\n",
    "\n",
    "# # replace '0' values with 'nan'\n",
    "# diabetes_data[[1,2,3,4,5]] = diabetes_data[[1,2,3,4,5]].replace(0, np.nan) \n",
    "\n",
    "# # split dataset into inputs and outputs\n",
    "# values = diabetes_data.values\n",
    "# X = values[:,0:8]\n",
    "# y = values[:,8]\n",
    "\n",
    "# # define the model\n",
    "# model  =  LinearDiscriminantAnalysis()\n",
    "\n",
    "# # define the model evaluation procedure\n",
    "# cv = KFold(n_splits=3, shuffle=True, random_state=1) \n",
    "\n",
    "# # evaluate the model\n",
    "# result = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# # report the mean performance \n",
    "# print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501121d-83e7-4f9e-8294-a053fc7ec77a",
   "metadata": {},
   "source": [
    "Example error message when trying to evaluate a model with missing values."
   ]
  },
  {
   "attachments": {
    "61856668-09d5-41f0-90df-46617f7e8e2f.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8d388191-a498-4504-a5b6-a9d1a53e3934",
   "metadata": {},
   "source": [
    "![image.png](attachment:61856668-09d5-41f0-90df-46617f7e8e2f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e26acd-c182-4682-aaa5-8e84ec2e4491",
   "metadata": {},
   "source": [
    "This is as we expect. We are prevented from evaluating an LDA algorithm (and other algorithms) on the dataset with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38c397-d1f0-490b-90ff-9ce8ed5526d7",
   "metadata": {},
   "source": [
    "#### Remove Rows With Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e9799-6f37-493f-a915-71ae96c694f3",
   "metadata": {},
   "source": [
    "The simplest strategy for handling missing data is to remove records that contain a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "234c7cf8-6d7c-4036-b2a2-2719b05855b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "# example of removing rows that contain missing values from numpy import nan\n",
    "from pandas import read_csv \n",
    "\n",
    "# load the dataset\n",
    "diabetes_data = diabetes_data_copy\n",
    "\n",
    "# summarize the shape of the raw data\n",
    "print(diabetes_data.shape)\n",
    "\n",
    "# replace '0' values with 'nan'\n",
    "diabetes_data[[1,2,3,4,5]]  =  diabetes_data[[1,2,3,4,5]].replace(0,  np.nan) \n",
    "\n",
    "# drop rows with missing values \n",
    "diabetes_data.dropna(inplace=True)\n",
    "\n",
    "# summarize the shape of the data with missing rows removed \n",
    "print(diabetes_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753edb7-4d01-4a21-b580-0afb8a3d1e2c",
   "metadata": {},
   "source": [
    "We now have a dataset that we could use to evaluate an algorithm sensitive to missing values like LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d965e2c-60ae-41fe-b4fc-40fd20f28629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on data after rows with missing data are removed from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load the dataset\n",
    "diabetes_data = diabetes_data_copy\n",
    "\n",
    "# replace '0' values with 'nan'\n",
    "diabetes_data[[1,2,3,4,5]]  =  diabetes_data[[1,2,3,4,5]].replace(0,  np.nan) \n",
    "\n",
    "# drop rows with missing values \n",
    "diabetes_data.dropna(inplace=True)\n",
    "\n",
    "# split dataset into inputs and outputs \n",
    "values = diabetes_data.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "\n",
    "# define the model\n",
    "model  =  LinearDiscriminantAnalysis()\n",
    "\n",
    "# define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1) \n",
    "\n",
    "# evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy') \n",
    "\n",
    "# report the mean performance\n",
    "print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda2a87-1bbe-4429-93c3-f6fc3c18e02b",
   "metadata": {},
   "source": [
    "**Removing rows with missing values can be too limiting on some predictive modeling problems, an alternative is to impute missing values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3f854-b862-44d1-97f2-28ba37bd07ae",
   "metadata": {},
   "source": [
    "### How to Use Statistical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36894fd9-0b5f-4bc0-9777-caaec7e06eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of horse_colic_data : (300, 28)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Load diabetes data\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "\n",
    "horse_colic_data=pd.read_csv(path_horse_colic,header=None)\n",
    "display(f\"Shape of horse_colic_data : {horse_colic_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4670e8f5-3942-4620-995a-fd318eabec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "horse_colic_data_copy = horse_colic_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6644402f-a0f6-45f5-843e-6557fcbd7720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.50</td>\n",
       "      <td>66</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>45.00</td>\n",
       "      <td>8.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.30</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>33.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.10</td>\n",
       "      <td>164</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>48.00</td>\n",
       "      <td>7.20</td>\n",
       "      <td>3</td>\n",
       "      <td>5.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.30</td>\n",
       "      <td>104</td>\n",
       "      <td>35</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>74.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1        2      3    4   5  6  7  8  9   ...     18    19 20    21 22  \\\n",
       "0  2   1   530101  38.50   66  28  3  3  ?  2  ...  45.00  8.40  ?     ?  2   \n",
       "1  1   1   534817   39.2   88  20  ?  ?  4  1  ...     50    85  2     2  3   \n",
       "2  2   1   530334  38.30   40  24  1  1  3  1  ...  33.00  6.70  ?     ?  1   \n",
       "3  1   9  5290409  39.10  164  84  4  1  6  2  ...  48.00  7.20  3  5.30  2   \n",
       "4  2   1   530255  37.30  104  35  ?  ?  6  2  ...  74.00  7.40  ?     ?  2   \n",
       "\n",
       "  23     24 25 26 27  \n",
       "0  2  11300  0  0  2  \n",
       "1  2   2208  0  0  2  \n",
       "2  2      0  0  0  1  \n",
       "3  1   2208  0  0  1  \n",
       "4  2   4300  0  0  2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of data\n",
    "horse_colic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530684f6-5ef2-4404-8521-8bddc80e162d",
   "metadata": {},
   "source": [
    "Marking missing values with a NaN (not a number) value in a loaded dataset using Python is a best practice. \n",
    "**We can load the dataset using the read csv() Pandas function and specify the na values to load values of â€œ?â€ as missing, marked with a NaN value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7079f79-aa25-47ec-8066-b808058fe7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of horse_colic_data : (300, 28)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "display(f\"Shape of horse_colic_data : {horse_colic_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b772d7f-bef8-4673-9ded-69291e2bdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "horse_colic_data_copy = horse_colic_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecb35143-d5e2-4361-b3b7-38b3f871c71d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
       "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
       "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
       "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
       "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
       "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
       "\n",
       "    20   21   22  23     24  25  26  27  \n",
       "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
       "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
       "2  NaN  NaN  1.0   2      0   0   0   1  \n",
       "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
       "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of data\n",
    "horse_colic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ff2b6ac-73bc-4165-bf43-a5a924f26a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 6 (2.0%)\n",
      "> 1, Missing: 5 (1.7%)\n",
      "> 2, Missing: 5 (1.7%)\n",
      "> 3, Missing: 1 (0.3%)\n",
      "> 4, Missing: 12 (4.0%)\n",
      "> 5, Missing: 8 (2.7%)\n",
      "> 6, Missing: 3 (1.0%)\n",
      "> 7, Missing: 8 (2.7%)\n",
      "> 8, Missing: 4 (1.3%)\n",
      "> 9, Missing: 4 (1.3%)\n",
      "> 10, Missing: 0 (0.0%)\n",
      "> 11, Missing: 4 (1.3%)\n",
      "> 12, Missing: 4 (1.3%)\n",
      "> 13, Missing: 2 (0.7%)\n",
      "> 14, Missing: 1 (0.3%)\n",
      "> 15, Missing: 3 (1.0%)\n",
      "> 16, Missing: 3 (1.0%)\n",
      "> 17, Missing: 16 (5.3%)\n",
      "> 18, Missing: 2 (0.7%)\n",
      "> 19, Missing: 9 (3.0%)\n",
      "> 20, Missing: 2 (0.7%)\n",
      "> 21, Missing: 3 (1.0%)\n",
      "> 22, Missing: 7 (2.3%)\n",
      "> 23, Missing: 10 (3.3%)\n",
      "> 24, Missing: 5 (1.7%)\n",
      "> 25, Missing: 15 (5.0%)\n",
      "> 26, Missing: 1 (0.3%)\n",
      "> 27, Missing: 2 (0.7%)\n"
     ]
    }
   ],
   "source": [
    "# summarize the number of rows with missing values for each column \n",
    "for i in range(horse_colic_data.shape[1]):\n",
    "    # count number of rows with missing values \n",
    "    n_miss = horse_colic_data.iloc[i].isnull().sum()\n",
    "    perc = n_miss / horse_colic_data.shape[0] * 100\n",
    "    perc = round(perc,1)\n",
    "    print(f'> {i}, Missing: {n_miss} ({perc}%)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2e73e-1df5-475d-81dc-ed99e2de2ec2",
   "metadata": {},
   "source": [
    "#### Statistical Imputation With SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65e0ab-30ae-4ad5-8f26-ea4b2290c12b",
   "metadata": {},
   "source": [
    "The scikit-learn machine learning library provides the SimpleImputer class that supports statistical imputation. In this section, we will explore how to effectively use the SimpleImputer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78d91352-22f4-420b-b2ce-1dc1825a65ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing:  1605\n",
      "Missing:  0\n"
     ]
    }
   ],
   "source": [
    "# statistical imputation transform for the horse colic dataset from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print(f\"Missing:  {sum(np.isnan(X).flatten())}\") \n",
    "\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy='mean') \n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset \n",
    "Xtrans = imputer.transform(X) \n",
    "\n",
    "# summarize total missing\n",
    "print(f\"Missing:  {sum(np.isnan(Xtrans).flatten())}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d96484-7dfe-4b7b-a1c0-5fde2227b745",
   "metadata": {},
   "source": [
    "#### SimpleImputer and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad368b-e34d-49db-bf6f-ebff617bbd47",
   "metadata": {},
   "source": [
    "It is a good practice to evaluate machine learning models on a dataset using k-fold cross- validation. **To correctly apply statistical missing data imputation and avoid data leakage**, it is required that the statistics calculated for each column are calculated on the training dataset only, then applied to the train and test sets for each fold in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f39e3-cab8-4e77-a42f-fc4ab25e2406",
   "metadata": {},
   "source": [
    "This can be achieved by creating a modeling pipeline where the first step is the statistical imputation, then the second step is the model. This can be achieved using the Pipeline class. For example, the Pipeline below uses a SimpleImputer with a â€˜meanâ€™ strategy, followed by a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d49e1ba-b2f9-4335-8ebd-1d180d6c3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.874 (0.056)\n"
     ]
    }
   ],
   "source": [
    "# evaluate mean imputation and random forest for the horse colic dataset from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)]) \n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "print(f'Mean Accuracy: {round(mean(scores),3)} ({round(std(scores),3)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef1573-c9c9-489c-a16e-cb4336013bb4",
   "metadata": {},
   "source": [
    "#### Comparing Different Imputed Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf39d4d-fdb0-4703-8302-428d1abb4145",
   "metadata": {},
   "source": [
    "How do we know that using a â€˜meanâ€™ statistical strategy is good or best for this dataset? The answer is that we donâ€™t and that it was chosen arbitrarily. We can design an experiment to test each statistical strategy and discover what works best for this dataset, comparing the mean, median, mode (most frequent), and constant (0) strategies. The mean accuracy of each approach can then be compared. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cc8d6e2-a1f4-48e1-b5b0-eb6ba13cf6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">mean 0.864 (0.056)\n",
      ">median 0.864 (0.052)\n",
      ">most_frequent 0.876 (0.055)\n",
      ">constant 0.874 (0.052)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare statistical imputation strategies for the horse colic dataset from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset \n",
    "results = list()\n",
    "strategies = ['mean', 'median', 'most_frequent', 'constant'] \n",
    "\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestClassifier())])\n",
    "\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # store results\n",
    "    results.append(scores)\n",
    "    print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True) \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18f8e2-117c-4680-84b9-e3e23544f856",
   "metadata": {},
   "source": [
    "#### SimpleImputer Transform When Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fc920aa-27b0-49b8-8ee4-95e504cc4f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "# constant imputation strategy and prediction for the horse colic dataset from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('i', SimpleImputer(strategy='constant')), ('m', RandomForestClassifier())])\n",
    "\n",
    "# fit the model \n",
    "pipeline.fit(X, y) \n",
    "\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, np.nan, 2, 5, 4, 4, np.nan, np.nan, np.nan, 3, 5, 45.00, 8.40, np.nan, np.nan, 2, 11300, 00000, 00000, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row]) \n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d'  % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cce2de-bcb7-48d0-8dcd-182fbabe0e9b",
   "metadata": {},
   "source": [
    "### How to Use KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53723529-e235-488d-964c-8f263dca5bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of horse_colic_data : (300, 28)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Load diabetes data\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "\n",
    "horse_colic_data=pd.read_csv(path_horse_colic,header=None)\n",
    "display(f\"Shape of horse_colic_data : {horse_colic_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "071a8122-41da-401a-9749-aa20acc0301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "horse_colic_data_copy = horse_colic_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5492226-5f2e-4cd5-9193-4503e0b6d2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.50</td>\n",
       "      <td>66</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>45.00</td>\n",
       "      <td>8.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.30</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>33.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.10</td>\n",
       "      <td>164</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>48.00</td>\n",
       "      <td>7.20</td>\n",
       "      <td>3</td>\n",
       "      <td>5.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.30</td>\n",
       "      <td>104</td>\n",
       "      <td>35</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>74.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1        2      3    4   5  6  7  8  9   ...     18    19 20    21 22  \\\n",
       "0  2   1   530101  38.50   66  28  3  3  ?  2  ...  45.00  8.40  ?     ?  2   \n",
       "1  1   1   534817   39.2   88  20  ?  ?  4  1  ...     50    85  2     2  3   \n",
       "2  2   1   530334  38.30   40  24  1  1  3  1  ...  33.00  6.70  ?     ?  1   \n",
       "3  1   9  5290409  39.10  164  84  4  1  6  2  ...  48.00  7.20  3  5.30  2   \n",
       "4  2   1   530255  37.30  104  35  ?  ?  6  2  ...  74.00  7.40  ?     ?  2   \n",
       "\n",
       "  23     24 25 26 27  \n",
       "0  2  11300  0  0  2  \n",
       "1  2   2208  0  0  2  \n",
       "2  2      0  0  0  1  \n",
       "3  1   2208  0  0  1  \n",
       "4  2   4300  0  0  2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of data\n",
    "horse_colic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4be4db-8f61-42d5-9434-93ec4689eb86",
   "metadata": {},
   "source": [
    "Marking missing values with a NaN (not a number) value in a loaded dataset using Python is a best practice. \n",
    "**We can load the dataset using the read csv() Pandas function and specify the na values to load values of â€œ?â€ as missing, marked with a NaN value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a719f8b1-7e73-49bd-bcb9-bedddfb56110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of horse_colic_data : (300, 28)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "display(f\"Shape of horse_colic_data : {horse_colic_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7dbdd71-ccdd-49c9-b013-7bd8b24637a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "horse_colic_data_copy = horse_colic_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf71b1f8-d855-4706-81ea-5d68d02be1a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
       "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
       "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
       "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
       "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
       "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
       "\n",
       "    20   21   22  23     24  25  26  27  \n",
       "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
       "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
       "2  NaN  NaN  1.0   2      0   0   0   1  \n",
       "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
       "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of data\n",
    "horse_colic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "536917a5-b2fb-4ff8-86f4-b4271a65251d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 6 (2.0%)\n",
      "> 1, Missing: 5 (1.7%)\n",
      "> 2, Missing: 5 (1.7%)\n",
      "> 3, Missing: 1 (0.3%)\n",
      "> 4, Missing: 12 (4.0%)\n",
      "> 5, Missing: 8 (2.7%)\n",
      "> 6, Missing: 3 (1.0%)\n",
      "> 7, Missing: 8 (2.7%)\n",
      "> 8, Missing: 4 (1.3%)\n",
      "> 9, Missing: 4 (1.3%)\n",
      "> 10, Missing: 0 (0.0%)\n",
      "> 11, Missing: 4 (1.3%)\n",
      "> 12, Missing: 4 (1.3%)\n",
      "> 13, Missing: 2 (0.7%)\n",
      "> 14, Missing: 1 (0.3%)\n",
      "> 15, Missing: 3 (1.0%)\n",
      "> 16, Missing: 3 (1.0%)\n",
      "> 17, Missing: 16 (5.3%)\n",
      "> 18, Missing: 2 (0.7%)\n",
      "> 19, Missing: 9 (3.0%)\n",
      "> 20, Missing: 2 (0.7%)\n",
      "> 21, Missing: 3 (1.0%)\n",
      "> 22, Missing: 7 (2.3%)\n",
      "> 23, Missing: 10 (3.3%)\n",
      "> 24, Missing: 5 (1.7%)\n",
      "> 25, Missing: 15 (5.0%)\n",
      "> 26, Missing: 1 (0.3%)\n",
      "> 27, Missing: 2 (0.7%)\n"
     ]
    }
   ],
   "source": [
    "# summarize the number of rows with missing values for each column \n",
    "for i in range(horse_colic_data.shape[1]):\n",
    "    # count number of rows with missing values \n",
    "    n_miss = horse_colic_data.iloc[i].isnull().sum()\n",
    "    perc = n_miss / horse_colic_data.shape[0] * 100\n",
    "    perc = round(perc,1)\n",
    "    print(f'> {i}, Missing: {n_miss} ({perc}%)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31282d-69c5-4a3d-85d3-6787517f678f",
   "metadata": {},
   "source": [
    "#### KNNImputer Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4554f4c-23d7-4cd1-a2c2-b0c07638a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# knn imputation transform for the horse colic dataset from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import KNNImputer \n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(np.isnan(X).flatten())) \n",
    "\n",
    "# define imputer\n",
    "imputer = KNNImputer() \n",
    "\n",
    "# fit on the dataset \n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset \n",
    "Xtrans = imputer.transform(X) \n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d'  % sum(np.isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e16c8f-4a35-40ce-830a-a21783a0290c",
   "metadata": {},
   "source": [
    "#### KNNImputer and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7d462a1-f1b8-4694-9a49-ea1a585301b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.863 (0.059)\n"
     ]
    }
   ],
   "source": [
    "# evaluate knn imputation and random forest for the horse colic dataset from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier() \n",
    "imputer = KNNImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdad58f-9292-4c59-9684-695b49596f10",
   "metadata": {},
   "source": [
    "#### KNNImputer and Different Number of Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b614705-37ba-4eca-bf3b-5fb14951503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 0.864 (0.052)\n",
      ">3 0.868 (0.055)\n",
      ">5 0.861 (0.048)\n",
      ">7 0.869 (0.054)\n",
      ">9 0.863 (0.054)\n",
      ">15 0.859 (0.055)\n",
      ">18 0.871 (0.055)\n",
      ">21 0.864 (0.055)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJ6lJREFUeJzt3X1wVHWe7/FPp5k8SQIyCXkAJAY0CcrDgEsmIKuzpggwNwu6YyGIQBziSJGqdbJqGUQiesc4dYcUFMUMrgvqwHXA1chajhtlshPUSxjcoDWiCRAQQSARspJAAgHS5/4xQzstDeR0Hs6vu9+vqlOS0+f8zvdrp8755Nenu12WZVkCAAAwWITTBQAAAFwLgQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYLx+ThfQEzwej44dO6a4uDi5XC6nywEAAF1gWZZOnz6t1NRURURcfQ4lJALLsWPHNGzYMKfLAAAAAThy5IiGDh161W1CIrDExcVJ+kvD8fHxDlcDAAC6orW1VcOGDfNex68mJALLpZeB4uPjCSwAAASZrtzOwU23AADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABgvJL78sLe1t7ervr7e72Nnz57VoUOHlJaWppiYmCuOkZmZqdjY2N4qsVeEa9/hKlyfb/q+HH3Ttz9O901g6YL6+npNmDChW2PU1tZq/PjxPVRR3wjXvsNVuD7f9B04+g4eodC3y7Isy7Gj95DW1lYNGDBALS0tio+P7/Hxr5ZM6+rqNG/ePG3atElZWVlXHMPpZBqIcO07XIXr803fl6Nv+vanN/q2c/1mhqULYmNjr5kqs7Kygi5xX0u49h2uwvX5pu8ro+/QEQp9c9MtAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMF5AgWXt2rVKS0tTdHS0srOztWvXritue+HCBT3zzDMaMWKEoqOjNXbsWFVWVvps8/TTT8vlcvksmZmZgZQGAABCkO3AsmXLFhUXF6u0tFS7d+/W2LFjlZeXp6+//trv9suWLdMLL7ygNWvW6PPPP9fDDz+su+++Wx9//LHPdrfccouOHz/uXT788MPAOgIAACHHdmApLy9XYWGhCgoKNGrUKK1bt06xsbHasGGD3+03btyopUuXasaMGUpPT9fixYs1Y8YMrVy50me7fv36KTk52bskJCQE1hEAAAg5tgLL+fPnVVtbq9zc3G8HiIhQbm6uampq/O7T0dGh6Ohon3UxMTGXzaDs379fqampSk9P1/3336/Dhw9fsY6Ojg61trb6LAAAIHTZCiwnT55UZ2enkpKSfNYnJSWpsbHR7z55eXkqLy/X/v375fF4tG3bNlVUVOj48ePebbKzs/Xyyy+rsrJSv/nNb/TFF19oypQpOn36tN8xy8rKNGDAAO8ybNgwO20AAIAg0+vvElq9erVuuukmZWZmKjIyUkVFRSooKFBExLeHnj59uu69916NGTNGeXl5euedd3Tq1Cm99tprfscsKSlRS0uLdzly5EhvtwEAABxkK7AkJCTI7XarqanJZ31TU5OSk5P97pOYmKitW7eqra1NX375perr69W/f3+lp6df8TgDBw7UzTffrIaGBr+PR0VFKT4+3mcBAAChy1ZgiYyM1IQJE1RVVeVd5/F4VFVVpZycnKvuGx0drSFDhujixYt64403NHPmzCtue+bMGR04cEApKSl2ygMAACHK9ktCxcXFevHFF/XKK6+orq5OixcvVltbmwoKCiRJ8+fPV0lJiXf7P/3pT6qoqNDBgwf1wQcfaNq0afJ4PHr88ce92zz66KPavn27Dh06pB07dujuu++W2+3WnDlzeqBFAAAQ7PrZ3WH27Nk6ceKEli9frsbGRo0bN06VlZXeG3EPHz7sc3/KuXPntGzZMh08eFD9+/fXjBkztHHjRg0cONC7zVdffaU5c+aoublZiYmJuv3227Vz504lJiZ2v0MAABD0bAcWSSoqKlJRUZHfx6qrq31+vuOOO/T5559fdbzNmzcHUgYAAAgTfJcQAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYLx+ThcAmKa9vV319fV+Hzt79qwOHTqktLQ0xcTEXHGMzMxMxcbG9laJABB2CCzAd9TX12vChAndGqO2tlbjx4/voYoAAAQW4DsyMzNVW1vr97G6ujrNmzdPmzZtUlZW1lXHAAD0HAIL8B2xsbHXnB3JyspiBgUA+hA33QIAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjBRRY1q5dq7S0NEVHRys7O1u7du264rYXLlzQM888oxEjRig6Olpjx45VZWVlt8YEAADhxXZg2bJli4qLi1VaWqrdu3dr7NixysvL09dff+13+2XLlumFF17QmjVr9Pnnn+vhhx/W3XffrY8//jjgMQEAQHixHVjKy8tVWFiogoICjRo1SuvWrVNsbKw2bNjgd/uNGzdq6dKlmjFjhtLT07V48WLNmDFDK1euDHhMAAAQXvrZ2fj8+fOqra1VSUmJd11ERIRyc3NVU1Pjd5+Ojg5FR0f7rIuJidGHH37YrTE7Ojq8P7e2ttppw6/9+/fr9OnTtverq6vz+W8g4uLidNNNNwW8f3fQtz30Td920Xffo297gqZvy4ajR49akqwdO3b4rH/sscesiRMn+t1nzpw51qhRo6x9+/ZZnZ2d1nvvvWfFxMRYkZGRAY9ZWlpqSbpsaWlpsdOO1759+/yO15fLvn37Aqq9O+ibvumbvumbvp3su6WlxZK6dv22NcMSiNWrV6uwsFCZmZlyuVwaMWKECgoKuvVyT0lJiYqLi70/t7a2atiwYQGPdymRbtq0SVlZWbb2PXv2rA4dOqS0tDTFxMTYPnZdXZ3mzZsXUCruLvqm766ib/q2g77puzfYCiwJCQlyu91qamryWd/U1KTk5GS/+yQmJmrr1q06d+6cmpublZqaqieeeELp6ekBjxkVFaWoqCg7pXdJVlaWxo8fb3u/yZMn93gtfYm+7aHv4ETf9tB3cArlvm3ddBsZGakJEyaoqqrKu87j8aiqqko5OTlX3Tc6OlpDhgzRxYsX9cYbb2jmzJndHhMAAIQH2y8JFRcXa8GCBbrttts0ceJErVq1Sm1tbSooKJAkzZ8/X0OGDFFZWZkk6U9/+pOOHj2qcePG6ejRo3r66afl8Xj0+OOPd3lMAAAQ3mwHltmzZ+vEiRNavny5GhsbNW7cOFVWViopKUmSdPjwYUVEfDtxc+7cOS1btkwHDx5U//79NWPGDG3cuFEDBw7s8pgAACC8BXTTbVFRkYqKivw+Vl1d7fPzHXfcoc8//7xbYwIAgPDGdwkBAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxuvndAEAAPSk5P4uxZzaJx3r27/JY07tU3J/V58eM5wQWAAAIeVnEyKV9f7PpPf79rhZfz02egeBBQAQUl6oPa/Zy19WVmZmnx63rr5eL6ycq3/s06OGDwILACCkNJ6xdHbgzVLquD497tlGjxrPWH16zHDCTbcAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAASTXHajRz60zVHKtxuhT4QWABAIQ9y7K0evdqHWw5qNW7V8uy+BJD0xBYAABhb8exHfqs+TNJ0mfNn2nHsR0OV4TvIrAAuCamyhHKLMvSmo/XKML1l0tihCtCaz5ewyyLYQgsAK6KqXKEukuzKx7LI0nyWB5mWQxEYEFA+Is7fDBVjlD23dmVS5hlMQ+BBbbxF3f4YKocoe67syuXMMtiHgILbOMv7vDBVDlC2aVA7pLL7+MuuQjoBiGwwBb+4g4fTJUj1F3wXFBjW6Ms+f9dtmSpsa1RFzwX+rgy+NPP6QJMkdzfpZhT+6RjfZvhYk7tU3J//+m+L9jte8fJP3tnV6S/+Yv7042anDCmy8cNtr57SjD1/d3n+pJAnvNg6rsn0bfZIt2R2vy/Nut/zv3PFbcZFD1Ike7IPqwqcKH+fBNY/upnEyKV9f7PpPft7VcTHaXnv3+9nmj+RjnnOmwfN+uvx3aKnb4tSWtSkxQRGSmP69tfzgjL0pqd/1uTjjVdYWL1csHUd08Klr4vPdeuyEhZrsufVZfN5zxY+u5p9G2+5OuSlXxdstNl9IhQv44RWP7qhdrzmr38ZWVlZnZ5H8uytHpXqQ62fqHVGT/UDyeukMvPyf1q6urr9cLKufpHuwX3EDt97zj5Z3328f+5bL3H5dJnUVHacc+aLv/FHUx996Rg6fuC54IaP3hE1vlWv49bLpca4wbrwqL/q8iI713zuMHS93fVNO/R83s36omMB5Tz/VttHzdY++4up/sOV6F+HSOw/FXjGUtnB94spY7r8j47jv4/fdb6hSTps9YvtEPtmpw62dZxzzZ61HjGuXsButq3ZVlas/t5ueTy+3qvSy6tOfyOJo1+oEu/7MHSd08Llr4jJW3+xzeuPVXexb9Mg6Xvv/WXd8M9r4Ntx7T6y7f1w1vvt30iD8a+e4LTfYerUL+OEVgC9Lc3JHosj/dGxEmpk2yf1IKBnZvTguX1XlxdKE2VB8Lfu+EmD7F3IgdMFmzXMQJLgP72ZCb5vt0zFE9qoXZzWqBqjtXo+V3P64mJTygnNcfpctBLgu1EDgQi2K5jBJYAfPdkdkmon9TC/S/u735g3g9TfhiSzzOC70QO2BWM1zE+hyUAfDJieOID88IDnz+DcBCM1zECi018MmJ44gPzwkcwnsgBO4L1OkZgsYlPRgxPfER9eAjWE3lP48tNQ1uwXse4h8Umbj4NP8H4Wi8Cw7vhuFcrHATrdYzAEoBwv/k03Hz3BsxLuBEz9ATribwn8Xbu8BCM1zECC3AVf/sSwRU/MI9ZlpASjCfynsLbuWEy7mEBriJYX+sFAsG9WjAZMyzAVfASAcIF92rBdAQW4BrC+SUChA/u1YLpeEkIAMIcb+dGMCCwAECY414tBANeEgKAMMe9WggGBBYAAPdqwXi8JAQAAIwXUGBZu3at0tLSFB0drezsbO3ateuq269atUoZGRmKiYnRsGHD9POf/1znzp3zPv7000/L5XL5LJmZmYGUBgAAQpDtl4S2bNmi4uJirVu3TtnZ2Vq1apXy8vK0d+9eDR48+LLtX331VT3xxBPasGGDJk2apH379mnhwoVyuVwqLy/3bnfLLbfoD3/4w7eF9ePVKgAA8Be2Z1jKy8tVWFiogoICjRo1SuvWrVNsbKw2bNjgd/sdO3Zo8uTJmjt3rtLS0jR16lTNmTPnslmZfv36KTk52bskJCQE1hEAAAg5tgLL+fPnVVtbq9zc3G8HiIhQbm6uamr8fw35pEmTVFtb6w0oBw8e1DvvvKMZM2b4bLd//36lpqYqPT1d999/vw4fPnzFOjo6OtTa2uqzAACA0GXrdZeTJ0+qs7NTSUlJPuuTkpJUX1/vd5+5c+fq5MmTuv3222VZli5evKiHH35YS5cu9W6TnZ2tl19+WRkZGTp+/LhWrFihKVOmaM+ePYqLi7tszLKyMq1YscJO6QAAIIj1+ruEqqur9dxzz+nXv/61du/erYqKCv3+97/Xs88+691m+vTpuvfeezVmzBjl5eXpnXfe0alTp/Taa6/5HbOkpEQtLS3e5ciRI73dBgAAcJCtGZaEhAS53W41NTX5rG9qalJysv/37z/11FN64IEHtGjRIknS6NGj1dbWpoceekhPPvmkIiIuz0wDBw7UzTffrIaGBr9jRkVFKSoqyk7pAAAgiNmaYYmMjNSECRNUVVXlXefxeFRVVaWcnBy/+7S3t18WStxutyRd8Xspzpw5owMHDiglJcVOeQAAIETZfu9wcXGxFixYoNtuu00TJ07UqlWr1NbWpoKCAknS/PnzNWTIEJWVlUmS8vPzVV5erh/84AfKzs5WQ0ODnnrqKeXn53uDy6OPPqr8/HwNHz5cx44dU2lpqdxut+bMmdODrQIAgGBlO7DMnj1bJ06c0PLly9XY2Khx48apsrLSeyPu4cOHfWZUli1bJpfLpWXLluno0aNKTExUfn6+fvGLX3i3+eqrrzRnzhw1NzcrMTFRt99+u3bu3KnExMQeaBEAAAS7gD6draioSEVFRX4fq66u9j1Av34qLS1VaWnpFcfbvHlzIGUAAIAwwXcJAQAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMbr53QBJmhvb5ck7d692/a+Z8+e1aFDh5SWlqaYmBjb+9fV1dnep6fQN313VTD3jfASrr/n4dA3gUVSfX29JKmwsNCxGuLi4vr8mPRN333Nib4RXsL19zwc+iawSJo1a5YkKTMzU7Gxsbb2raur07x587Rp0yZlZWUFdPy4uDjddNNNAe3bHfRN310VzH0jvITr73k49E1gkZSQkKBFixZ1a4ysrCyNHz++hyrqG/QdOPoGzBSuv+fh0Dc33QIAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxuvndAEA0Jva29slSbt377a979mzZ3Xo0CGlpaUpJibG9v51dXW29+kp4do3QheBBUBIq6+vlyQVFhY6VkNcXFyfHzNc+0boIrAACGmzZs2SJGVmZio2NtbWvnV1dZo3b542bdqkrKysgI4fFxenm266KaB9uyNc+0boIrAACGkJCQlatGhRt8bIysrS+PHje6iivhGufSN0cdMtAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjBRRY1q5dq7S0NEVHRys7O1u7du266varVq1SRkaGYmJiNGzYMP385z/XuXPnujUmAAAIH7YDy5YtW1RcXKzS0lLt3r1bY8eOVV5enr7++mu/27/66qt64oknVFpaqrq6Oq1fv15btmzR0qVLAx4TAACEF9uBpby8XIWFhSooKNCoUaO0bt06xcbGasOGDX6337FjhyZPnqy5c+cqLS1NU6dO1Zw5c3xmUOyOCQAAwoutwHL+/HnV1tYqNzf32wEiIpSbm6uamhq/+0yaNEm1tbXegHLw4EG98847mjFjRsBjdnR0qLW11WcBAAChy9Yn3Z48eVKdnZ1KSkryWZ+UlOT93orvmjt3rk6ePKnbb79dlmXp4sWLevjhh70vCQUyZllZmVasWGGndAAAEMR6/V1C1dXVeu655/TrX/9au3fvVkVFhX7/+9/r2WefDXjMkpIStbS0eJcjR470YMUAAMA0tmZYEhIS5Ha71dTU5LO+qalJycnJfvd56qmn9MADD3i/02L06NFqa2vTQw89pCeffDKgMaOiohQVFWWndAAAEMRszbBERkZqwoQJqqqq8q7zeDyqqqpSTk6O333a29sVEeF7GLfbLUmyLCugMQEAQHix/W3NxcXFWrBggW677TZNnDhRq1atUltbmwoKCiRJ8+fP15AhQ1RWViZJys/PV3l5uX7wgx8oOztbDQ0Neuqpp5Sfn+8NLtcaEwAAhDfbgWX27Nk6ceKEli9frsbGRo0bN06VlZXem2YPHz7sM6OybNkyuVwuLVu2TEePHlViYqLy8/P1i1/8ostjAgCA8GY7sEhSUVGRioqK/D5WXV3te4B+/VRaWqrS0tKAxwQAAOGN7xICAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjNfP6QKCQXt7u+rr6/0+VldX5/PfK8nMzFRsbGyP19ab6Pty9E3f/tB38KDvywVN31YIaGlpsSRZLS0tvTJ+bW2tJalbS21tba/U1pvom77pm77pm757s28712+XZVmWglxra6sGDBiglpYWxcfH9/j4V0umZ8+e1aFDh5SWlqaYmJgrjuF4Mg0AfV+OvunbH/oOHvR9OSf7tnP9JrAAAABH2Ll+c9MtAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMF5AgWXt2rVKS0tTdHS0srOztWvXritue+edd8rlcl22/PjHP/Zus3DhwssenzZtWiClAQCAENTP7g5btmxRcXGx1q1bp+zsbK1atUp5eXnau3evBg8efNn2FRUVOn/+vPfn5uZmjR07Vvfee6/PdtOmTdNLL73k/TkqKspuaQAAIETZnmEpLy9XYWGhCgoKNGrUKK1bt06xsbHasGGD3+0HDRqk5ORk77Jt2zbFxsZeFliioqJ8trv++usD6wgAAIQcW4Hl/Pnzqq2tVW5u7rcDREQoNzdXNTU1XRpj/fr1uu+++3Tdddf5rK+urtbgwYOVkZGhxYsXq7m5+YpjdHR0qLW11WcBAAChy1ZgOXnypDo7O5WUlOSzPikpSY2Njdfcf9euXdqzZ48WLVrks37atGn67W9/q6qqKv3yl7/U9u3bNX36dHV2dvodp6ysTAMGDPAuw4YNs9MGAAAIMrbvYemO9evXa/To0Zo4caLP+vvuu8/779GjR2vMmDEaMWKEqqurddddd102TklJiYqLi70/t7a2EloAAAhhtmZYEhIS5Ha71dTU5LO+qalJycnJV923ra1Nmzdv1k9/+tNrHic9PV0JCQlqaGjw+3hUVJTi4+N9FgAAELpsBZbIyEhNmDBBVVVV3nUej0dVVVXKycm56r7//u//ro6ODs2bN++ax/nqq6/U3NyslJQUO+UBAIAQZftdQsXFxXrxxRf1yiuvqK6uTosXL1ZbW5sKCgokSfPnz1dJScll+61fv16zZs3S97//fZ/1Z86c0WOPPaadO3fq0KFDqqqq0syZMzVy5Ejl5eUF2BYAAAgltu9hmT17tk6cOKHly5ersbFR48aNU2VlpfdG3MOHDysiwjcH7d27Vx9++KHee++9y8Zzu93685//rFdeeUWnTp1Samqqpk6dqmeffZbPYgEAAJIkl2VZltNFdFdra6sGDBiglpYW7mcBACBI2Ll+811CAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABjP9rc1A52dnfrggw90/PhxpaSkaMqUKXK73U6XBQAIYcywwJaKigqNHDlSP/rRjzR37lz96Ec/0siRI1VRUeF0aQCAEEZgQZdVVFToJz/5iUaPHq2amhqdPn1aNTU1Gj16tH7yk58QWgAAvcZlWZbldBHd1draqgEDBqilpUXx8fFOlxOSOjs7NXLkSI0ePVpbt25VRMS3Wdfj8WjWrFnas2eP9u/fz8tDAIAusXP9ZoYFXfLBBx/o0KFDWrp0qU9YkaSIiAiVlJToiy++0AcffOBQhQCAUEZgQZccP35cknTrrbf6ffzS+kvbAQDQkwgs6JKUlBRJ0p49e/w+fmn9pe0AAOhJBBZ0yZQpU5SWlqbnnntOHo/H5zGPx6OysjLdeOONmjJlikMVAgBCGYEFXeJ2u7Vy5Uq9/fbbmjVrls+7hGbNmqW3335bv/rVr7jhFgDQK/jgOHTZPffco9dff13/8i//okmTJnnX33jjjXr99dd1zz33OFgdACCU8bZm2MYn3QIAeoKd6zczLLDN7XbrzjvvdLoMAEAY4R4WAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGC8kPik20vfLtDa2upwJQAAoKsuXbe78i1BIRFYTp8+LUkaNmyYw5UAAAC7Tp8+rQEDBlx1m5D48kOPx6Njx44pLi5OLperT4/d2tqqYcOG6ciRI2H1xYv0Td/hgL7pOxw42bdlWTp9+rRSU1MVEXH1u1RCYoYlIiJCQ4cOdbSG+Pj4sPoFv4S+wwt9hxf6Di9O9X2tmZVLuOkWAAAYj8ACAACMR2DppqioKJWWlioqKsrpUvoUfdN3OKBv+g4HwdJ3SNx0CwAAQhszLAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/A0g3vv/++8vPzlZqaKpfLpa1btzpdUq/7zW9+ozFjxng/YCgnJ0f/+Z//6XRZve7pp5+Wy+XyWTIzM50uq9elpaVd1rfL5dKSJUucLq3XnT59Wo888oiGDx+umJgYTZo0SR999JHTZfWoa53DFi5ceNlzP23aNGeK7UHX6vvMmTMqKirS0KFDFRMTo1GjRmndunXOFNuDysrK9Hd/93eKi4vT4MGDNWvWLO3du9dnm3/913/VnXfeqfj4eLlcLp06dcqZYv0gsHRDW1ubxo4dq7Vr1zpdSp8ZOnSonn/+edXW1uq///u/9Q//8A+aOXOmPvvsM6dL63W33HKLjh8/7l0+/PBDp0vqdR999JFPz9u2bZMk3XvvvQ5X1vsWLVqkbdu2aePGjfr00081depU5ebm6ujRo06X1mO6cg6bNm2az+/A7373uz6ssHdcq+/i4mJVVlZq06ZNqqur0yOPPKKioiK99dZbfVxpz9q+fbuWLFminTt3atu2bbpw4YKmTp2qtrY27zbt7e2aNm2ali5d6mClV2ChR0iy3nzzTafLcMT1119v/du//ZvTZfSq0tJSa+zYsU6X4bh//ud/tkaMGGF5PB6nS+lV7e3tltvttt5++22f9ePHj7eefPJJh6rqXf7OYQsWLLBmzpzpSD19xV/ft9xyi/XMM8/4rAvF5/7rr7+2JFnbt2+/7LE//vGPliTrm2++6fvCroAZFgSss7NTmzdvVltbm3Jycpwup9ft379fqampSk9P1/3336/Dhw87XVKfOn/+vDZt2qQHH3ywz79ktK9dvHhRnZ2dio6O9lkfExMTFjNrf6u6ulqDBw9WRkaGFi9erObmZqdL6nWTJk3SW2+9paNHj8qyLP3xj3/Uvn37NHXqVKdL61EtLS2SpEGDBjlcSdeExJcfom99+umnysnJ0blz59S/f3+9+eabGjVqlNNl9ars7Gy9/PLLysjI0PHjx7VixQpNmTJFe/bsUVxcnNPl9YmtW7fq1KlTWrhwodOl9Lq4uDjl5OTo2WefVVZWlpKSkvS73/1ONTU1GjlypNPl9Zlp06bpnnvu0Y033qgDBw5o6dKlmj59umpqauR2u50ur9esWbNGDz30kIYOHap+/fopIiJCL774ov7+7//e6dJ6jMfj0SOPPKLJkyfr1ltvdbqcLiGwwLaMjAx98sknamlp0euvv64FCxZo+/btIR1apk+f7v33mDFjlJ2dreHDh+u1117TT3/6Uwcr6zvr16/X9OnTlZqa6nQpfWLjxo168MEHNWTIELndbo0fP15z5sxRbW2t06X1mfvuu8/779GjR2vMmDEaMWKEqqurdddddzlYWe9as2aNdu7cqbfeekvDhw/X+++/ryVLlig1NVW5ublOl9cjlixZoj179gTVjCGBBbZFRkZ6/8qcMGGCPvroI61evVovvPCCw5X1nYEDB+rmm29WQ0OD06X0iS+//FJ/+MMfVFFR4XQpfWbEiBHavn272tra1NraqpSUFM2ePVvp6elOl+aY9PR0JSQkqKGhIWQDy9mzZ7V06VK9+eab+vGPfyzpL3+kfPLJJ/rVr34VEoGlqKhIb7/9tt5//30NHTrU6XK6jHtY0G0ej0cdHR1Ol9Gnzpw5owMHDiglJcXpUvrESy+9pMGDB3tP4OHkuuuuU0pKir755hu9++67mjlzptMlOearr75Sc3NzSP/eX7hwQRcuXFBEhO/l0e12y+PxOFRVz7AsS0VFRXrzzTf1X//1X7rxxhudLskWZli64cyZMz5/YX/xxRf65JNPNGjQIN1www0OVtZ7SkpKNH36dN1www06ffq0Xn31VVVXV+vdd991urRe9eijjyo/P1/Dhw/XsWPHVFpaKrfbrTlz5jhdWq/zeDx66aWXtGDBAvXrFz6njHfffVeWZSkjI0MNDQ167LHHlJmZqYKCAqdL6zFXO4cNGjRIK1as0D/90z8pOTlZBw4c0OOPP66RI0cqLy/Pwaq771rn7jvuuEOPPfaYYmJiNHz4cG3fvl2//e1vVV5e7mDV3bdkyRK9+uqr+o//+A/FxcWpsbFRkjRgwADFxMRIkhobG9XY2Oj9//Ppp58qLi5ON9xwg/M35zr8LqWgdultX99dFixY4HRpvebBBx+0hg8fbkVGRlqJiYnWXXfdZb333ntOl9XrZs+ebaWkpFiRkZHWkCFDrNmzZ1sNDQ1Ol9Un3n33XUuStXfvXqdL6VNbtmyx0tPTrcjISCs5OdlasmSJderUKafL6lFXO4e1t7dbU6dOtRITE63vfe971vDhw63CwkKrsbHR6bK77Vrn7uPHj1sLFy60UlNTrejoaCsjI8NauXJl0L+d31/PkqyXXnrJu01paek1t3GKy7Isq4+yEQAAQEC4hwUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4/1/2PNzz7vu/jEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare knn imputation strategies for the horse colic dataset from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset \n",
    "results = list()\n",
    "strategies = [str(i) for i in [1,3,5,7,9,15,18,21]] \n",
    "\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m', RandomForestClassifier())])\n",
    "    \n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    \n",
    "    # store results\n",
    "    results.append(scores)  \n",
    "    print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498972a1-e060-4c58-ba44-58f8121fff68",
   "metadata": {},
   "source": [
    "#### KNNImputer Transform When Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5df94eb1-4de6-4051-9f0c-e55448fb5b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "# knn imputation strategy and prediction for the horse colic dataset from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "horse_colic_data = horse_colic_data_copy\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=21)), ('m', RandomForestClassifier())])\n",
    "\n",
    "# fit the model \n",
    "pipeline.fit(X, y) \n",
    "\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, np.nan, 2, 5, 4, 4, np.nan, np.nan, np.nan, 3, 5, 45.00,\n",
    "8.40, np.nan, np.nan, 2, 11300, 00000, 00000, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row]) \n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d'  % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26fff23-e399-49d8-b15f-73e31642fe4c",
   "metadata": {},
   "source": [
    "### How to Use Iterative Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "484e54dc-a9f9-4191-82c0-0ad9ed463e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of horse_colic_data : (300, 28)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Load diabetes data\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "\n",
    "horse_colic_data=pd.read_csv(path_horse_colic,header=None)\n",
    "display(f\"Shape of horse_colic_data : {horse_colic_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9704414e-43bc-460d-94f2-04d9b501a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "horse_colic_data_copy = horse_colic_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bb784ee-b003-49d4-abb3-a73f12318167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.50</td>\n",
       "      <td>66</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>45.00</td>\n",
       "      <td>8.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.30</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>33.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.10</td>\n",
       "      <td>164</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>48.00</td>\n",
       "      <td>7.20</td>\n",
       "      <td>3</td>\n",
       "      <td>5.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.30</td>\n",
       "      <td>104</td>\n",
       "      <td>35</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>74.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1        2      3    4   5  6  7  8  9   ...     18    19 20    21 22  \\\n",
       "0  2   1   530101  38.50   66  28  3  3  ?  2  ...  45.00  8.40  ?     ?  2   \n",
       "1  1   1   534817   39.2   88  20  ?  ?  4  1  ...     50    85  2     2  3   \n",
       "2  2   1   530334  38.30   40  24  1  1  3  1  ...  33.00  6.70  ?     ?  1   \n",
       "3  1   9  5290409  39.10  164  84  4  1  6  2  ...  48.00  7.20  3  5.30  2   \n",
       "4  2   1   530255  37.30  104  35  ?  ?  6  2  ...  74.00  7.40  ?     ?  2   \n",
       "\n",
       "  23     24 25 26 27  \n",
       "0  2  11300  0  0  2  \n",
       "1  2   2208  0  0  2  \n",
       "2  2      0  0  0  1  \n",
       "3  1   2208  0  0  1  \n",
       "4  2   4300  0  0  2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of data\n",
    "horse_colic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3dcd91-28c5-4084-93d0-352e691843a9",
   "metadata": {},
   "source": [
    "Marking missing values with a NaN (not a number) value in a loaded dataset using Python is a best practice. \n",
    "**We can load the dataset using the read csv() Pandas function and specify the na values to load values of â€œ?â€ as missing, marked with a NaN value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee7d7954-51df-4252-bd56-f40a695cd059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of horse_colic_data : (300, 28)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "display(f\"Shape of horse_colic_data : {horse_colic_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d16baa2-e1eb-43fa-bb74-312771c9197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of datasets\n",
    "horse_colic_data_copy = horse_colic_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7093379b-2a38-4829-820c-f1901e9debd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
       "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
       "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
       "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
       "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
       "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
       "\n",
       "    20   21   22  23     24  25  26  27  \n",
       "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
       "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
       "2  NaN  NaN  1.0   2      0   0   0   1  \n",
       "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
       "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of data\n",
    "horse_colic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1912d844-e321-44f9-a884-94dc8422f456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 6 (2.0%)\n",
      "> 1, Missing: 5 (1.7%)\n",
      "> 2, Missing: 5 (1.7%)\n",
      "> 3, Missing: 1 (0.3%)\n",
      "> 4, Missing: 12 (4.0%)\n",
      "> 5, Missing: 8 (2.7%)\n",
      "> 6, Missing: 3 (1.0%)\n",
      "> 7, Missing: 8 (2.7%)\n",
      "> 8, Missing: 4 (1.3%)\n",
      "> 9, Missing: 4 (1.3%)\n",
      "> 10, Missing: 0 (0.0%)\n",
      "> 11, Missing: 4 (1.3%)\n",
      "> 12, Missing: 4 (1.3%)\n",
      "> 13, Missing: 2 (0.7%)\n",
      "> 14, Missing: 1 (0.3%)\n",
      "> 15, Missing: 3 (1.0%)\n",
      "> 16, Missing: 3 (1.0%)\n",
      "> 17, Missing: 16 (5.3%)\n",
      "> 18, Missing: 2 (0.7%)\n",
      "> 19, Missing: 9 (3.0%)\n",
      "> 20, Missing: 2 (0.7%)\n",
      "> 21, Missing: 3 (1.0%)\n",
      "> 22, Missing: 7 (2.3%)\n",
      "> 23, Missing: 10 (3.3%)\n",
      "> 24, Missing: 5 (1.7%)\n",
      "> 25, Missing: 15 (5.0%)\n",
      "> 26, Missing: 1 (0.3%)\n",
      "> 27, Missing: 2 (0.7%)\n"
     ]
    }
   ],
   "source": [
    "# summarize the number of rows with missing values for each column \n",
    "for i in range(horse_colic_data.shape[1]):\n",
    "    # count number of rows with missing values \n",
    "    n_miss = horse_colic_data.iloc[i].isnull().sum()\n",
    "    perc = n_miss / horse_colic_data.shape[0] * 100\n",
    "    perc = round(perc,1)\n",
    "    print(f'> {i}, Missing: {n_miss} ({perc}%)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fb2b1-3976-478d-81ea-7aaf27708752",
   "metadata": {},
   "source": [
    "#### IterativeImputer Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7136508-a20f-4806-b6f3-1441bf3c52ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# iterative imputation transform for the horse colic dataset from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# load dataset\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(np.isnan(X).flatten())) \n",
    "\n",
    "# define imputer\n",
    "imputer = IterativeImputer() \n",
    "\n",
    "# fit on the dataset \n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset \n",
    "Xtrans = imputer.transform(X) \n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(np.isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b08b3-c7e7-40ae-a2eb-ca75d489ea98",
   "metadata": {},
   "source": [
    "#### IterativeImputer and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a12c731b-1872-438f-bc94-91c168b99f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.874 (0.054)\n"
     ]
    }
   ],
   "source": [
    "# evaluate iterative imputation and random forest for the horse colic dataset from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier() \n",
    "imputer = IterativeImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean score: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746faee-53f3-4fda-ac48-7c16bc2e5745",
   "metadata": {},
   "source": [
    "#### IterativeImputer and Different Imputation Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5c94e094-0a45-4a3a-a985-65f27893ae0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">ascending 0.868 (0.051)\n",
      ">descending 0.876 (0.051)\n",
      ">roman 0.873 (0.053)\n",
      ">arabic 0.869 (0.053)\n",
      ">random 0.874 (0.058)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare iterative imputation strategies for the horse colic dataset from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load dataset\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset \n",
    "results = list()\n",
    "strategies = ['ascending', 'descending', 'roman', 'arabic', 'random'] \n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', IterativeImputer(imputation_order=s)), ('m', RandomForestClassifier())])\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe6871-1d44-4e8a-8924-b408c60ee858",
   "metadata": {},
   "source": [
    "#### IterativeImputer and Different Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5cedfe62-c483-44ec-8bf4-5182087010a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "765292d6-ac94-476d-ab9f-da5100ae017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 0.872 (0.050)\n",
      ">2 0.873 (0.053)\n",
      ">3 0.871 (0.054)\n",
      ">4 0.871 (0.056)\n",
      ">5 0.874 (0.052)\n",
      ">6 0.870 (0.047)\n",
      ">7 0.872 (0.052)\n",
      ">8 0.872 (0.049)\n",
      ">9 0.866 (0.048)\n",
      ">10 0.868 (0.051)\n",
      ">11 0.873 (0.053)\n",
      ">12 0.872 (0.053)\n",
      ">13 0.870 (0.046)\n",
      ">14 0.873 (0.056)\n",
      ">15 0.867 (0.049)\n",
      ">16 0.871 (0.049)\n",
      ">17 0.873 (0.055)\n",
      ">18 0.868 (0.051)\n",
      ">19 0.866 (0.051)\n",
      ">20 0.867 (0.051)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare iterative imputation number of iterations for the horse colic dataset from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load dataset\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset \n",
    "results = list()\n",
    "strategies = [str(i) for i in range(1, 21)] \n",
    "\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', IterativeImputer(max_iter=int(s))), ('m', RandomForestClassifier())])\n",
    "    \n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    \n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True) \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a3967-6ce7-434d-a1f1-81080cf4792d",
   "metadata": {},
   "source": [
    "#### IterativeImputer Transform When Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e8ed75b-1d49-4f44-8bb5-ecf299128e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "# iterative imputation strategy and prediction for the horse colic dataset from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# load dataset\n",
    "path_horse_colic= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "horse_colic_data = read_csv(path_horse_colic, header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = horse_colic_data.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23] \n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('i', IterativeImputer()), ('m', RandomForestClassifier())]) \n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y) \n",
    "\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, np.nan, 2, 5, 4, 4, np.nan, np.nan, np.nan, 3, 5, 45.00,\n",
    "8.40, np.nan, np.nan, 2, 11300, 00000, 00000, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row]) \n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d'  % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95956a33-f56f-418f-8a7c-6f51999c02be",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d19c8-b248-4f5e-9600-a0b7ebec65df",
   "metadata": {},
   "source": [
    "### How to Select Categorical Input Features (for Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1774bcf-df6a-4c31-a428-7406a4a72922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (191, 9) (191,)\n",
      "Test (95, 9) (95,)\n"
     ]
    }
   ],
   "source": [
    "# example of loading and preparing the breast cancer dataset from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset\n",
    "    data = read_csv(filename, header=None)     \n",
    "    # retrieve array\n",
    "    dataset = data.values   \n",
    "    # split into input and output variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]    \n",
    "    # format all fields as string\n",
    "    X = X.astype(str) \n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder() \n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train) \n",
    "    X_test_enc = oe.transform(X_test) \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder() \n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train) \n",
    "    y_test_enc = le.transform(y_test) \n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "path_breast_cancer_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "X, y = load_dataset(path_breast_cancer_data) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) \n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test) \n",
    "\n",
    "# summarize\n",
    "print('Train', X_train_enc.shape, y_train_enc.shape) \n",
    "print('Test', X_test_enc.shape, y_test_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf67af6-da3b-4fcf-9389-3642cd97b33c",
   "metadata": {},
   "source": [
    "#### Chi-Squared Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4fce84f-2858-4f6a-9239-1f3b601f701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.472553\n",
      "Feature 1: 0.029193\n",
      "Feature 2: 2.137658\n",
      "Feature 3: 29.381059\n",
      "Feature 4: 8.222601\n",
      "Feature 5: 8.100183\n",
      "Feature 6: 1.273822\n",
      "Feature 7: 0.950682\n",
      "Feature 8: 3.699989\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of chi squared feature selection for categorical data \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset\n",
    "    data = read_csv(filename, header=None)     \n",
    "    # retrieve array\n",
    "    dataset = data.values   \n",
    "    # split into input and output variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]    \n",
    "    # format all fields as string\n",
    "    X = X.astype(str) \n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder() \n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train) \n",
    "    X_test_enc = oe.transform(X_test) \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder() \n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train) \n",
    "    y_test_enc = le.transform(y_test) \n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test): \n",
    "    fs = SelectKBest(score_func=chi2, k='all') \n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "path_breast_cancer_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "X, y = load_dataset(path_breast_cancer_data) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) \n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc) \n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)): \n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "    \n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d87567-4876-4e7a-b52c-72eee1407459",
   "metadata": {},
   "source": [
    "#### Mutual Information Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "54253cef-0458-4621-a15b-7e459a6519e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.015322\n",
      "Feature 1: 0.000000\n",
      "Feature 2: 0.024701\n",
      "Feature 3: 0.000000\n",
      "Feature 4: 0.046487\n",
      "Feature 5: 0.061315\n",
      "Feature 6: 0.008881\n",
      "Feature 7: 0.019667\n",
      "Feature 8: 0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of chi squared feature selection for categorical data \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset\n",
    "    data = read_csv(filename, header=None)     \n",
    "    # retrieve array\n",
    "    dataset = data.values   \n",
    "    # split into input and output variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]    \n",
    "    # format all fields as string\n",
    "    X = X.astype(str) \n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder() \n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train) \n",
    "    X_test_enc = oe.transform(X_test) \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder() \n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train) \n",
    "    y_test_enc = le.transform(y_test) \n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test): \n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k='all') \n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "path_breast_cancer_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "X, y = load_dataset(path_breast_cancer_data) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) \n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc) \n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)): \n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "    \n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590951eb-a656-4a50-93b0-96bbad5eb52c",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using All Features (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5dd5c45-2738-4f3b-ac90-45e864a66b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.79\n"
     ]
    }
   ],
   "source": [
    "#evaluation of a model using all input features \n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset\n",
    "    data = read_csv(filename, header=None)     \n",
    "    # retrieve array\n",
    "    dataset = data.values   \n",
    "    # split into input and output variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]    \n",
    "    # format all fields as string\n",
    "    X = X.astype(str) \n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder() \n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train) \n",
    "    X_test_enc = oe.transform(X_test) \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder() \n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train) \n",
    "    y_test_enc = le.transform(y_test) \n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "path_breast_cancer_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "X, y = load_dataset(path_breast_cancer_data) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) \n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test) \n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='lbfgs') \n",
    "model.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_enc) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat) \n",
    "print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eef2ca-0ed3-42f4-a197-f7ad4a5b3c29",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using Chi-Squared Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "86e37f42-276d-4f33-9f0b-d4b09c4f75ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.74\n"
     ]
    }
   ],
   "source": [
    "#evaluation of a model using all input features \n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset\n",
    "    data = read_csv(filename, header=None)     \n",
    "    # retrieve array\n",
    "    dataset = data.values   \n",
    "    # split into input and output variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]    \n",
    "    # format all fields as string\n",
    "    X = X.astype(str) \n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder() \n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train) \n",
    "    X_test_enc = oe.transform(X_test) \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder() \n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train) \n",
    "    y_test_enc = le.transform(y_test) \n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test): \n",
    "    fs = SelectKBest(score_func=chi2, k=4) \n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "# load the dataset\n",
    "path_breast_cancer_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "X, y = load_dataset(path_breast_cancer_data) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) \n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='lbfgs') \n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat) \n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bd5de-9ec1-42c6-aa30-0f5a117926d9",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using Mutual Information Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66f9db14-07f2-496c-af91-289dd40497a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.68\n"
     ]
    }
   ],
   "source": [
    "#evaluation of a model using all input features \n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset\n",
    "    data = read_csv(filename, header=None)     \n",
    "    # retrieve array\n",
    "    dataset = data.values   \n",
    "    # split into input and output variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]    \n",
    "    # format all fields as string\n",
    "    X = X.astype(str) \n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder() \n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train) \n",
    "    X_test_enc = oe.transform(X_test) \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder() \n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train) \n",
    "    y_test_enc = le.transform(y_test) \n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test): \n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=4) \n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "# load the dataset\n",
    "path_breast_cancer_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "X, y = load_dataset(path_breast_cancer_data) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) \n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='lbfgs') \n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat) \n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820eb4c4-d5ca-4c84-a0a1-68f309181ed9",
   "metadata": {},
   "source": [
    "### How to Select Numerical Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf6b4eb0-dcfd-4d97-b4b9-8d93e7e26190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (514, 8) (514,)\n",
      "Test (254, 8) (254,)\n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# summarize\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45011db7-0df2-46ec-818a-e09c404397bb",
   "metadata": {},
   "source": [
    "#### ANOVA F-test Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c68d4558-c3e5-43aa-9dd6-88446cac1356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 16.527385\n",
      "Feature 1: 131.325562\n",
      "Feature 2: 0.042371\n",
      "Feature 3: 1.415216\n",
      "Feature 4: 12.778966\n",
      "Feature 5: 49.209523\n",
      "Feature 6: 13.377142\n",
      "Feature 7: 25.126440\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of anova f-test feature selection for numerical data \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import f_classif\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_classif, k='all') \n",
    "    # learn relationship from training data \n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test) \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs,  X_test_fs,  fs  =  select_features(X_train,  y_train,  X_test) \n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)): \n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "    \n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728ec66-3a2d-4506-bf69-249b76411371",
   "metadata": {},
   "source": [
    "#### Mutual Information Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "737567e2-60e9-4dd4-9505-1d6f8a573fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.025578\n",
      "Feature 1: 0.088535\n",
      "Feature 2: 0.032740\n",
      "Feature 3: 0.002998\n",
      "Feature 4: 0.044981\n",
      "Feature 5: 0.082367\n",
      "Feature 6: 0.011278\n",
      "Feature 7: 0.055919\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of mutual information feature selection for numerical input data \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k='all') \n",
    "    # learn relationship from training data \n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test) \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs,  X_test_fs,  fs  =  select_features(X_train,  y_train,  X_test) \n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)): \n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "    \n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfc258-6887-49a1-ac01-cde21945b644",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3494d96a-a900-4d1c-af6d-0af2b99bf737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.56\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using all input features \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='liblinear') \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat) \n",
    "print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0716f50e-a4e2-4fb6-9c7e-2dd3cf182506",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using ANOVA F-test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9e4e24c-b279-44b7-ae57-4b00eb9226c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.74\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using 4 features chosen with anova f-test \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_classif, k=4) \n",
    "    # learn relationship from training data \n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test) \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "    \n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) \n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train_fs, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat) \n",
    "print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7226d5-7afb-4ceb-bfbb-0739e516fd5a",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using Mutual Information Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "979a165b-afbe-4ee6-acba-57b231d57158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.56\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using 4 features chosen with mutual information \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=4) \n",
    "    # learn relationship from training data \n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test) \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "    \n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) \n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='liblinear') \n",
    "model.fit(X_train_fs, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca34c9-e227-4388-89d7-0c851fcebcde",
   "metadata": {},
   "source": [
    "#### Tune the Number of Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca2cf266-9b91-43d9-b3c8-75e9362602f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Accuracy: 0.770\n",
      "Best Config: {'anova__k': 5}\n"
     ]
    }
   ],
   "source": [
    "# compare different numbers of features selected using anova f-test \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "# define dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "\n",
    "# define the pipeline to evaluate\n",
    "model = LogisticRegression(solver='liblinear') \n",
    "fs = SelectKBest(score_func=f_classif)\n",
    "pipeline = Pipeline(steps=[('anova',fs), ('lr', model)]) \n",
    "\n",
    "# define the grid\n",
    "grid = dict()\n",
    "grid['anova__k'] = [i+1 for i in range(X.shape[1])] \n",
    "\n",
    "# define the grid search\n",
    "search = GridSearchCV(pipeline, param_grid=grid, scoring='accuracy', n_jobs=-1, cv=cv) \n",
    "\n",
    "# perform the search\n",
    "results = search.fit(X, y) \n",
    "\n",
    "# summarize best\n",
    "print('Best Mean Accuracy: %.3f' % results.best_score_) \n",
    "print('Best Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb32e1-c5fb-4e93-a456-c0f42043446e",
   "metadata": {},
   "source": [
    "We might want to see the relationship between the number of selected features and clas- sification accuracy. In this relationship, we may expect that more features result in a better performance to a point. This relationship can be explored by manually evaluating each con- figuration of k for the SelectKBest from 1 to 8, gathering the sample of accuracy scores, and plotting the results using box and whisker plots side-by-side. The spread and mean of these box plots would be expected to show any interesting relationship between the number of selected features and the classification accuracy of the pipeline. The complete example of achieving this is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2229b311-5f56-40b0-95c8-31e09e6aa347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 0.748 (0.048)\n",
      ">2 0.756 (0.042)\n",
      ">3 0.761 (0.044)\n",
      ">4 0.759 (0.042)\n",
      ">5 0.770 (0.041)\n",
      ">6 0.766 (0.042)\n",
      ">7 0.770 (0.042)\n",
      ">8 0.768 (0.040)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare different numbers of features selected using anova f-test \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array \n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1] \n",
    "    return X, y\n",
    "\n",
    "# evaluate a given model using cross-validation \n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "X, y = load_dataset(path_diabetes) \n",
    "\n",
    "# define number of features to evaluate \n",
    "num_features = [i+1 for i in range(X.shape[1])] \n",
    "\n",
    "# enumerate each number of features\n",
    "results = list()\n",
    "\n",
    "for k in num_features:\n",
    "    # create pipeline\n",
    "    model = LogisticRegression(solver='liblinear') \n",
    "    fs = SelectKBest(score_func=f_classif, k=k)\n",
    "    pipeline = Pipeline(steps=[('anova',fs), ('lr', model)]) \n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(pipeline) \n",
    "    results.append(scores)\n",
    "    # summarize the results\n",
    "    print('>%d %.3f (%.3f)'  % (k, mean(scores),  std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=num_features, showmeans=True) \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb8d17-ec3c-4d2d-9026-32e183bf8d2d",
   "metadata": {},
   "source": [
    "### How to Select Features for Numerical Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "11d6caad-0584-4bd8-b2f4-076dcca16313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (670, 100) (670,)\n",
      "Test (330, 100) (330,)\n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# summarize\n",
    "print('Train', X_train.shape, y_train.shape) \n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356372c5-ab04-42c7-8c07-802becda8a95",
   "metadata": {},
   "source": [
    "#### Correlation Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d8c3b4b-ecbd-466a-94bf-e4a4be761acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.009419\n",
      "Feature 1: 1.018881\n",
      "Feature 2: 1.205187\n",
      "Feature 3: 0.000138\n",
      "Feature 4: 0.167511\n",
      "Feature 5: 5.985083\n",
      "Feature 6: 0.062405\n",
      "Feature 7: 1.455257\n",
      "Feature 8: 0.420384\n",
      "Feature 9: 101.392225\n",
      "Feature 10: 0.387091\n",
      "Feature 11: 1.581124\n",
      "Feature 12: 3.014463\n",
      "Feature 13: 0.232705\n",
      "Feature 14: 0.076281\n",
      "Feature 15: 4.299652\n",
      "Feature 16: 1.497530\n",
      "Feature 17: 0.261242\n",
      "Feature 18: 5.960005\n",
      "Feature 19: 0.523219\n",
      "Feature 20: 0.003365\n",
      "Feature 21: 0.024178\n",
      "Feature 22: 0.220958\n",
      "Feature 23: 0.576770\n",
      "Feature 24: 0.627198\n",
      "Feature 25: 0.350687\n",
      "Feature 26: 0.281877\n",
      "Feature 27: 0.584210\n",
      "Feature 28: 52.196337\n",
      "Feature 29: 0.046855\n",
      "Feature 30: 0.147323\n",
      "Feature 31: 0.368485\n",
      "Feature 32: 0.077631\n",
      "Feature 33: 0.698140\n",
      "Feature 34: 45.744046\n",
      "Feature 35: 2.047376\n",
      "Feature 36: 0.786270\n",
      "Feature 37: 0.996190\n",
      "Feature 38: 2.733533\n",
      "Feature 39: 63.957656\n",
      "Feature 40: 231.885540\n",
      "Feature 41: 1.372448\n",
      "Feature 42: 0.581860\n",
      "Feature 43: 1.072930\n",
      "Feature 44: 1.066976\n",
      "Feature 45: 0.344656\n",
      "Feature 46: 13.951551\n",
      "Feature 47: 3.575080\n",
      "Feature 48: 0.007299\n",
      "Feature 49: 0.004651\n",
      "Feature 50: 1.094585\n",
      "Feature 51: 0.241065\n",
      "Feature 52: 0.355137\n",
      "Feature 53: 0.020294\n",
      "Feature 54: 0.154567\n",
      "Feature 55: 2.592512\n",
      "Feature 56: 0.300175\n",
      "Feature 57: 0.357798\n",
      "Feature 58: 3.060090\n",
      "Feature 59: 0.890357\n",
      "Feature 60: 122.132164\n",
      "Feature 61: 2.029982\n",
      "Feature 62: 0.091551\n",
      "Feature 63: 1.081123\n",
      "Feature 64: 0.056041\n",
      "Feature 65: 2.930717\n",
      "Feature 66: 0.054886\n",
      "Feature 67: 1.332787\n",
      "Feature 68: 0.145579\n",
      "Feature 69: 0.986331\n",
      "Feature 70: 0.092661\n",
      "Feature 71: 0.083219\n",
      "Feature 72: 0.198847\n",
      "Feature 73: 2.065792\n",
      "Feature 74: 0.236594\n",
      "Feature 75: 0.512608\n",
      "Feature 76: 1.095650\n",
      "Feature 77: 0.015359\n",
      "Feature 78: 2.193730\n",
      "Feature 79: 1.574530\n",
      "Feature 80: 5.360863\n",
      "Feature 81: 0.041874\n",
      "Feature 82: 5.717705\n",
      "Feature 83: 0.436560\n",
      "Feature 84: 5.594438\n",
      "Feature 85: 0.000065\n",
      "Feature 86: 0.026748\n",
      "Feature 87: 0.408422\n",
      "Feature 88: 2.092557\n",
      "Feature 89: 9.568498\n",
      "Feature 90: 0.642445\n",
      "Feature 91: 0.065794\n",
      "Feature 92: 198.705931\n",
      "Feature 93: 0.073807\n",
      "Feature 94: 1.048605\n",
      "Feature 95: 0.004106\n",
      "Feature 96: 0.042110\n",
      "Feature 97: 0.034228\n",
      "Feature 98: 0.792433\n",
      "Feature 99: 0.015365\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of correlation feature selection for numerical data \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import f_regression \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_regression, k='all') \n",
    "    # learn relationship from training data \n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test) \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs,  X_test_fs,  fs  =  select_features(X_train,  y_train,  X_test) \n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)): \n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "    \n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba27b8f-b66b-4122-9809-019a089580c0",
   "metadata": {},
   "source": [
    "#### Mutual Information Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb881413-3d3f-4053-a881-2f5f294d4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.045484\n",
      "Feature 1: 0.000000\n",
      "Feature 2: 0.000000\n",
      "Feature 3: 0.000000\n",
      "Feature 4: 0.024816\n",
      "Feature 5: 0.000000\n",
      "Feature 6: 0.022659\n",
      "Feature 7: 0.000000\n",
      "Feature 8: 0.000000\n",
      "Feature 9: 0.074320\n",
      "Feature 10: 0.000000\n",
      "Feature 11: 0.000000\n",
      "Feature 12: 0.000000\n",
      "Feature 13: 0.000000\n",
      "Feature 14: 0.020390\n",
      "Feature 15: 0.004307\n",
      "Feature 16: 0.000000\n",
      "Feature 17: 0.000000\n",
      "Feature 18: 0.016566\n",
      "Feature 19: 0.003688\n",
      "Feature 20: 0.007579\n",
      "Feature 21: 0.018640\n",
      "Feature 22: 0.025206\n",
      "Feature 23: 0.017967\n",
      "Feature 24: 0.069173\n",
      "Feature 25: 0.000000\n",
      "Feature 26: 0.022232\n",
      "Feature 27: 0.000000\n",
      "Feature 28: 0.007849\n",
      "Feature 29: 0.012849\n",
      "Feature 30: 0.017402\n",
      "Feature 31: 0.008083\n",
      "Feature 32: 0.047321\n",
      "Feature 33: 0.002829\n",
      "Feature 34: 0.028968\n",
      "Feature 35: 0.000000\n",
      "Feature 36: 0.071652\n",
      "Feature 37: 0.027969\n",
      "Feature 38: 0.000000\n",
      "Feature 39: 0.064796\n",
      "Feature 40: 0.137695\n",
      "Feature 41: 0.008732\n",
      "Feature 42: 0.003983\n",
      "Feature 43: 0.000000\n",
      "Feature 44: 0.009387\n",
      "Feature 45: 0.000000\n",
      "Feature 46: 0.038385\n",
      "Feature 47: 0.000000\n",
      "Feature 48: 0.000000\n",
      "Feature 49: 0.000000\n",
      "Feature 50: 0.000000\n",
      "Feature 51: 0.000000\n",
      "Feature 52: 0.000000\n",
      "Feature 53: 0.008130\n",
      "Feature 54: 0.041779\n",
      "Feature 55: 0.000000\n",
      "Feature 56: 0.000000\n",
      "Feature 57: 0.000000\n",
      "Feature 58: 0.031228\n",
      "Feature 59: 0.002689\n",
      "Feature 60: 0.146192\n",
      "Feature 61: 0.000000\n",
      "Feature 62: 0.000000\n",
      "Feature 63: 0.000000\n",
      "Feature 64: 0.018194\n",
      "Feature 65: 0.021368\n",
      "Feature 66: 0.046071\n",
      "Feature 67: 0.034707\n",
      "Feature 68: 0.033530\n",
      "Feature 69: 0.002262\n",
      "Feature 70: 0.018332\n",
      "Feature 71: 0.000000\n",
      "Feature 72: 0.000000\n",
      "Feature 73: 0.074876\n",
      "Feature 74: 0.000000\n",
      "Feature 75: 0.004429\n",
      "Feature 76: 0.002617\n",
      "Feature 77: 0.031354\n",
      "Feature 78: 0.000000\n",
      "Feature 79: 0.000000\n",
      "Feature 80: 0.000000\n",
      "Feature 81: 0.033931\n",
      "Feature 82: 0.010400\n",
      "Feature 83: 0.019373\n",
      "Feature 84: 0.000000\n",
      "Feature 85: 0.033191\n",
      "Feature 86: 0.000000\n",
      "Feature 87: 0.028745\n",
      "Feature 88: 0.000000\n",
      "Feature 89: 0.000000\n",
      "Feature 90: 0.000000\n",
      "Feature 91: 0.017698\n",
      "Feature 92: 0.129797\n",
      "Feature 93: 0.000000\n",
      "Feature 94: 0.002171\n",
      "Feature 95: 0.029995\n",
      "Feature 96: 0.000000\n",
      "Feature 97: 0.014428\n",
      "Feature 98: 0.000000\n",
      "Feature 99: 0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKA1JREFUeJzt3X9w1PWdx/FXfpANCAmVDLsCgcXKCRRMICFp0DHeuNNwzZ3GehEZTtLI4HhHTujepBKE5A/O21yFTKhkzOEMOp3KhWNOKFUuPboI1iMQSUgtYtG7qslAdwPHkEDQhMt+7w/H1S0LZNeQfLJ5Pma+0+z3+/5+9/39WMKLz/f73Y2zLMsSAACAweKHuwEAAICbIbAAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIyXONwNDIZAIKCzZ89qwoQJiouLG+52AADAAFiWpUuXLmnKlCmKj7/xHEpMBJazZ88qPT19uNsAAABR6Ojo0LRp025YExOBZcKECZK+OOGUlJRh7gYAAAxEd3e30tPTg3+P30hMBJYvLwOlpKQQWAAAGGEGcjsHN90CAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGC9xuBsAgFjlXPdmyOtPqguHqRNg5GOGBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMF1Vgqaurk9PpVHJysnJzc9Xc3Hzd2vfff1+PPvqonE6n4uLiVFtbe8NjV1dXKy4uTmvXro2mNQAAEIMiDiy7du2S2+1WVVWVWltblZGRoYKCAnV2doatv3Lliu68805VV1fL4XDc8Njvvvuu/uVf/kX33HNPpG0BAIAYFnFgqamp0apVq1RaWqq5c+eqvr5e48aN044dO8LWL1q0SC+88IIef/xx2Wy26x738uXLWr58uV5++WV961vfirQtAAAQwyIKLH19fWppaZHL5frqAPHxcrlcampq+kaNrF69WoWFhSHHvp7e3l51d3eHLAAAIHZFFFjOnz+v/v5+2e32kPV2u10+ny/qJhoaGtTa2iqPxzOgeo/Ho9TU1OCSnp4e9XsDAADzDftTQh0dHVqzZo1ee+01JScnD2ifiooKdXV1BZeOjo5b3CUAABhOiZEUp6WlKSEhQX6/P2S93++/6Q2119PS0qLOzk4tXLgwuK6/v19vv/22tm3bpt7eXiUkJITsY7PZbng/DAAAiC0RzbAkJSUpKytLXq83uC4QCMjr9SovLy+qBh588EH97ne/U1tbW3DJzs7W8uXL1dbWdk1YAQAAo09EMyyS5Ha7VVJSouzsbOXk5Ki2tlY9PT0qLS2VJK1YsUJTp04N3o/S19enU6dOBX8+c+aM2traNH78eN11112aMGGC5s2bF/Iet912myZNmnTNegAAMDpFHFiWLl2qc+fOqbKyUj6fT5mZmWpsbAzeiNve3q74+K8mbs6ePasFCxYEX2/evFmbN29Wfn6+Dh069M3PAAAAxLw4y7Ks4W7im+ru7lZqaqq6urqUkpIy3O0AxnOuezP48yfVhcPYSWz7+jhLjDXwpyL5+3vYnxICAAC4GQILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMF7icDcAAADCc657M/jzJ9WFw9jJ8GOGBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIwXVWCpq6uT0+lUcnKycnNz1dzcfN3a999/X48++qicTqfi4uJUW1t7TY3H49GiRYs0YcIETZ48WUVFRTp9+nQ0rQEAgBgUcWDZtWuX3G63qqqq1NraqoyMDBUUFKizszNs/ZUrV3TnnXequrpaDocjbM3hw4e1evVqHT16VAcOHNDVq1f1ve99Tz09PZG2BwAAYlBipDvU1NRo1apVKi0tlSTV19frzTff1I4dO7Ru3bpr6hctWqRFixZJUtjtktTY2Bjy+tVXX9XkyZPV0tKi+++/P9IWAQBAjIlohqWvr08tLS1yuVxfHSA+Xi6XS01NTYPWVFdXlyTp9ttvD7u9t7dX3d3dIQsAAIhdEQWW8+fPq7+/X3a7PWS93W6Xz+cblIYCgYDWrl2re++9V/PmzQtb4/F4lJqaGlzS09MH5b0BAICZjHtKaPXq1Tp58qQaGhquW1NRUaGurq7g0tHRMYQdAgCAoRbRPSxpaWlKSEiQ3+8PWe/3+697Q20kysrK9MYbb+jtt9/WtGnTrltns9lks9m+8fsBAICRIaIZlqSkJGVlZcnr9QbXBQIBeb1e5eXlRd2EZVkqKyvTnj17dPDgQc2cOTPqYwEAgNgT8VNCbrdbJSUlys7OVk5Ojmpra9XT0xN8amjFihWaOnWqPB6PpC9u1D116lTw5zNnzqitrU3jx4/XXXfdJemLy0A7d+7UL37xC02YMCF4P0xqaqrGjh07KCcKAABGrogDy9KlS3Xu3DlVVlbK5/MpMzNTjY2NwRtx29vbFR//1cTN2bNntWDBguDrzZs3a/PmzcrPz9ehQ4ckSS+99JIk6YEHHgh5r1deeUU//OEPI20RAADEmIgDi/TFvSZlZWVht30ZQr7kdDplWdYNj3ez7QAAYHQz7ikhAACAP0VgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4UQWWuro6OZ1OJScnKzc3V83Nzdetff/99/Xoo4/K6XQqLi5OtbW13/iYAABgdIk4sOzatUtut1tVVVVqbW1VRkaGCgoK1NnZGbb+ypUruvPOO1VdXS2HwzEoxwQAAKNLxIGlpqZGq1atUmlpqebOnav6+nqNGzdOO3bsCFu/aNEivfDCC3r88cdls9kG5ZgAAGB0iSiw9PX1qaWlRS6X66sDxMfL5XKpqakpqgaiOWZvb6+6u7tDFgAAELsiCiznz59Xf3+/7HZ7yHq73S6fzxdVA9Ec0+PxKDU1Nbikp6dH9d4AAGBkGJFPCVVUVKirqyu4dHR0DHdLAADgFkqMpDgtLU0JCQny+/0h6/1+/3VvqL0Vx7TZbNe9HwYAAMSeiGZYkpKSlJWVJa/XG1wXCATk9XqVl5cXVQO34pgAACC2RDTDIklut1slJSXKzs5WTk6Oamtr1dPTo9LSUknSihUrNHXqVHk8Hklf3FR76tSp4M9nzpxRW1ubxo8fr7vuumtAxwQAAKNbxIFl6dKlOnfunCorK+Xz+ZSZmanGxsbgTbPt7e2Kj/9q4ubs2bNasGBB8PXmzZu1efNm5efn69ChQwM6JgAAGN0iDiySVFZWprKysrDbvgwhX3I6nbIs6xsdEwAAjG4j8ikhAAAwuhBYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADBeVIGlrq5OTqdTycnJys3NVXNz8w3rd+/erdmzZys5OVnz58/X/v37Q7ZfvnxZZWVlmjZtmsaOHau5c+eqvr4+mtYAAEAMijiw7Nq1S263W1VVVWptbVVGRoYKCgrU2dkZtv7IkSNatmyZVq5cqRMnTqioqEhFRUU6efJksMbtdquxsVE///nP9cEHH2jt2rUqKyvTvn37oj8zAAAQMyIOLDU1NVq1apVKS0uDMyHjxo3Tjh07wtZv3bpVS5YsUXl5uebMmaNNmzZp4cKF2rZtW7DmyJEjKikp0QMPPCCn06mnnnpKGRkZN525AQAAo0NEgaWvr08tLS1yuVxfHSA+Xi6XS01NTWH3aWpqCqmXpIKCgpD6xYsXa9++fTpz5owsy9Jbb72lDz/8UN/73vciaQ8AAMSoxEiKz58/r/7+ftnt9pD1drtdv//978Pu4/P5wtb7fL7g6xdffFFPPfWUpk2bpsTERMXHx+vll1/W/fffH/aYvb296u3tDb7u7u6O5DQAAMAIY8RTQi+++KKOHj2qffv2qaWlRVu2bNHq1av161//Omy9x+NRampqcElPTx/ijgEAwFCKaIYlLS1NCQkJ8vv9Iev9fr8cDkfYfRwOxw3rP/vsM61fv1579uxRYWGhJOmee+5RW1ubNm/efM3lJEmqqKiQ2+0Ovu7u7ia0AAAQwyKaYUlKSlJWVpa8Xm9wXSAQkNfrVV5eXth98vLyQuol6cCBA8H6q1ev6urVq4qPD20lISFBgUAg7DFtNptSUlJCFgAAELsimmGRvngEuaSkRNnZ2crJyVFtba16enpUWloqSVqxYoWmTp0qj8cjSVqzZo3y8/O1ZcsWFRYWqqGhQcePH9f27dslSSkpKcrPz1d5ebnGjh2rGTNm6PDhw/rZz36mmpqaQTxVAAAwUkUcWJYuXapz586psrJSPp9PmZmZamxsDN5Y297eHjJbsnjxYu3cuVMbNmzQ+vXrNWvWLO3du1fz5s0L1jQ0NKiiokLLly/XhQsXNGPGDD3//PN6+umnB+EUAQDASBdxYJGksrIylZWVhd126NCha9YVFxeruLj4usdzOBx65ZVXomkFAACMAkY8JQQAAHAjBBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMaL6rFmAAC+zrnuzeDPn1QXDmMniFUElhjBLwsAQCzjkhAAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPF4rBkYQXh8HcBoxQwLAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADj8ZQQYs7Xn6SReJoGAGIBMywAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHh80i0Q4/jkXwCxgBkWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxogosdXV1cjqdSk5OVm5urpqbm29Yv3v3bs2ePVvJycmaP3++9u/ff03NBx98oIceekipqam67bbbtGjRIrW3t0fTHgAAiDERB5Zdu3bJ7XarqqpKra2tysjIUEFBgTo7O8PWHzlyRMuWLdPKlSt14sQJFRUVqaioSCdPngzW/M///I/uu+8+zZ49W4cOHdJ7772njRs3Kjk5OfozAwAAMSPiwFJTU6NVq1aptLRUc+fOVX19vcaNG6cdO3aErd+6dauWLFmi8vJyzZkzR5s2bdLChQu1bdu2YM1zzz2n73//+/rJT36iBQsW6Nvf/rYeeughTZ48OfozAwAAMSOiwNLX16eWlha5XK6vDhAfL5fLpaamprD7NDU1hdRLUkFBQbA+EAjozTff1J/92Z+poKBAkydPVm5urvbu3XvdPnp7e9Xd3R2yAACA2BVRYDl//rz6+/tlt9tD1tvtdvl8vrD7+Hy+G9Z3dnbq8uXLqq6u1pIlS/Sf//mfeuSRR/SDH/xAhw8fDntMj8ej1NTU4JKenh7JaQAAgBFm2J8SCgQCkqSHH35YP/rRj5SZmal169bpL//yL1VfXx92n4qKCnV1dQWXjo6OoWwZAAAMsYi+rTktLU0JCQny+/0h6/1+vxwOR9h9HA7HDevT0tKUmJiouXPnhtTMmTNH77zzTthj2mw22Wy2SFoHAAAjWEQzLElJScrKypLX6w2uCwQC8nq9ysvLC7tPXl5eSL0kHThwIFiflJSkRYsW6fTp0yE1H374oWbMmBFJewAAIEZFNMMiSW63WyUlJcrOzlZOTo5qa2vV09Oj0tJSSdKKFSs0depUeTweSdKaNWuUn5+vLVu2qLCwUA0NDTp+/Li2b98ePGZ5ebmWLl2q+++/X3/+53+uxsZG/fKXv9ShQ4cG5ywBAMCIFnFgWbp0qc6dO6fKykr5fD5lZmaqsbExeGNte3u74uO/mrhZvHixdu7cqQ0bNmj9+vWaNWuW9u7dq3nz5gVrHnnkEdXX18vj8eiZZ57R3XffrX//93/XfffdNwinCAAARrqIA4sklZWVqaysLOy2cLMixcXFKi4uvuExn3zyST355JPRtAMAAGLcsD8lBAAAcDMEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMlDncDwGjgXPdm8OdPqguHsRMAGJmYYQEAAMYjsAAAAONxSQgAYsDXLztKXHpE7GGGBQAAGI8ZlgHgXy4AAAwvZlgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAONFFVjq6urkdDqVnJys3NxcNTc337B+9+7dmj17tpKTkzV//nzt37//urVPP/204uLiVFtbG01rAHBDznVvhiwARoaIA8uuXbvkdrtVVVWl1tZWZWRkqKCgQJ2dnWHrjxw5omXLlmnlypU6ceKEioqKVFRUpJMnT15Tu2fPHh09elRTpkyJ/EwAAEDMijiw1NTUaNWqVSotLdXcuXNVX1+vcePGaceOHWHrt27dqiVLlqi8vFxz5szRpk2btHDhQm3bti2k7syZM/r7v/97vfbaaxozZkx0ZwMAAGJSRIGlr69PLS0tcrlcXx0gPl4ul0tNTU1h92lqagqpl6SCgoKQ+kAgoCeeeELl5eX6zne+c9M+ent71d3dHbIAAIDYFVFgOX/+vPr7+2W320PW2+12+Xy+sPv4fL6b1v/zP/+zEhMT9cwzzwyoD4/Ho9TU1OCSnp4eyWkAAIARZtifEmppadHWrVv16quvKi4ubkD7VFRUqKurK7h0dHTc4i4BAMBwiiiwpKWlKSEhQX6/P2S93++Xw+EIu4/D4bhh/W9+8xt1dnZq+vTpSkxMVGJioj799FP9wz/8g5xOZ9hj2mw2paSkhCwAACB2RRRYkpKSlJWVJa/XG1wXCATk9XqVl5cXdp+8vLyQekk6cOBAsP6JJ57Qe++9p7a2tuAyZcoUlZeX61e/+lWk5wMAAGJQYqQ7uN1ulZSUKDs7Wzk5OaqtrVVPT49KS0slSStWrNDUqVPl8XgkSWvWrFF+fr62bNmiwsJCNTQ06Pjx49q+fbskadKkSZo0aVLIe4wZM0YOh0N33333Nz0/AMPs65918kl14TB2AmAkiziwLF26VOfOnVNlZaV8Pp8yMzPV2NgYvLG2vb1d8fFfTdwsXrxYO3fu1IYNG7R+/XrNmjVLe/fu1bx58wbvLAAAQEyLOLBIUllZmcrKysJuO3To0DXriouLVVxcPODjf/LJJ9G0BQAAYtSwPyUEAABwMwQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGi+qxZgCA+fjQPsQSZlgAAIDxmGEBAMScr88uScwwxQICCwCMEvwlHjnGzBxcEgIAAMYjsAAAAOMRWAAAgPEILAAAwHjcdAsAUeBmzFuDccX1MMMCAACMR2ABAADG45IQAAAxJFYvqzHDAgAAjEdgAQAAxuOSEGAIvlkXGJli9RKMaZhhAQAAxmOGBRjB+JcdgNGCGRYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOPxWDNGFB7jBYDRicAyhPjLFgCA6HBJCAAAGI/AAgAAjEdgAQAAxiOwAAAA43HTrWG4MRcAgGsRWDAqEQwBYGSJ6pJQXV2dnE6nkpOTlZubq+bm5hvW7969W7Nnz1ZycrLmz5+v/fv3B7ddvXpVzz77rObPn6/bbrtNU6ZM0YoVK3T27NloWgMAADEo4sCya9cuud1uVVVVqbW1VRkZGSooKFBnZ2fY+iNHjmjZsmVauXKlTpw4oaKiIhUVFenkyZOSpCtXrqi1tVUbN25Ua2urXn/9dZ0+fVoPPfTQNzszAIhhznVvBhdgNIg4sNTU1GjVqlUqLS3V3LlzVV9fr3HjxmnHjh1h67du3aolS5aovLxcc+bM0aZNm7Rw4UJt27ZNkpSamqoDBw7oscce0913363vfve72rZtm1paWtTe3v7Nzg4AAMSEiO5h6evrU0tLiyoqKoLr4uPj5XK51NTUFHafpqYmud3ukHUFBQXau3fvdd+nq6tLcXFxmjhxYtjtvb296u3tDb7u7u4e+EkgItzrAQAwQUQzLOfPn1d/f7/sdnvIervdLp/PF3Yfn88XUf3nn3+uZ599VsuWLVNKSkrYGo/Ho9TU1OCSnp4eyWkAAIARxqjPYbl69aoee+wxWZall1566bp1FRUV6urqCi4dHR1D2CUAABhqEV0SSktLU0JCgvx+f8h6v98vh8MRdh+HwzGg+i/DyqeffqqDBw9ed3ZFkmw2m2w2WyStAwAQgkveI0tEMyxJSUnKysqS1+sNrgsEAvJ6vcrLywu7T15eXki9JB04cCCk/suw8tFHH+nXv/61Jk2aFElbiGFffxKCpyEAYPSK+IPj3G63SkpKlJ2drZycHNXW1qqnp0elpaWSpBUrVmjq1KnyeDySpDVr1ig/P19btmxRYWGhGhoadPz4cW3fvl3SF2Hlr//6r9Xa2qo33nhD/f39wftbbr/9diUlJQ3WuQLAiDSUMwHMOsBUEQeWpUuX6ty5c6qsrJTP51NmZqYaGxuDN9a2t7crPv6riZvFixdr586d2rBhg9avX69Zs2Zp7969mjdvniTpzJkz2rdvnyQpMzMz5L3eeustPfDAA1GeGkYLZl4AIPZF9dH8ZWVlKisrC7vt0KFD16wrLi5WcXFx2Hqn0ynLsqJpAzDC1wMT/xoFEC1mt27MqKeEAAAAwuHLDzEk+JcDAOCbYIYFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxeEoIAIBRZiQ+uckMCwAAMB6BBQAAGI9LQkAERuI0KgDEAmZYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj6eEAACjwtef8uMJv5GHGRYAAGA8AgsAADAel4QARI0pdgBDhcACgE/wBWA8LgkBAADjMcMC3EA0lzyYrTAH/y2A2EFggTH4ywUAcD1cEgIAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/PYUHE+P4YANHgdwe+CQILhg2/vMzGfx8AJuGSEAAAMB4zLMAw4GsIACAyBBYAGCRcRosMwR2RILAAwChGyBr5Rst/w6juYamrq5PT6VRycrJyc3PV3Nx8w/rdu3dr9uzZSk5O1vz587V///6Q7ZZlqbKyUnfccYfGjh0rl8uljz76KJrWAABADIo4sOzatUtut1tVVVVqbW1VRkaGCgoK1NnZGbb+yJEjWrZsmVauXKkTJ06oqKhIRUVFOnnyZLDmJz/5iX7605+qvr5ex44d02233aaCggJ9/vnn0Z8ZAACIGREHlpqaGq1atUqlpaWaO3eu6uvrNW7cOO3YsSNs/datW7VkyRKVl5drzpw52rRpkxYuXKht27ZJ+mJ2pba2Vhs2bNDDDz+se+65Rz/72c909uxZ7d279xudHAAAiA0R3cPS19enlpYWVVRUBNfFx8fL5XKpqakp7D5NTU1yu90h6woKCoJh5OOPP5bP55PL5QpuT01NVW5urpqamvT4449fc8ze3l719vYGX3d1dUmSuru7IzmdAQv0Xgl5He37DOQ40b7X1/cbzHEI10807zWQ49ysJpyB1gykn5v1PZAeB7Mm2n5utM+t7ifamltlIP+fGsw/z9H897mV7zVYNZH2OFh/niJ5/xu9V7hjf5M/P9HUDMRw/14YDl++r2VZNy+2InDmzBlLknXkyJGQ9eXl5VZOTk7YfcaMGWPt3LkzZF1dXZ01efJky7Is67/+678sSdbZs2dDaoqLi63HHnss7DGrqqosSSwsLCwsLCwxsHR0dNw0g4zIp4QqKipCZm0CgYAuXLigSZMmKS4u7pa8Z3d3t9LT09XR0aGUlJRb8h5gnIcSYz00GOehwTgPncEca8uydOnSJU2ZMuWmtREFlrS0NCUkJMjv94es9/v9cjgcYfdxOBw3rP/yf/1+v+64446QmszMzLDHtNlsstlsIesmTpwYyalELSUlhT8MQ4BxHjqM9dBgnIcG4zx0BmusU1NTB1QX0U23SUlJysrKktfrDa4LBALyer3Ky8sLu09eXl5IvSQdOHAgWD9z5kw5HI6Qmu7ubh07duy6xwQAAKNLxJeE3G63SkpKlJ2drZycHNXW1qqnp0elpaWSpBUrVmjq1KnyeDySpDVr1ig/P19btmxRYWGhGhoadPz4cW3fvl2SFBcXp7Vr1+of//EfNWvWLM2cOVMbN27UlClTVFRUNHhnCgAARqyIA8vSpUt17tw5VVZWyufzKTMzU42NjbLb7ZKk9vZ2xcd/NXGzePFi7dy5Uxs2bND69es1a9Ys7d27V/PmzQvW/PjHP1ZPT4+eeuopXbx4Uffdd58aGxuVnJw8CKc4OGw2m6qqqq65FIXBxTgPHcZ6aDDOQ4NxHjrDNdZxljWQZ4kAAACGT1QfzQ8AADCUCCwAAMB4BBYAAGA8AgsAADAegWUA6urq5HQ6lZycrNzcXDU3Nw93SyOax+PRokWLNGHCBE2ePFlFRUU6ffp0SM3nn3+u1atXa9KkSRo/frweffTRaz6AEJGprq4OfozAlxjnwXPmzBn9zd/8jSZNmqSxY8dq/vz5On78eHC7ZVmqrKzUHXfcobFjx8rlcumjjz4axo5Hnv7+fm3cuFEzZ87U2LFj9e1vf1ubNm0K+R4axjk6b7/9tv7qr/5KU6ZMUVxc3DVfPjyQcb1w4YKWL1+ulJQUTZw4UStXrtTly5cHr8mbfnj/KNfQ0GAlJSVZO3bssN5//31r1apV1sSJEy2/3z/crY1YBQUF1iuvvGKdPHnSamtrs77//e9b06dPty5fvhysefrpp6309HTL6/Vax48ft7773e9aixcvHsauR7bm5mbL6XRa99xzj7VmzZrgesZ5cFy4cMGaMWOG9cMf/tA6duyY9Yc//MH61a9+Zf33f/93sKa6utpKTU219u7da/32t7+1HnroIWvmzJnWZ599NoydjyzPP/+8NWnSJOuNN96wPv74Y2v37t3W+PHjra1btwZrGOfo7N+/33ruuees119/3ZJk7dmzJ2T7QMZ1yZIlVkZGhnX06FHrN7/5jXXXXXdZy5YtG7QeCSw3kZOTY61evTr4ur+/35oyZYrl8XiGsavY0tnZaUmyDh8+bFmWZV28eNEaM2aMtXv37mDNBx98YEmympqahqvNEevSpUvWrFmzrAMHDlj5+fnBwMI4D55nn33Wuu+++667PRAIWA6Hw3rhhReC6y5evGjZbDbrX//1X4eixZhQWFhoPfnkkyHrfvCDH1jLly+3LItxHix/GlgGMq6nTp2yJFnvvvtusOY//uM/rLi4OOvMmTOD0heXhG6gr69PLS0tcrlcwXXx8fFyuVxqamoaxs5iS1dXlyTp9ttvlyS1tLTo6tWrIeM+e/ZsTZ8+nXGPwurVq1VYWBgynhLjPJj27dun7OxsFRcXa/LkyVqwYIFefvnl4PaPP/5YPp8vZKxTU1OVm5vLWEdg8eLF8nq9+vDDDyVJv/3tb/XOO+/oL/7iLyQxzrfKQMa1qalJEydOVHZ2drDG5XIpPj5ex44dG5Q+RuS3NQ+V8+fPq7+/P/gpvl+y2+36/e9/P0xdxZZAIKC1a9fq3nvvDX76sc/nU1JS0jVfaGm32+Xz+Yahy5GroaFBra2tevfdd6/ZxjgPnj/84Q966aWX5Ha7tX79er377rt65plnlJSUpJKSkuB4hvtdwlgP3Lp169Td3a3Zs2crISFB/f39ev7557V8+XJJYpxvkYGMq8/n0+TJk0O2JyYm6vbbbx+0sSewYFitXr1aJ0+e1DvvvDPcrcScjo4OrVmzRgcOHDDqay5iUSAQUHZ2tv7pn/5JkrRgwQKdPHlS9fX1KikpGebuYse//du/6bXXXtPOnTv1ne98R21tbVq7dq2mTJnCOI8CXBK6gbS0NCUkJFzz1ITf75fD4RimrmJHWVmZ3njjDb311luaNm1acL3D4VBfX58uXrwYUs+4R6alpUWdnZ1auHChEhMTlZiYqMOHD+unP/2pEhMTZbfbGedBcscdd2ju3Lkh6+bMmaP29nZJCo4nv0u+mfLycq1bt06PP/645s+fryeeeEI/+tGPgl+2yzjfGgMZV4fDoc7OzpDt//d//6cLFy4M2tgTWG4gKSlJWVlZ8nq9wXWBQEBer1d5eXnD2NnIZlmWysrKtGfPHh08eFAzZ84M2Z6VlaUxY8aEjPvp06fV3t7OuEfgwQcf1O9+9zu1tbUFl+zsbC1fvjz4M+M8OO69995rHs3/8MMPNWPGDEnSzJkz5XA4Qsa6u7tbx44dY6wjcOXKlZAv15WkhIQEBQIBSYzzrTKQcc3Ly9PFixfV0tISrDl48KACgYByc3MHp5FBuXU3hjU0NFg2m8169dVXrVOnTllPPfWUNXHiRMvn8w13ayPW3/7t31qpqanWoUOHrD/+8Y/B5cqVK8Gap59+2po+fbp18OBB6/jx41ZeXp6Vl5c3jF3Hhq8/JWRZjPNgaW5uthITE63nn3/e+uijj6zXXnvNGjdunPXzn/88WFNdXW1NnDjR+sUvfmG999571sMPP8zjthEqKSmxpk6dGnys+fXXX7fS0tKsH//4x8Eaxjk6ly5dsk6cOGGdOHHCkmTV1NRYJ06csD799FPLsgY2rkuWLLEWLFhgHTt2zHrnnXesWbNm8VjzUHvxxRet6dOnW0lJSVZOTo519OjR4W5pRJMUdnnllVeCNZ999pn1d3/3d9a3vvUta9y4cdYjjzxi/fGPfxy+pmPEnwYWxnnw/PKXv7TmzZtn2Ww2a/bs2db27dtDtgcCAWvjxo2W3W63bDab9eCDD1qnT58epm5Hpu7ubmvNmjXW9OnTreTkZOvOO++0nnvuOau3tzdYwzhH56233gr7e7mkpMSyrIGN6//+7/9ay5Yts8aPH2+lpKRYpaWl1qVLlwatxzjL+tpHBAIAABiIe1gAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMN7/A7l+kbxlC2QbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of mutual information feature selection for numerical input data \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k='all') \n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train,  y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test) \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# feature selection\n",
    "X_train_fs,  X_test_fs,  fs  =  select_features(X_train,  y_train,  X_test) \n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)): \n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "\n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda0b72-74c7-478a-bd4c-f741e017acb8",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "09485a8b-8b57-4894-b53a-e3c63a5ba310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.086\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using all input features \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression() \n",
    "model.fit(X_train,  y_train) \n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat) \n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb8ca8-facf-4f3e-92fb-c8c3987cacd0",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using Correlation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cee62c57-c190-422a-a745-14f2fabee8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.740\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using 10 features chosen with correlation \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import f_regression \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectKBest(score_func=f_regression, k=10) \n",
    "    # learn relationship from training data \n",
    "    fs.fit(X_train,  y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) \n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression() \n",
    "model.fit(X_train_fs,  y_train) \n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs) \n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat) \n",
    "print('MAE: %.3f' % mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "78549860-4cdc-46a8-a5d7-901ecb8369bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.085\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using 88 features chosen with correlation \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import f_regression \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectKBest(score_func=f_regression, k=88) \n",
    "    # learn relationship from training data \n",
    "    fs.fit(X_train,  y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) \n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression() \n",
    "model.fit(X_train_fs,  y_train) \n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs) \n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat) \n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f55b1-6406-437c-b7ba-c9fa9b6f9f7c",
   "metadata": {},
   "source": [
    "#### Modeling With Selected Features - Model Built Using Mutual Information Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "927bc1aa-041c-4e7b-a16e-3154da2a4bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.084\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using 88 features chosen with mutual information \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k=88) \n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train,  y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data \n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) \n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_fs,  y_train) \n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat) \n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188faa71-1115-4327-8a30-30a015bb498b",
   "metadata": {},
   "source": [
    "#### Tune the Number of Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "273b7887-dca5-48a9-ba99-5cbd32d6d32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE: -0.082\n",
      "Best Config: {'sel__k': 81}\n",
      ">-1.100 with: {'sel__k': 80}\n",
      ">-0.082 with: {'sel__k': 81}\n",
      ">-0.082 with: {'sel__k': 82}\n",
      ">-0.082 with: {'sel__k': 83}\n",
      ">-0.082 with: {'sel__k': 84}\n",
      ">-0.082 with: {'sel__k': 85}\n",
      ">-0.082 with: {'sel__k': 86}\n",
      ">-0.082 with: {'sel__k': 87}\n",
      ">-0.082 with: {'sel__k': 88}\n",
      ">-0.083 with: {'sel__k': 89}\n",
      ">-0.083 with: {'sel__k': 90}\n",
      ">-0.083 with: {'sel__k': 91}\n",
      ">-0.083 with: {'sel__k': 92}\n",
      ">-0.083 with: {'sel__k': 93}\n",
      ">-0.083 with: {'sel__k': 94}\n",
      ">-0.083 with: {'sel__k': 95}\n",
      ">-0.083 with: {'sel__k': 96}\n",
      ">-0.083 with: {'sel__k': 97}\n",
      ">-0.083 with: {'sel__k': 98}\n",
      ">-0.083 with: {'sel__k': 99}\n",
      ">-0.083 with: {'sel__k': 100}\n"
     ]
    }
   ],
   "source": [
    "# compare different numbers of features selected using mutual information \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# define the evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "\n",
    "# define the pipeline to evaluate\n",
    "model = LinearRegression()\n",
    "fs = SelectKBest(score_func=mutual_info_regression) \n",
    "pipeline = Pipeline(steps=[('sel',fs), ('lr', model)]) \n",
    "\n",
    "# define the grid\n",
    "grid = dict()\n",
    "grid['sel__k'] = [i for i in range(X.shape[1]-20, X.shape[1]+1)] \n",
    "\n",
    "# define the grid search\n",
    "search = GridSearchCV(pipeline, grid, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
    "\n",
    "# perform the search \n",
    "results = search.fit(X, y) \n",
    "\n",
    "# summarize best\n",
    "print('Best MAE: %.3f' % results.best_score_) \n",
    "print('Best Config: %s' % results.best_params_) \n",
    "\n",
    "# summarize all\n",
    "means = results.cv_results_['mean_test_score'] \n",
    "params = results.cv_results_['params']\n",
    "for mean, param in zip(means, params): \n",
    "    print('>%.3f with: %r' % (mean, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7686be-e85e-4c55-b5d2-b1b2c8d4d10f",
   "metadata": {},
   "source": [
    "We might want to see the relationship between the number of selected features and MAE. In this relationship, we may expect that more features result in better performance, to a point. This relationship can be explored by manually evaluating each configuration of k for the SelectKBest from 81 to 100, gathering the sample of MAE scores, and plotting the results using box and whisker plots side by side. The spread and mean of these box plots would be expected to show any interesting relationship between the number of selected features and the MAE of the pipeline. Note that we started the spread of k values at 81 instead of 80 because the distribution of MAE scores for k=80 is dramatically larger than all other values of k considered and it washed out the plot of the results on the graph. The complete example of achieving this is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaaa361-634d-4711-a9eb-85b5a8debb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">81 -0.082 (0.006)\n",
      ">82 -0.082 (0.006)\n",
      ">83 -0.082 (0.006)\n",
      ">84 -0.082 (0.006)\n",
      ">85 -0.082 (0.006)\n"
     ]
    }
   ],
   "source": [
    "#compare different numbers of features selected using mutual information \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "\n",
    "# define number of features to evaluate\n",
    "num_features = [i for i in range(X.shape[1]-19, X.shape[1]+1)] \n",
    "\n",
    "# enumerate each number of features\n",
    "results = list()\n",
    "\n",
    "for k in num_features:\n",
    "    # create pipeline\n",
    "    model = LinearRegression()\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k=k) \n",
    "    pipeline = Pipeline(steps=[('sel',fs), ('lr', model)])\n",
    "    \n",
    "    # evaluate the model\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    results.append(scores)  \n",
    "\n",
    "    # summarize the results\n",
    "    print('>%d %.3f (%.3f)' % (k, mean(scores), std(scores))) \n",
    "    \n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=num_features, showmeans=True) \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838823c-7edd-4324-b60d-68fb721a6bd3",
   "metadata": {},
   "source": [
    "### How to Use RFE for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331f6c1-4d4c-4c0b-a559-948968855194",
   "metadata": {},
   "source": [
    "#### RFE for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848a561-90e0-49ad-9e68-e8e23ca67308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate RFE for classification \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) \n",
    "model = DecisionTreeClassifier()\n",
    "pipeline    =    Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784cd32-b4b8-4b2f-8b99-126ec5f6c616",
   "metadata": {},
   "source": [
    "We can also use the RFE model pipeline as a final model and make predictions for classifica- tion. First, the RFE and model are fit on all available data, then the predict() function can be called to make predictions on new data. The example below demonstrates this on our binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8675ee3-eb73-435d-bb39-8afc7535ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with an RFE pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) \n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "\n",
    "# fit the model on all available data \n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# make a prediction for one example\n",
    "data = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057,-2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "\n",
    "yhat = pipeline.predict(data) \n",
    "print(f'Predicted Class: {yhat[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33be3e-21a1-4a14-a428-f3bd35223245",
   "metadata": {},
   "source": [
    "#### RFE for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25408ba4-9570-4f46-82aa-1ef2ea533bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate RFE for regression \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5) \n",
    "model = DecisionTreeRegressor()\n",
    "pipeline    =    Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e440a-3bfe-4747-83dc-169f3bcc607a",
   "metadata": {},
   "source": [
    "We can also use the RFE as part of the final model and make predictions for regression. First, the Pipeline is fit on all available data, then the predict() function can be called to make predictions on new data. The example below demonstrates this on our regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283eb94-a41a-449f-8d7a-7d947f42090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a regression prediction with an RFE pipeline \n",
    "from sklearn.datasets import make_regression\n",
    "from  sklearn.feature_selection  import  RFE\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) \n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5) \n",
    "model = DecisionTreeRegressor()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\n",
    "# fit the model on all available data \n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# make a prediction for one example\n",
    "data = [[-2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381,0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "yhat = pipeline.predict(data) \n",
    "print(f'Predicted Class: {round(yhat[0],3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e454af-5f2d-44cf-847d-6e6763cf6393",
   "metadata": {},
   "source": [
    "#### RFE  Hyperparameters - Explore Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff26f91-70aa-4193-ac68-8912c228b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the number of selected features for RFE \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset \n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "    return X, y\n",
    "    \n",
    "# get a list of models to evaluate \n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(2, 10):\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i) \n",
    "        model = DecisionTreeClassifier()\n",
    "        models[str(i)]  =  Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    return scores\n",
    "    \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate \n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names =  list(),  list() \n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y) \n",
    "    results.append(scores) \n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)'  % (name, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=names, showmeans=True) \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402aae0b-8037-4c9a-b186-28f078921444",
   "metadata": {},
   "source": [
    "#### RFE  Hyperparameters - Automatically Select the Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996001e6-3343-4b81-b187-617388212adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically select the number of features for RFE \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFECV(estimator=DecisionTreeClassifier()) \n",
    "model = DecisionTreeClassifier()\n",
    "pipeline    =    Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8755eb9-9580-45a0-aea3-6b8f5d63194c",
   "metadata": {},
   "source": [
    "#### RFE  Hyperparameters - Which Features Were Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8d21e-1182-4224-8471-150d04f8ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report which features were selected by RFE \n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# define RFE\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) \n",
    "\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# summarize all features \n",
    "for i in range(X.shape[1]):\n",
    "    print('Column: %d, Selected=%s, Rank: %d' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6f762-0fee-4057-a21a-5b8c06e459ce",
   "metadata": {},
   "source": [
    "#### RFE  Hyperparameters - Explore Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809b481-80e0-4688-9412-763c6775313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the algorithm wrapped by RFE \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset \n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "    return X, y\n",
    "    \n",
    "# get a list of models to evaluate \n",
    "def get_models():\n",
    "    models = dict() \n",
    "    # lr\n",
    "    rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5) \n",
    "    model = DecisionTreeClassifier()\n",
    "    models['lr'] = Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "    \n",
    "    # perceptron\n",
    "    rfe = RFE(estimator=Perceptron(), n_features_to_select=5) \n",
    "    model = DecisionTreeClassifier()\n",
    "    models['per']  =  Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "    \n",
    "    # cart\n",
    "    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) \n",
    "    model = DecisionTreeClassifier()\n",
    "    models['cart'] = Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "    \n",
    "    # rf\n",
    "    rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5) \n",
    "    model = DecisionTreeClassifier()\n",
    "    models['rf']  =  Pipeline(steps=[('s',rfe),('m',model)]) \n",
    "    \n",
    "    # gbm\n",
    "    rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=5) \n",
    "    model = DecisionTreeClassifier()\n",
    "    models['gbm']   =   Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate \n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results \n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y) \n",
    "    results.append(scores) \n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, labels=names, showmeans=True) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b5116-297a-4197-a493-6018815e6412",
   "metadata": {},
   "source": [
    "### How to Use Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c430b4-3d58-49e0-ac02-89b3b9b383a4",
   "metadata": {},
   "source": [
    "#### Coefficients as Feature Importance - Linear Regression Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257f7ca-1faf-4da2-8a00-418308315fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression feature importance\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) \n",
    "\n",
    "# define the model\n",
    "model = LinearRegression() \n",
    "\n",
    "# fit the model \n",
    "model.fit(X, y)\n",
    "\n",
    "# get importance \n",
    "importance = model.coef_\n",
    "\n",
    "# summarize feature importance \n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v)) \n",
    "\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476a5f8-b87d-46f6-b6f5-f1f65774203b",
   "metadata": {},
   "source": [
    "#### Coefficients as Feature Importance - Logistic Regression Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7df6f-2a2b-42af-aa95-271b15c18974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression for feature importance \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# define the model\n",
    "model = LogisticRegression() \n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y) \n",
    "\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "\n",
    "# summarize feature importance \n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v)) \n",
    "\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8043f96-9c6e-41af-8a3d-3966127b5c74",
   "metadata": {},
   "source": [
    "#### Decision Tree Feature Importance - CART Regression Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0cdfc-6772-4778-aa5d-5338f2a01ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree for feature importance on a regression problem \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) \n",
    "\n",
    "# define the model\n",
    "model = DecisionTreeRegressor() \n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y) \n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_ \n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance): \n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d659f04-80d0-4b9c-9e9d-3b1805bfd595",
   "metadata": {},
   "source": [
    "#### Decision Tree Feature Importance - CART Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7460679-08ec-4177-8daa-fb8743124095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree for feature importance on a classification problem \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# define the model\n",
    "model = DecisionTreeClassifier() \n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y) \n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_ \n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance): \n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac449ab3-b17d-4240-9ad9-d86b3be5fb11",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ffaf0-0e04-4316-8545-da8a687f76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest for feature importance on a regression problem \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) \n",
    "\n",
    "# define the model\n",
    "model = RandomForestRegressor() \n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y) \n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_ \n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance): \n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e64ae8-2604-4421-aa3c-1fca9b7ed972",
   "metadata": {},
   "source": [
    "#### Random Forest Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bb32e-3b43-45c3-af6c-47efa39050b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest for feature importance on a classification problem \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# define the model\n",
    "model = RandomForestClassifier() \n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y) \n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_ \n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance): \n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f4f9a-41d5-4cd9-aa52-ee5d0798d9c8",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0461457-ca14-4465-9d64-6f3110ac37c4",
   "metadata": {},
   "source": [
    "Permutation feature importance is a technique for calculating relative importance scores that is **independent** of the model used. <br><br>First, a model is fit on the dataset, such as a model that does not support native feature importance scores. <br><br>Then the model is used to make predictions on a dataset, although the values of a feature (column) in the dataset are scrambled. <br><br>This is repeated for each feature in the dataset. <br>Then this whole process is repeated 3, 5, 10 or more times. <br>The result is a **mean importance score for each input feature** (and distribution of scores given the repeats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d67f9-ac23-4257-a8f1-2549c4595e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation feature importance with knn for regression \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.inspection import permutation_importance \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) \n",
    "\n",
    "# define the model\n",
    "model = KNeighborsRegressor() \n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# perform permutation importance\n",
    "results = permutation_importance(model, X, y, scoring='neg_mean_squared_error') \n",
    "\n",
    "# get importance\n",
    "importance = results.importances_mean \n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance): \n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def3ce1-8ec7-4daa-b4c1-df3c24e3f569",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf3dcd-e492-4d5d-9de5-e843c885dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation feature importance with knn for classification \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.inspection import permutation_importance \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# define the model\n",
    "model = KNeighborsClassifier() \n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# perform permutation importance\n",
    "results = permutation_importance(model, X, y, scoring='accuracy') \n",
    "\n",
    "# get importance\n",
    "importance = results.importances_mean \n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e487292-926b-4386-b8c2-c7aa601338c3",
   "metadata": {},
   "source": [
    "#### Feature Selection with Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfab66-19c2-42af-a980-27da73185e3b",
   "metadata": {},
   "source": [
    "Feature importance scores can be used to help interpret the data, but they can also be used directly to help rank and select features that are most useful to a predictive model. <br>We can demonstrate this with a small example. <br>Recall, our synthetic dataset has 1,000 examples each with 10 input variables, five of which are redundant and five of which are important to the outcome. <br>We can use feature importance scores to help select the five variables that are relevant and only use them as inputs to a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef266ce7-7850-495e-90ac-ea22b5488496",
   "metadata": {},
   "source": [
    "#### Feature Selection with Importance - Example of evaluating a model with all selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618c236-898e-465b-8bcc-b40fb261a9fe",
   "metadata": {},
   "source": [
    "First, we can split the training dataset into train and test sets and train a model on the training dataset, make predictions on the test set and evaluate the result using classification accuracy. <br>We will use a logistic regression model as the predictive model. <br>This **provides a baseline for comparison** when we remove some features using feature importance scores. <br>The complete example of evaluating a logistic regression model using **all features** as input on our synthetic dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39634c-2dee-4048-aa57-5e1797cfbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model using all features\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define the dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='liblinear') \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e325424d-9d18-433a-bcae-da9c6e5af56d",
   "metadata": {},
   "source": [
    "#### Feature Selection with Importance - Example of evaluating a model with feature selection performed using feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986c160-5390-48e4-b561-a2ea375ddb4c",
   "metadata": {},
   "source": [
    "Given that we created the dataset to have 5 informative features, we would expect better or the same results with half the number of input variables. <br>**We could use any of the feature importance scores explored above**, but in this case we will use the feature importance scores provided by random forest. <br>We can use the **SelectFromModel class** to define both the model we wish to calculate importance scores, RandomForestClassifier in this case, and the number of features to select, 5 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd87649-a058-47d7-bce1-31e6c22b2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation of a model using 5 features chosen with random forest importance \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectFromModel(RandomForestClassifier(n_estimators=1000), max_features=5) \n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train,  y_train)\n",
    "    # transform train input data \n",
    "    X_train_fs = fs.transform(X_train) \n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test) \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# define the dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) \n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='liblinear') \n",
    "model.fit(X_train_fs, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb5e37-afe7-40cc-9f7e-5d6f50d57532",
   "metadata": {},
   "source": [
    "In this case, we can see that the model achieves the **same performance on the dataset**, **although with half the number of input features**. <br>As expected, the feature importance scores calculated by random forest allowed us **to accurately rank the input features** and **delete those that were not relevant to the target variable**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b702ad-6f8b-44f7-b8bd-ed78daba145f",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7189c-1f58-4990-93f3-8c39a4c11683",
   "metadata": {},
   "source": [
    "### How to Scale Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2d644-587d-4381-81d9-4e6542a04453",
   "metadata": {},
   "source": [
    "#### Numerical Data Scaling Methods - Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b849a92-6731-46bb-964c-f989d37d9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a normalization \n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# define data\n",
    "data = asarray([[100, 0.001], [8, 0.05],\n",
    "[50, 0.005],\n",
    "[88, 0.07],\n",
    "[4, 0.1]])\n",
    "\n",
    "print(data)\n",
    "\n",
    "# define min max scaler \n",
    "scaler = MinMaxScaler() \n",
    "\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data) \n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bf025-b9c1-4c60-84af-23b8fe29bb76",
   "metadata": {},
   "source": [
    "#### Numerical Data Scaling Methods - Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e88a3c8-3131-43f2-9113-4de6997d30e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example of a standardization \n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# define data\n",
    "data = asarray([[100, 0.001], [8, 0.05],\n",
    "[50, 0.005],\n",
    "[88, 0.07],\n",
    "[4, 0.1]])\n",
    "\n",
    "print(data)\n",
    "\n",
    "# define standard scaler \n",
    "scaler = StandardScaler() \n",
    "\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data) \n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fd251-eeab-48ea-86e2-82541c913294",
   "metadata": {},
   "source": [
    "#### Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc474f-6964-42e7-9695-2a9c2a4aa9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the diabetes dataset \n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable \n",
    "print(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4)  for  x  in  fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8a87e-f9c5-43fe-9c26-002f13b51ed9",
   "metadata": {},
   "source": [
    "Next, letâ€™s fit and evaluate a machine learning model on the raw dataset. We will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60847810-9fb4-4621-9e1e-ce52c9715773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw diabetes dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str'))\n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier() \n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b23311-40e7-4bf2-9d65-32decb3b5670",
   "metadata": {},
   "source": [
    "#### MinMaxScaler Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fc571-b54d-452c-9f16-01e9518aac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a minmax scaler transform of the diabetes dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a robust scaler transform of the dataset \n",
    "trans = MinMaxScaler()\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# summarize \n",
    "print(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae117c0-440f-4b07-bce6-473bcfa41edc",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case, on a MinMaxScaler transform of the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bcbaa3-e4d5-4754-811a-a4d4fe1171cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with minmax scaler transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "trans = MinMaxScaler()\n",
    "model  =  KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec728e-28af-41d9-b24b-b08d00d57759",
   "metadata": {},
   "source": [
    "#### StandardScaler Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59251e1-ef29-45d6-a38a-bdd36fc2fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a standard scaler transform of the diabetes dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a robust scaler transform of the dataset\n",
    "trans = StandardScaler()\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "# summarize \n",
    "print(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fd34a-5136-47be-970c-a0341b71b74d",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case, on a StandardScaler transform of the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a917f8-928d-45ce-971a-5049ae91845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with standard scaler transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "trans = StandardScaler()\n",
    "model  =  KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a4cce-e1b9-4115-956d-1f11c43508af",
   "metadata": {},
   "source": [
    "### How to Scale Data With Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366c4ec-f24a-4844-994a-4ae8ed9bbdcf",
   "metadata": {},
   "source": [
    "#### Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ad5be-70eb-4691-bc5e-9d71bfc8e399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load and summarize the diabetes dataset \n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable \n",
    "print(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4)  for  x  in  fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16578a22-b418-48cc-8a2a-dcd1914602ec",
   "metadata": {},
   "source": [
    "Next, letâ€™s fit and evaluate a machine learning model on the raw dataset. <br>We will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. <br>The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1a6ab-5387-40ce-b77d-75f6f12c95a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw diabetes dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# load the dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str'))\n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier() \n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a5d74-63a5-442e-9659-d50a13ae4856",
   "metadata": {},
   "source": [
    "#### IQR Robust Scaler Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9acd03-ec07-460e-b365-dafe01102810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a robust scaler transform of the diabetes dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a robust scaler transform of the dataset \n",
    "trans = RobustScaler()\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data) \n",
    "\n",
    "# summarize\n",
    "print(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157a984-43cb-4377-b3da-1b141ca1cdd2",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a robust scaler transform of the dataset. <br>The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13542343-a736-4ce4-a07e-dd998f697ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with robust scaler transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "dataset = read_csv(path_diabetes, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str'))\n",
    "\n",
    "# define the pipeline\n",
    "trans = RobustScaler()\n",
    "model  =  KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b0c90-7ab1-4fa4-af9d-b819b7b35397",
   "metadata": {},
   "source": [
    "Next, letâ€™s explore the effect of different scaling ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c22a9-fb3e-4fa3-899b-759bf2d956a7",
   "metadata": {},
   "source": [
    "#### Explore Robust Scaler Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b67bc0-6a73-4ce3-a996-72e47ac6c7bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# explore the scaling range of the robust scaler transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset \n",
    "def get_dataset():\n",
    "    # load dataset\n",
    "    path_diabetes= \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "    dataset = read_csv(path_diabetes, header=None)     \n",
    "    data = dataset.values\n",
    "    # separate into input and output columns\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype('float32')\n",
    "    y = LabelEncoder().fit_transform(y.astype('str')) \n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate \n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for value in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        # define the pipeline\n",
    "        trans = RobustScaler(quantile_range=(value, 100-value)) \n",
    "        model = KNeighborsClassifier()\n",
    "        models[str(value)] = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate \n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results \n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y) \n",
    "    results.append(scores) \n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)'  % (name, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=names, showmeans=True) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38088d-52dc-4e42-8a99-98055d5b58ff",
   "metadata": {},
   "source": [
    "### How to Encode Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2bf94-b44b-4e66-bc1b-5f488efa590c",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b086c4e-4557-4d53-beb4-8adf451d296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a ordinal encoding \n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "\n",
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']]) \n",
    "print(data)\n",
    "\n",
    "# define ordinal encoding \n",
    "encoder = OrdinalEncoder() \n",
    "\n",
    "# transform data\n",
    "result = encoder.fit_transform(data) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea01e67-6785-4939-94f1-e81323f202a5",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb88889-374b-4ce7-ab89-b9f575fdd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a one hot encoding \n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)\n",
    "\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False) \n",
    "\n",
    "# transform data\n",
    "onehot = encoder.fit_transform(data) \n",
    "\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c397e6-18b1-4caf-85d0-1d0551bc0624",
   "metadata": {},
   "source": [
    "#### Dummy Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c70c0-c1ee-4e50-94c1-ba23816818da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a dummy variable encoding \n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']]) \n",
    "print(data)\n",
    "\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False) \n",
    "\n",
    "# transform data\n",
    "onehot = encoder.fit_transform(data) \n",
    "\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3855b24-72e2-4c68-a6e8-3739e09de9b2",
   "metadata": {},
   "source": [
    "#### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cecf1e-1c4e-4fed-a12c-5e3acd1b83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset \n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "path_breast_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "dataset = read_csv(path_breast_data, header=None) \n",
    "\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str) \n",
    "\n",
    "# summarize\n",
    "print('Input', X.shape) \n",
    "print('Output', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8b5a1-9c72-43a4-850e-4ef807b1a267",
   "metadata": {},
   "source": [
    "#### OrdinalEncoder Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f768698-1d8c-41d6-bd1a-3da89ec513a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal encode the breast cancer dataset \n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "\n",
    "# load the dataset\n",
    "path_breast_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "dataset = read_csv(path_breast_data, header=None) \n",
    "\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "\n",
    "# ordinal encode input variables \n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X = ordinal_encoder.fit_transform(X) \n",
    "\n",
    "# ordinal encode target variable \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y) \n",
    "\n",
    "# summarize the transformed data \n",
    "print('Input', X.shape) \n",
    "print(X[:5, :])\n",
    "\n",
    "print('Output', y.shape) \n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4955896-025a-46f4-965f-79624ca3c7ba",
   "metadata": {},
   "source": [
    "We can then fit a logistic regression algorithm on the training dataset and evaluate it on the test dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979a2af-7a6f-418e-b07e-294b39502cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate logistic regression on the breast cancer dataset with an ordinal encoding \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "path_breast_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "dataset = read_csv(path_breast_data, header=None) \n",
    "\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# ordinal encode input variables\n",
    "ordinal_encoder = OrdinalEncoder() \n",
    "ordinal_encoder.fit(X_train)\n",
    "X_train = ordinal_encoder.transform(X_train) \n",
    "X_test = ordinal_encoder.transform(X_test)\n",
    "\n",
    "# ordinal encode target variable \n",
    "label_encoder = LabelEncoder() \n",
    "label_encoder.fit(y_train)\n",
    "y_train = label_encoder.transform(y_train) \n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# define the model\n",
    "model = LogisticRegression() \n",
    "\n",
    "# fit on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat) \n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55677dfc-b9d7-4728-bf29-53788859a1bf",
   "metadata": {},
   "source": [
    "#### OneHotEncoder Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9062bd-0281-43e2-822f-286ef8dc6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the breast cancer dataset \n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# load the dataset\n",
    "path_breast_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "dataset = read_csv(path_breast_data, header=None) \n",
    "\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "\n",
    "# one hot encode input variables \n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "X = onehot_encoder.fit_transform(X) \n",
    "\n",
    "# ordinal encode target variable \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y) \n",
    "\n",
    "# summarize the transformed data \n",
    "print('Input', X.shape) \n",
    "print(X[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c5744-2c34-4911-9491-d41a35f504ef",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate machine learning on this dataset with this encoding as we did in the previous section. The encoding is fit on the training set then applied to both train and test sets as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28c6b5-1bd5-40b9-af02-1ebd0968c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate logistic regression on the breast cancer dataset with a one-hot encoding \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "path_breast_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "dataset = read_csv(path_breast_data, header=None) \n",
    "\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# one-hot encode input variables\n",
    "onehot_encoder = OneHotEncoder() \n",
    "onehot_encoder.fit(X_train)\n",
    "X_train = onehot_encoder.transform(X_train) \n",
    "X_test = onehot_encoder.transform(X_test)\n",
    "\n",
    "# ordinal encode target variable \n",
    "label_encoder = LabelEncoder() \n",
    "label_encoder.fit(y_train)\n",
    "y_train = label_encoder.transform(y_train) \n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# define the model\n",
    "model = LogisticRegression() \n",
    "\n",
    "# fit on the training set \n",
    "model.fit(X_train,  y_train) \n",
    "\n",
    "# predict on test set\n",
    "yhat = model.predict(X_test) \n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat) \n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab247668-80b6-41d9-898a-f1dbcdd7113a",
   "metadata": {},
   "source": [
    "### How to Make Distributions More Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2ce6b-4f06-406c-8c12-fec3618b0880",
   "metadata": {},
   "source": [
    "âœ… Key Takeaways (Summary) <br>\n",
    "- **Gaussian-shaped data boosts model performance.**\n",
    "- **Power transforms** help achieve Gaussian-like distributions:\n",
    "- Use **Box-Cox** for strictly positive data.\n",
    "- Use **Yeo-Johnson** for general-purpose transformation.\n",
    "- PowerTransformer in scikit-learn makes applying these easy and consistent.\n",
    "- Combine with **pipelines** for scalable, reusable data preparation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40675fd-2c5b-434b-bf25-f493e056f2a4",
   "metadata": {},
   "source": [
    "#### Power transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ec632-2bbc-40c8-8e00-31dbcbdc0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of the power transform on data with a skew \n",
    "from numpy import exp\n",
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate gaussian data sample \n",
    "data = randn(1000)\n",
    "\n",
    "# add a skew to the data distribution \n",
    "data = exp(data)\n",
    "\n",
    "# histogram of the raw data with a skew \n",
    "pyplot.hist(data,  bins=25) \n",
    "pyplot.show()\n",
    "\n",
    "# reshape data to have rows and columns \n",
    "data  =  data.reshape((len(data),1))\n",
    "\n",
    "# power transform the raw data\n",
    "power = PowerTransformer(method='yeo-johnson', standardize=True) \n",
    "data_trans = power.fit_transform(data)\n",
    "\n",
    "# histogram of the transformed data\n",
    "pyplot.hist(data_trans, bins=25) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80767936-85e5-4410-9622-eec7c7d79fcf",
   "metadata": {},
   "source": [
    "#### Sonar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99db26-526a-42e3-8eaf-91f2c5a6edf3",
   "metadata": {},
   "source": [
    "The sonar dataset is a standard machine learning dataset for binary classification. It involves 60 real-valued inputs and a two-class target variable. There are 208 examples in the dataset and the classes are reasonably balanced. A baseline classification algorithm can achieve a classification accuracy of about 53.4 percent using repeated stratified 10-fold cross-validation. Top performance on this dataset is about 88 percent using repeated stratified 10-fold cross- validation. The dataset describes sonar returns of rocks or simulated mines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ea0ec-9fb0-4974-aca3-45017e75c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the sonar dataset \n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# summarize the shape of the dataset \n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable \n",
    "display(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ffa9e-0e57-4da9-9913-c6651c060d0e",
   "metadata": {},
   "source": [
    "Next, letâ€™s fit and evaluate a machine learning model on the raw dataset. <br>We will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. <br>The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8c160-df4e-455b-bd39-372ecb1bace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw sonar dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90f47c-27d7-4208-9f08-a8035d6a1f18",
   "metadata": {},
   "source": [
    "#### Box-Cox Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909b21f-28c7-4681-b2d3-152b09dc0c0b",
   "metadata": {},
   "source": [
    "The Box-Cox transform is named for the two authors of the method. **It is a power transform that assumes the values of the input variable to which it is applied are strictly positive**. That means **0 and negative values are not supported**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca2bd6-9255-445e-b437-f81c860dd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize a box-cox transform of the sonar dataset \n",
    "# from pandas import read_csv\n",
    "# from pandas import DataFrame\n",
    "# from sklearn.preprocessing import PowerTransformer \n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# # Load dataset\n",
    "# path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "# dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# # retrieve just the numeric input values \n",
    "# data = dataset.values[:, :-1]\n",
    "\n",
    "# # perform a box-cox transform of the dataset \n",
    "# pt = PowerTransformer(method='box-cox')\n",
    "\n",
    "# # NOTE: we expect this to cause an error!!! \n",
    "# data = pt.fit_transform(data)\n",
    "\n",
    "# # convert the array back to a dataframe \n",
    "# dataset = DataFrame(data)\n",
    "\n",
    "# # histograms of the variables\n",
    "# fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "# [x.title.set_size(4) for x in fig.ravel()]\n",
    "# # show the plot \n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbc9ce-4110-49db-988b-ad5914a5f393",
   "metadata": {},
   "source": [
    "**NOTE: we expect this to cause an error!!!**"
   ]
  },
  {
   "attachments": {
    "3f44911d-0c04-49a4-b713-85a03c7cd325.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c68ce383-cda0-464c-8b6d-4a6f0f9a1db3",
   "metadata": {},
   "source": [
    "![image.png](attachment:3f44911d-0c04-49a4-b713-85a03c7cd325.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04490dc6-7ec3-492a-ad08-120c0cfefa67",
   "metadata": {},
   "source": [
    "As expected, **we cannot use the transform on the raw data because it is not strictly positive**.<br> One way to solve this problem is to use a **MixMaxScaler** transform **first** to **scale the data to positive values**, **then** apply the transform. <br>We can use a Pipeline object to apply both transforms in sequence; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e667cd5-9f40-48df-a813-638a9760c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a box-cox transform of the scaled sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# Load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# retrieve just the numeric input values \n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a box-cox transform of the dataset \n",
    "scaler = MinMaxScaler(feature_range=(1, 2)) \n",
    "power = PowerTransformer(method='box-cox')\n",
    "pipeline = Pipeline(steps=[('s', scaler),('p', power)]) \n",
    "data = pipeline.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()] \n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf9812-f59e-4134-949c-bf9bcc6ce344",
   "metadata": {},
   "source": [
    "Running the example transforms the dataset and plots histograms of each input variable. We can see that the shape of the histograms for each variable looks more Gaussian than the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42824a62-1416-4bf4-b609-91e8ef4eb330",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a Box-Cox transform of the scaled dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8f5fc-1b11-4c4e-81e7-791eab1e682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the box-cox sonar dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "scaler = MinMaxScaler(feature_range=(1, 2)) \n",
    "power = PowerTransformer(method='box-cox') \n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('s', scaler),('p', power), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88437d2c-e29d-4566-a5ae-b645a1c1f45f",
   "metadata": {},
   "source": [
    "Running the example, we can see that the Box-Cox transform results in a lift in performance from 79.7 percent accuracy without the transform to about 81.1 percent with the transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5d50d-10e7-4657-ae03-3d8958246be7",
   "metadata": {},
   "source": [
    "#### Yeo-Johnson Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ee5e8-3b95-4517-b75d-072dd1e4a62f",
   "metadata": {},
   "source": [
    "Unlike the Box-Cox transform, it does not require the values for each input variable to be strictly positive. **It supports zero values and negative values**. This means we can apply it to our dataset without scaling it first. We can apply the transform by defining a PowerTransformer object and setting the method argument to â€˜yeo-johnsonâ€™ (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a251a-7fb4-4265-b32e-c8be01fbe0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a yeo-johnson transform of the sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "\n",
    "# retrieve just the numeric input values \n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a yeo-johnson transform of the dataset \n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "data = pt.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4)  for  x  in  fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60ade5-e2b1-4f62-8a32-68b12dd67d3b",
   "metadata": {},
   "source": [
    "Running the example transforms the dataset and plots histograms of each input variable. We can see that the shape of the histograms for each variable look more Gaussian than the raw data, much like the Box-Cox transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef15b1b-4325-4711-8ed8-c85a1a967719",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a Yeo-Johnson transform of the raw dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab6091-c69a-423f-b54d-27cc6b4426f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the yeo-johnson sonar dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "power = PowerTransformer(method='yeo-johnson') \n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('p', power), ('m', model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaefd64-91f3-455c-a4d5-047f8e8d9ec6",
   "metadata": {},
   "source": [
    "Running the example, we can see that the Yeo-Johnson transform results in a lift in performance from 79.7 percent accuracy without the transform to about 80.8 percent with the transform, less than the Box-Cox transform that achieved about 81.1 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218f19d-e8e4-4337-9e51-7cf90155f934",
   "metadata": {},
   "source": [
    "Sometimes a lift in performance can be achieved by first standardizing the raw dataset prior to performing a Yeo-Johnson transform. We can explore this **by adding a StandardScaler** as a first step in the pipeline. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ce08b-e283-4845-9d35-bb4303b26088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate knn on the yeo-johnson standardized sonar dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str'))\n",
    "\n",
    "# define the pipeline\n",
    "scaler = StandardScaler()\n",
    "power = PowerTransformer(method='yeo-johnson') \n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('s', scaler), ('p', power), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844f1c5-4568-4ccb-a397-769aa4075a24",
   "metadata": {},
   "source": [
    "### How to Change Numerical Data Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8c72f-aca7-479e-be68-d347e46fade2",
   "metadata": {},
   "source": [
    "Numerical input variables may have a highly skewed or non-standard distribution. This could be caused by outliers in the data, multi-modal distributions, highly exponential distributions, and more. Many machine learning algorithms prefer or perform better when numerical input variables and even output variables in the case of regression have a standard probability distribution, such as a Gaussian (normal) or a uniform distribution.\n",
    "The quantile transform provides an automatic way to transform a numeric input variable to have a different data distribution, which in turn, can be used as input to a predictive model. In this tutorial, you will discover how to use quantile transforms to change the distribution of numeric variables for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d52b0-710c-4182-a967-9cca14d1a363",
   "metadata": {},
   "source": [
    "ðŸ§  **Core Idea**\n",
    "- Machine learning algorithms often perform better with numerical features following a standard distribution, like Gaussian (normal) or uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf59f25-7315-4960-97cc-6fb49125d9b5",
   "metadata": {},
   "source": [
    "ðŸ” **Why Change Data Distributions?**\n",
    "- Real-world data often has:  **Skewed distributions** (e.g., exponential), **Outliers**, **Multi-modal distributions**\n",
    "- Algorithms like **linear/logistic regression** assume Gaussian inputs.\n",
    "- **Even non-linear models can benefit from normalized inputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c47a4-0044-4615-9d7f-02c1d314b5f2",
   "metadata": {},
   "source": [
    "ðŸ”§ **Quantile Transform**\n",
    "- **Goal**: Map any distribution to **Gaussian** or **uniform**.\n",
    "- Uses **Quantile Function (inverse CDF)**.\n",
    "- Implemented in scikit-learn via QuantileTransformer.\n",
    "- Parameters:\n",
    "- **output_distribution**: 'normal' or 'uniform'\n",
    "- **n_quantiles**: resolution of mapping (default: 1000; must be < sample size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69555fa-199c-464f-b5e8-678b643400b8",
   "metadata": {},
   "source": [
    "âœ… **Key Takeaways**\n",
    "- Normalize data using quantile transforms for better learning.\n",
    "- Try both **normal** and **uniform** output distributions.\n",
    "- Always **experiment with n_quantiles** to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5cf06-c6a1-4f16-8052-9229e0507719",
   "metadata": {},
   "source": [
    "#### Quantile Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea7528-2859-41e9-b32b-1b9e84df9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of the quantile transform \n",
    "from numpy import exp\n",
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import QuantileTransformer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate gaussian data sample\n",
    "data = randn(1000)\n",
    "\n",
    "# add a skew to the data distribution \n",
    "data = exp(data)\n",
    "\n",
    "# histogram of the raw data with a skew \n",
    "pyplot.hist(data,  bins=25) \n",
    "pyplot.show()\n",
    "\n",
    "# reshape data to have rows and columns \n",
    "data  =  data.reshape((len(data),1))\n",
    "\n",
    "# quantile transform the raw data\n",
    "quantile = QuantileTransformer(output_distribution='normal') \n",
    "data_trans = quantile.fit_transform(data)\n",
    "\n",
    "# histogram of the transformed data \n",
    "pyplot.hist(data_trans, bins=25) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87c4ad-457c-4da3-8368-44108d0441ce",
   "metadata": {},
   "source": [
    "Running the example first creates a sample of 1,000 random Gaussian values and adds a skew to the dataset. A histogram is created from the skewed dataset and clearly shows the distribution pushed to the far left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2508283-27c3-445e-af24-8788e0346598",
   "metadata": {},
   "source": [
    "Then a QuantileTransformer is used to map the data to a Gaussian distribution and standardize the result, centering the values on the mean value of 0 and a standard deviation of 1.0. A histogram of the transform data is created showing a Gaussian shaped data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d9e94-0ece-4f6d-9068-3c569f8f5cea",
   "metadata": {},
   "source": [
    "#### Sonar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8af3e-3510-47fb-b290-4d213a9bf79f",
   "metadata": {},
   "source": [
    "The sonar dataset is a standard machine learning dataset for binary classification. It involves 60 real-valued inputs and a two-class target variable. There are 208 examples in the dataset and the classes are reasonably balanced. A baseline classification algorithm can achieve a classification accuracy of about 53.4 percent using repeated stratified 10-fold cross-validation. Top performance on this dataset is about 88 percent using repeated stratified 10-fold cross- validation. The dataset describes sonar returns of rocks or simulated mines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f786f-abf4-4ea8-9247-7597f375b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the sonar dataset \n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# summarize the shape of the dataset \n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable \n",
    "display(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa536e1-d841-4998-99f7-693a47e43b12",
   "metadata": {},
   "source": [
    "Next, letâ€™s fit and evaluate a machine learning model on the raw dataset. <br>We will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. <br>The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d3eed-e236-4285-8b3c-2613a6950971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw sonar dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00803c-b7cf-433b-98de-55a9b5d7bbce",
   "metadata": {},
   "source": [
    "#### Normal Quantile Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30cfa7-0264-4602-88f4-d948487539d8",
   "metadata": {},
   "source": [
    "It is often desirable to transform an input variable to have a normal probability distribu- tion to improve the modeling performance. We can apply the Quantile transform using the QuantileTransformer class and set the output distribution argument to â€˜normalâ€™. We must also set the n quantiles argument to a value less than the number of observations in the training dataset, in this case, 100. Once defined, we can call the fit transform() function and pass it to our dataset to create a quantile transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655f67f-208f-43f0-91a6-bd84814a8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a normal quantile transform of the sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import QuantileTransformer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# retrieve just the numeric input values \n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a normal quantile transform of the dataset\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal') \n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50f2d6-9c36-44f5-b27e-56af382942e4",
   "metadata": {},
   "source": [
    "Running the example transforms the dataset and plots histograms of each input variable. We can see that the shape of the histograms for each variable looks very Gaussian as compared to the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3713d6-f101-4714-8ccb-97bc85282f3f",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a normal quantile transform of the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6008474-a3de-40d8-9a61-626c3cab9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with normal quantile transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91285d6-9d51-4349-8129-28a7f0342c10",
   "metadata": {},
   "source": [
    "Running the example, we can see that the normal quantile transform results in a lift in performance from 79.7 percent accuracy without the transform to about 81.7 percent with the transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b1c44-0aa2-4bcb-8609-731050420661",
   "metadata": {},
   "source": [
    "#### Uniform Quantile Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942daa4-3746-4a54-8c31-9f300072a149",
   "metadata": {},
   "source": [
    "Sometimes it can be beneficial to transform a highly exponential or multi-modal distribution to have a uniform distribution. This is especially useful for data with a large and sparse range of values, e.g. outliers that are common rather than rare. We can apply the transform by defining a QuantileTransformer class and setting the output distribution argument to â€˜uniformâ€™ (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c446c-5b23-497c-a7ac-0f4145b7c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a uniform quantile transform of the sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import QuantileTransformer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# retrieve just the numeric input values \n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a uniform quantile transform of the dataset\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='uniform') \n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4)  for  x  in  fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af720-0cdf-4dde-9330-6e6d80ae617a",
   "metadata": {},
   "source": [
    "Running the example transforms the dataset and plots histograms of each input variable. We can see that the shape of the histograms for each variable looks very uniform compared to the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52af0ad-3cab-43cb-9f66-5eeec9069a9d",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a uniform quantile transform of the raw dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694724fe-927b-4402-82f9-de7b37565d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with uniform quantile transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='uniform')\n",
    "model  =  KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec833f-411c-4af7-ade6-06fb5f1f939c",
   "metadata": {},
   "source": [
    "Running the example, we can see that the uniform transform results in a lift in performance from 79.7 percent accuracy without the transform to about 84.5 percent with the transform, better than the normal transform that achieved a score of 81.7 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51218140-97ba-4b28-ba89-bfcd69480f1c",
   "metadata": {},
   "source": [
    "We chose the number of quantiles as an arbitrary number, in this case, 100. This hyperpa- rameter can be tuned to explore the effect of the resolution of the transform on the resulting skill of the model. The example below performs this experiment and plots the mean accuracy for different n quantiles values from 1 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db4301-a42d-470a-80b4-cec06611638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore number of quantiles on classification accuracy \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import QuantileTransformer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "    # load dataset\n",
    "    dataset = read_csv(filename, header=None)\n",
    "    data = dataset.values\n",
    "    # separate into input and output columns \n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype('float32')\n",
    "    y = LabelEncoder().fit_transform(y.astype('str')) \n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(1,100):\n",
    "        # define the pipeline\n",
    "        trans = QuantileTransformer(n_quantiles=i, output_distribution='uniform') \n",
    "        model = KNeighborsClassifier()\n",
    "        models[str(i)] = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    return scores\n",
    "    \n",
    "# define dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "X, y = get_dataset(path_sonar_data) \n",
    "\n",
    "# get the models to evaluate \n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results \n",
    "results = list()\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y) \n",
    "    results.append(mean(scores))\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.plot(results) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bf862-3789-45a1-a9ff-44591401d3a5",
   "metadata": {},
   "source": [
    "In this case, we can see that surprisingly smaller values resulted in better accuracy, with values such as 4 achieving an accuracy of about 85.4 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693486e-5cd2-4f3f-b3cc-8b849ffc0db6",
   "metadata": {},
   "source": [
    "A line plot is created showing the number of quantiles used in the transform versus the classification accuracy of the resulting model. We can see a bump with values less than 10 and drop and flat performance after that. <br>**The results highlight that there is likely some benefit in exploring different distributions and number of quantiles to see if better performance can be achieved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0d398-26c0-4536-8308-392f603e709c",
   "metadata": {},
   "source": [
    "### How to Transform Numerical to Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060941b5-e442-46c0-956b-38e2423e0a31",
   "metadata": {},
   "source": [
    "ðŸ”· **Why Transform Numerical to Categorical (Discretization)?**\n",
    "- Numerical variables may have **skewed or complex distributions** (e.g., outliers, exponential, multi-modal).\n",
    "- **ML models perform better** when inputs have **standard or discrete distributions**.\n",
    "- Some models (e.g., decision trees) prefer **ordinal or categorical inputs**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db413e5d-cefa-435a-a67b-6a253f32ab3c",
   "metadata": {},
   "source": [
    "ðŸ”· **What is Discretization?**\n",
    "- Converts **numeric variables into discrete ordinal categories** (a process also known as **binning**).\n",
    "- Helps **simplify relationships** and **improve performance** of models.\n",
    "- Can be applied to **inputs or outputs** (features or targets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d1ff2-5c7a-4865-9386-24aa6528f45c",
   "metadata": {},
   "source": [
    "ðŸ”· **Best Practices**\n",
    "- Always experiment with different **n_bins** values; optimal performance often requires tuning.\n",
    "- For ordinal relationships, **ordinal encoding** is preferred.\n",
    "- For capturing non-ordinal relationships (e.g., kmeans), **onehot encoding** may help.\n",
    "- Apply discretization **after train-test split** to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84820b9-eee9-49e9-9fcb-f218f7793159",
   "metadata": {},
   "source": [
    "#### Discretization Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b279f6-3440-4479-85f6-895c7a4e352e",
   "metadata": {},
   "source": [
    "Different methods for grouping the values into k discrete bins can be used; common techniques include:\n",
    "- **Uniform**: Each bin has the same width in the span of possible values for the variable.\n",
    "- **Quantile**: Each bin has the same number of values, split based on percentiles.\n",
    "- **Clustered**: Clusters are identified and examples are assigned to each group.<br><br>\n",
    "The discretization transform is available in the scikit-learn Python machine learning library via the KBinsDiscretizer class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f90f4-ecf6-4fa8-b1b3-ebbc553b3c60",
   "metadata": {},
   "source": [
    "We can demonstrate the KBinsDiscretizer with a small worked example. We can generate a sample of random Gaussian numbers. The KBinsDiscretizer can then be used to convert the floating values into fixed number of discrete categories with an ranked ordinal relationship. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0826df-0e24-4d6b-9ed7-3d3d24faa023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of the discretization transform \n",
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate gaussian data sample \n",
    "data = randn(1000)\n",
    "\n",
    "# histogram of the raw data \n",
    "pyplot.hist(data, bins=25) \n",
    "pyplot.show()\n",
    "\n",
    "# reshape data to have rows and columns \n",
    "data  =  data.reshape((len(data),1))\n",
    "\n",
    "# discretization transform the raw data\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform', subsample=200_000) \n",
    "data_trans = kbins.fit_transform(data)\n",
    "\n",
    "# summarize first few rows \n",
    "print(data_trans[:10, :])\n",
    "\n",
    "# histogram of the transformed data \n",
    "pyplot.hist(data_trans, bins=10) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769bc26-a96f-4c6e-ab74-30b21c94c223",
   "metadata": {},
   "source": [
    "Running the example first creates a sample of 1,000 random Gaussian floating-point values and plots the data as a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10463091-c18d-4a46-91fc-52fe79897ff0",
   "metadata": {},
   "source": [
    "Next the KBinsDiscretizer is used to map the numerical values to categorical values. We configure the transform to create 10 categories (0 to 9), to output the result in ordinal format (integers) and to divide the range of the input data uniformly. A sample of the transformed data is printed, clearly showing the integer format of the data as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84582717-38d7-4263-b334-909ec0d6b963",
   "metadata": {},
   "source": [
    "Finally, a histogram is created showing the 10 discrete categories and how the observations are distributed across these groups, following the same pattern as the original data with a Gaussian shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221aee19-9aeb-4b7c-879e-44b397c7c318",
   "metadata": {},
   "source": [
    "#### Sonar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d8eef-4718-4530-aacb-705347a53046",
   "metadata": {},
   "source": [
    "The sonar dataset is a standard machine learning dataset for binary classification. It involves 60 real-valued inputs and a two-class target variable. There are 208 examples in the dataset and the classes are reasonably balanced. A baseline classification algorithm can achieve a classification accuracy of about 53.4 percent using repeated stratified 10-fold cross-validation. Top performance on this dataset is about 88 percent using repeated stratified 10-fold cross- validation. The dataset describes sonar returns of rocks or simulated mines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a409a-6cbf-4035-87e7-c9f862cb6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the sonar dataset \n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# summarize the shape of the dataset \n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable \n",
    "display(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027770f-b169-4323-9250-81a187426a79",
   "metadata": {},
   "source": [
    "Next, letâ€™s fit and evaluate a machine learning model on the raw dataset. <br>We will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. <br>The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d1246-9f1c-472b-b1ea-fd7c585d263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw sonar dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47e48e-d38d-457c-95a7-5ef067fceb39",
   "metadata": {},
   "source": [
    "#### Uniform Discretization Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c319e-a83e-4955-91d0-5f03977bb9c1",
   "metadata": {},
   "source": [
    "A uniform discretization transform will preserve the probability distribution of each input variable but will make it discrete with the specified number of ordinal groups or labels. We can apply the uniform discretization transform using the KBinsDiscretizer class and setting the strategy argument to â€˜uniformâ€™. We must also set the desired number of bins set via the n bins argument; in this case, we will use 10. Once defined, we can call the fit transform() function and pass it our dataset to create a discretized transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c1544-8a34-4e02-89cc-b9bf7e93c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a uniform ordinal discretization transform of the sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# retrieve just the numeric input values \n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a uniform discretization transform of the dataset\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform',subsample=200_000) \n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4)  for  x  in  fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4fa576-9950-4dc1-a96c-c79688b93860",
   "metadata": {},
   "source": [
    "Running the example transforms the dataset and plots histograms of each input variable. We can see that the shape of the histograms generally matches the shape of the raw dataset, although in this case, each variable has a fixed number of 10 values or ordinal groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b6b5d-60c6-4b14-b037-148341b80ae4",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a uniform discretization transform of the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b53875-4060-436d-92e2-a1419ad8aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with uniform ordinal discretization transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform',subsample=200_000)\n",
    "model  =  KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38641b5e-d887-4000-b104-6a7baadf0474",
   "metadata": {},
   "source": [
    "Running the example, we can see that the uniform discretization transform results in a lift in performance from 79.7 percent accuracy without the transform to about 82.7 percent with the transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d094bb-abf1-48fd-b9a4-1a6b2db0b89b",
   "metadata": {},
   "source": [
    "#### k-Means Discretization Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ba8be-f611-49e8-b8a7-57e247d87cd2",
   "metadata": {},
   "source": [
    "A k-means discretization transform will attempt to fit k clusters for each input variable and then assign each observation to a cluster. Unless the empirical distribution of the variable is complex, the number of clusters is likely to be small, such as 3-to-5. We can apply the k-means discretization transform using the KBinsDiscretizer class and setting the strategy argument to â€˜kmeansâ€™. We must also set the desired number of bins set via the n bins argument; in this case, we will use three. Once defined, we can call the fit transform() function and pass it to our dataset to create a discretized transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a39c46-5a2f-43bb-aa82-06d4e08843e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a k-means ordinal discretization transform of the sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a k-means discretization transform of the dataset\n",
    "trans = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans',subsample=200_000) \n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0c442-2995-438f-8767-ecd5fc3f2450",
   "metadata": {},
   "source": [
    "Running the example transforms the dataset and plots histograms of each input variable. We can see that the observations for each input variable are organized into one of three groups, some of which appear to be quite even in terms of observations, and others much less so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16720393-f316-4341-8f30-4141745128ab",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a k-means discretization transform of the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d86b82-8ba7-4d3d-8d01-261c3d15fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate knn on the sonar dataset with k-means ordinal discretization transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str'))\n",
    "\n",
    "# define the pipeline\n",
    "trans = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans',subsample=200_000)\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6137923-4822-4fc4-bd18-e05ad16d51c7",
   "metadata": {},
   "source": [
    "Running the example, we can see that the k-means discretization transform results in a lift in performance from 79.7 percent accuracy without the transform to about 81.4 percent with the transform, although slightly less than the uniform distribution in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2202645-e695-4801-b790-ea9607069916",
   "metadata": {},
   "source": [
    "#### Quantile Discretization Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cb6ab-d874-4fdd-9f63-f21c72eb02fa",
   "metadata": {},
   "source": [
    "A quantile discretization transform will attempt to split the observations for each input variable into k groups, where the number of observations assigned to each group is approximately equal. Unless there are a large number of observations or a complex empirical distribution, the number of bins must be kept small, such as 5-10. We can apply the quantile discretization transform using the KBinsDiscretizer class and setting the strategy argument to â€˜quantileâ€™. We must also set the desired number of bins set via the n bins argument; in this case, we will use 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544d483-b982-42f0-abcb-f9f724895a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a quantile ordinal discretization transform of the sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "\n",
    "# retrieve just the numeric input values \n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a quantile discretization transform of the dataset\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile',subsample=200_000) \n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4)  for  x  in  fig.ravel()]\n",
    "\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a691b72-658d-40c5-94df-9ce599c6f4fc",
   "metadata": {},
   "source": [
    "Running the example transforms the dataset and plots histograms of each input variable. We can see that the histograms all show a uniform probability distribution for each input variable, where each of the 10 groups has the same number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e0fb4-0da2-46ed-9ef5-2358598217e2",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case, on a quantile discretization transform of the raw dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca370cde-8b5e-4452-8f5f-1f77892af662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with quantile ordinal discretization transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile',subsample=200_000)\n",
    "model  =  KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3992d-1005-4770-a877-66024592e535",
   "metadata": {},
   "source": [
    "Running the example, we can see that the uniform transform results in a lift in performance from 79.7 percent accuracy without the transform to about 84.0 percent with the transform, better than the uniform and k-means methods of the previous sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b32a16-76dc-44fd-aa62-cc7eca1f4ebd",
   "metadata": {},
   "source": [
    "We chose the number of bins as an arbitrary number; in this case, 10. This hyperparameter can be tuned to explore the effect of the resolution of the transform on the resulting skill of the model. The example below performs this experiment and plots the mean accuracy for different n bins values from two to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c05273-8ef6-4546-a5b9-edb8af56a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore number of discrete bins on classification accuracy \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset \n",
    "def get_dataset():\n",
    "    # load dataset\n",
    "    path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "    dataset = read_csv(path_sonar_data, header=None)\n",
    "    data = dataset.values\n",
    "    # separate into input and output columns \n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype('float32')\n",
    "    y = LabelEncoder().fit_transform(y.astype('str')) \n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(2,11):\n",
    "        # define the pipeline\n",
    "        trans = KBinsDiscretizer(n_bins=i, encode='ordinal', strategy='quantile',subsample=200_000) \n",
    "        model = KNeighborsClassifier()\n",
    "        models[str(i)] = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    return scores\n",
    "\n",
    "# get the dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate \n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results \n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y) \n",
    "    results.append(scores) \n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)'  % (name, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df00616-d6a2-4187-b60f-f4f17249c268",
   "metadata": {},
   "source": [
    "Running the example reports the mean classification accuracy for each value of the n bins\n",
    "argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d46aa-f3ee-4602-85f8-e25fafb138b5",
   "metadata": {},
   "source": [
    "In this case, we can see that surprisingly smaller values resulted in better accuracy, with values such as three achieving an accuracy of about 86.7 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c3a4d-dae4-4c88-8dab-47de663389c2",
   "metadata": {},
   "source": [
    "Box and whisker plots are created to summarize the classification accuracy scores for each number of discrete bins on the dataset. We can see a small bump in accuracy at three bins and the scores drop and remain flat for larger values. The results highlight that there is likely some benefit in exploring different numbers of discrete bins for the chosen method to see if better performance can be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065d32d-f5d6-45aa-aa66-665705151150",
   "metadata": {},
   "source": [
    "### How to Derive New Input Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd8f02-566f-4aee-be67-aea049d53f51",
   "metadata": {},
   "source": [
    "Often, the input features for a predictive modeling task interact in unexpected and often nonlinear ways. <br>These interactions can be identified and modeled by a learning algorithm. <br>Another approach is to engineer new features that expose these interactions and see if they improve model performance.<br> Additionally, transforms like raising input variables to a power can help to better expose the important relationships between input variables and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe79798-62f5-48d5-b81b-15f048fceecb",
   "metadata": {},
   "source": [
    "These features are called interaction and polynomial features and allow the use of simpler modeling algorithms as some of the complexity of interpreting the input variables and their relationships is pushed back to the data preparation stage. <br>Sometimes these features can result in improved modeling performance, although at the cost of adding thousands or even millions of additional input variables. <br>In this tutorial, you will discover how to use polynomial feature transforms for feature engineering with numerical input variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e402f6d-4dd7-49ec-a3b6-5e074ec1a441",
   "metadata": {},
   "source": [
    "After completing this tutorial, you will know:\n",
    "- Some machine learning algorithms prefer or perform better with polynomial input features.\n",
    "- How to use the polynomial features transform to create new versions of input variables for predictive modeling.\n",
    "- How the degree of the polynomial impacts the number of input features created by the transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90a370-a4f3-4633-b79b-e576cef1244c",
   "metadata": {},
   "source": [
    "ðŸ”· **Purpose of Polynomial Features**\n",
    "- Helps expose **non-linear interactions** between input variables.\n",
    "- Enables **simpler models** to capture complex relationships.\n",
    "- Often improves performance of models like **Linear Regression**, **Logistic Regression**.\n",
    "- Useful in **numerical input tasks**, especially **regression problems**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644f436-1b78-4f34-94b1-566a44efa3ac",
   "metadata": {},
   "source": [
    "ðŸ”· **Best Practices**\n",
    "- Use **low-degree** polynomials (2 or 3) unless data is very small.\n",
    "- Combine with **scaling (power/Gaussian transforms)** for better results.\n",
    "- **Visualize** accuracy results (e.g., boxplots) to detect overfitting trends.\n",
    "- Always validate with **cross-validation** due to stochastic variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579ccf3-890e-4375-93d4-6aa16240f9c4",
   "metadata": {},
   "source": [
    "#### Polynomial Feature Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd47b65-a771-4002-8386-9155ecb50e3b",
   "metadata": {},
   "source": [
    "The polynomial features transform is available in the scikit-learn Python machine learning library via the PolynomialFeatures class. <br>The features created include:\n",
    "- The bias (the value of 1.0)\n",
    "- Values raised to a power for each degree (e.g. x1, x2, x3,â€¦)\n",
    "- Interactions between all pairs of features (e.g. x1 Ã— x2, x1 Ã— x3,â€¦)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02027ec3-c1fc-4efb-b604-a8a815d89814",
   "metadata": {},
   "source": [
    "For example, with two input variables with values 2 and 3 and a degree of 2, the features created would be:\n",
    "- 1 (the bias)\n",
    "- 2^1 = 2\n",
    "- 3^1 = 3\n",
    "- 2^2 = 4\n",
    "- 3^2 = 9\n",
    "- 2 Ã— 3 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84f5b4-e30b-494d-8fb4-d12e4823a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate the types of features created \n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "\n",
    "# define the dataset\n",
    "data = asarray([[2,3],[2,3],[2,3]]) \n",
    "print(data)\n",
    "\n",
    "# perform a polynomial features transform of the dataset \n",
    "trans = PolynomialFeatures(degree=2)\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d7bf8-592a-490b-b025-39384c015b9c",
   "metadata": {},
   "source": [
    "Running the example first reports the raw data with two features (columns) and each feature has the same value, either 2 or 3. Then the polynomial features are created, resulting in six features, matching what was described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e4f8b-f6ce-4d11-b0e0-b1dce6c93f37",
   "metadata": {},
   "source": [
    "#### Sonar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a94823-e777-4a78-88b8-0ab0e096b506",
   "metadata": {},
   "source": [
    "The sonar dataset is a standard machine learning dataset for binary classification. It involves 60 real-valued inputs and a two-class target variable. There are 208 examples in the dataset and the classes are reasonably balanced. A baseline classification algorithm can achieve a classification accuracy of about 53.4 percent using repeated stratified 10-fold cross-validation. Top performance on this dataset is about 88 percent using repeated stratified 10-fold cross- validation. The dataset describes sonar returns of rocks or simulated mines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f126f20-4a6a-446b-b586-ff5619727c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the sonar dataset \n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# summarize the shape of the dataset \n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable \n",
    "display(dataset.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4) \n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df9758-31fa-4fdb-918a-ce6ab5dd97b4",
   "metadata": {},
   "source": [
    "Next, letâ€™s fit and evaluate a machine learning model on the raw dataset. <br>We will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. <br>The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7029bd0-9b01-4122-a83d-3895645fe6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw sonar dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c8f52-699b-4a48-9231-ad04ea9478e3",
   "metadata": {},
   "source": [
    "#### Polynomial Feature Transform Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab86ac9-327e-48a2-aa24-2b097c240fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a polynomial features transform of the sonar dataset \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "\n",
    "# retrieve just the numeric input values \n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a polynomial features transform of the dataset \n",
    "trans = PolynomialFeatures(degree=3)\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe \n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# summarize \n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdc573-5725-4bd6-a26d-e28fe6d3657f",
   "metadata": {},
   "source": [
    "Running the example performs the polynomial features transform on the sonar dataset. We can see that our features increased from 61 (60 input features) for the raw dataset to 39,711 features (39,710 input features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053c64c-1ec0-4d03-8caa-a6a72c2427f3",
   "metadata": {},
   "source": [
    "Next, letâ€™s evaluate the same KNN model as the previous section, but in this case on a polynomial features transform of the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001eb5e-78c0-484b-a472-60d4ac682401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with polynomial features transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(path_sonar_data, header=None) \n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns \n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y  =  LabelEncoder().fit_transform(y.astype('str')) \n",
    "\n",
    "# define the pipeline\n",
    "trans = PolynomialFeatures(degree=3) \n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8852e-7438-4a80-ac85-09233188c14e",
   "metadata": {},
   "source": [
    "Running the example, we can see that the polynomial features transform results in a lift in performance from 79.7 percent accuracy without the transform to about 80.0 percent with the transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc16329-0a9f-4f24-86d5-e3b1a9d12c01",
   "metadata": {},
   "source": [
    "#### Effect of Polynomial Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251419e5-3496-4989-9c46-3d7eaa79e8e3",
   "metadata": {},
   "source": [
    "The degree of the polynomial dramatically increases the number of input features. To get an idea of how much this impacts the number of features, we can perform the transform with a range of different degrees and compare the number of features in the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2fdfc-1532-458d-9f15-8ddfb3fba973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the effect of the degree on the number of created features \n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "    # load dataset\n",
    "    dataset = read_csv(filename, header=None) \n",
    "    data = dataset.values\n",
    "    # separate into input and output columns \n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype('float32')\n",
    "    y = LabelEncoder().fit_transform(y.astype('str')) \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "X, y = get_dataset(path_sonar_data)\n",
    "\n",
    "# calculate change in number of features \n",
    "num_features = list()\n",
    "degress = [i for i in range(1, 6)] \n",
    "\n",
    "for d in degress:\n",
    "    # create transform\n",
    "    trans = PolynomialFeatures(degree=d) \n",
    "    \n",
    "    # fit and transform\n",
    "    data = trans.fit_transform(X) \n",
    "    \n",
    "    # record number of features\n",
    "    num_features.append(data.shape[1]) \n",
    "    \n",
    "    # summarize\n",
    "    print('Degree: %d, Features: %d' % (d, data.shape[1])) \n",
    "\n",
    "# plot degree vs number of features\n",
    "pyplot.plot(degress, num_features) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df146eca-9294-4534-b45f-61d32dc19896",
   "metadata": {},
   "source": [
    "Running the example first reports the degree from 1 to 5 and the number of features in the dataset. We can see that a degree of 1 has no effect and that the number of features dramatically increases from 2 through to 5. This highlights that for anything other than very small datasets, a degree of 2 or 3 should be used to avoid a dramatic increase in input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfbda5-c38d-478f-8460-403e2f832fbb",
   "metadata": {},
   "source": [
    "More features may result in more overfitting, and in turn, worse results. It may be a good idea to treat the degree for the polynomial features transform as a hyperparameter and test different values for your dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74971c7b-21f0-40e7-9f0e-ebd0e1c294e2",
   "metadata": {},
   "source": [
    "The example below explores degree values from 1 to 4 and evaluates their effect on classification accuracy with the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b127a2b-0692-4a49-a7d6-7b8bc833f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the effect of degree on accuracy for the polynomial features transform \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline \n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "    # load dataset\n",
    "    dataset = read_csv(filename, header=None) \n",
    "    data = dataset.values\n",
    "    # separate into input and output columns \n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype('float32')\n",
    "    y = LabelEncoder().fit_transform(y.astype('str')) \n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate \n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for d in range(1,3):\n",
    "        # define the pipeline\n",
    "        trans = PolynomialFeatures(degree=d) \n",
    "        model = KNeighborsClassifier()\n",
    "        models[str(d)] = Pipeline(steps=[('t', trans), ('m', model)]) \n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "path_sonar_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "X, y = get_dataset(path_sonar_data) \n",
    "\n",
    "# get the models to evaluate \n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results \n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y) \n",
    "    results.append(scores) \n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)'  % (name, mean(scores), std(scores))) \n",
    "\n",
    "# plot model performance for comparison \n",
    "pyplot.boxplot(results, tick_labels=names, showmeans=True) \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c10bf-74ea-47a9-a812-94a04906eb02",
   "metadata": {},
   "source": [
    "**/!\\ Degrees 3 and 4 not taken into account. Too computationnally expensive for my laptop configuration !**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b7d49-adb2-47bb-9a65-90c0224af0a5",
   "metadata": {},
   "source": [
    "Running the example reports the mean classification accuracy for each polynomial degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f1450-464b-43b8-9a62-3b803ea066d0",
   "metadata": {},
   "source": [
    "In this case, we can see that performance is generally worse than no transform (degree 1) except for a degree 3. It might be interesting to explore scaling the data before or after performing the transform to see how it impacts model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c7cba-0668-4f06-9d6f-bf764d2c79b0",
   "metadata": {},
   "source": [
    "Box and whisker plots are created to summarize the classification accuracy scores for each polynomial degree. We can see that performance remains flat, perhaps with the first signs of overfitting with a degree of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a0d48-992d-4b17-b0c1-8d1a463995d0",
   "metadata": {},
   "source": [
    "## Advanced Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e124a-e1e2-47c2-b076-17e3f6cc2c72",
   "metadata": {},
   "source": [
    "### How to Transform Numerical and Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31bf60-a738-45be-95b7-4e36936645ca",
   "metadata": {},
   "source": [
    "ðŸ”§ **Why Data Transformation Is Needed**\n",
    "- Essential for preparing raw data before model training.\n",
    "- Ensures machine learning algorithms can understand and learn from the data structure.\n",
    "- Different types of data (numerical vs. categorical) require different preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0e591-9de7-4549-aa9e-dca03d5a2246",
   "metadata": {},
   "source": [
    "âš ï¸ **Challenges with Mixed Data Types**\n",
    "- Mixed-type datasets need tailored preprocessing per column type.\n",
    "- Manual separation and transformation of columns is inefficient and error-prone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ee7a2-0e05-4486-bbdb-699c7568eef9",
   "metadata": {},
   "source": [
    "ðŸ§° **Solution: ColumnTransformer (scikit-learn)**\n",
    "- Enables selective transformation of specific columns.\n",
    "- Automatically applies different transforms to numerical vs. categorical data.\n",
    "- Avoids manual column management and recombination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a2e67-f5d3-4520-8a08-381879519dc1",
   "metadata": {},
   "source": [
    "ðŸ”¨ **How to Use ColumnTransformer**\n",
    "- Define a list of transformers: Each as a tuple: (\"name\", transform_object, columns)\n",
    "- Example usage:<br>\n",
    "o\tApply **OneHotEncoder** to **categorical columns** (e.g., columns 0 & 1).<br>\n",
    "o\tApply **SimpleImputer(median)** to **numerical columns**.\n",
    "- **Remainder options**:<br>\n",
    "o\t**drop (default)**: drops unspecified columns.<br>\n",
    "o\t**passthrough**: passes through untouched columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b2656-ccce-44a8-8ca7-4a1ad41d9b12",
   "metadata": {},
   "source": [
    "ðŸ” **Integrating ColumnTransformer in a Pipeline**\n",
    "- Combine ColumnTransformer with model in a Pipeline.\n",
    "- Ensures consistent preprocessing for: **Training**, **Cross-validation**, **Future predictions**\n",
    "- Enables automation and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e00164-ed5d-4d4b-a5df-ad65504fd0f8",
   "metadata": {},
   "source": [
    "ðŸ§  **Key Takeaways for Practice**\n",
    "- Mixed-type preprocessing is streamlined with ColumnTransformer.\n",
    "- Always evaluate with cross-validation for reliable metrics.\n",
    "- Modular and reusable pipelines increase efficiency and model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb402927-e8f0-4137-a743-571b0668f7d7",
   "metadata": {},
   "source": [
    "#### Data Preparation for the Abalone Regression Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134bdde-4348-465d-aacb-bf5cd6416a83",
   "metadata": {},
   "source": [
    "The abalone dataset is a standard machine learning problem that involves predicting the age of an abalone given measurements of an abalone. The dataset has 4,177 examples, 8 input variables, and the target variable is an integer. A naive model can achieve a mean absolute error (MAE) of about 2.363 (std 0.092) by predicting the mean value, evaluated via 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cea79-0fd2-4b70-ba7b-00dff3147207",
   "metadata": {},
   "source": [
    "We can see that the first column is categorical and the remainder of the columns are numerical. We may want to one hot encode the first column and normalize the remaining numerical columns, and this can be achieved using the ColumnTransformer. We can model this as a regression predictive modeling problem with a support vector machine model (SVR). First, we need to load the dataset. We can load the dataset directly from the file using the read csv() Pandas function, then split the data into two data frames: one for input and one for the output. The complete example of loading the dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e7a7b-1e2c-49a8-b449-47ef72a1cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from pandas import read_csv \n",
    "\n",
    "# load dataset\n",
    "path_abalone_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv  \"\n",
    "dataframe = read_csv(path_abalone_data, header=None) \n",
    "\n",
    "# split into inputs and outputs\n",
    "last_ix = len(dataframe.columns) - 1\n",
    "X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix] \n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9aee0-73e2-4a78-b1e5-11ecdd750646",
   "metadata": {},
   "source": [
    "Next, we can use the **select dtypes()** function to select the column indexes that match different data types. <br>We are interested in a list of columns that are numerical columns marked as float64 or int64 in Pandas, and a list of categorical columns, marked as object or bool type in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f44e0-8317-4f13-b651-50ad65742b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using the ColumnTransformer for the Abalone dataset \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# load dataset\n",
    "path_abalone_data = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv  \"\n",
    "dataframe = read_csv(path_abalone_data, header=None) \n",
    "\n",
    "# split into inputs and outputs\n",
    "last_ix = len(dataframe.columns) - 1\n",
    "X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix] \n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# determine categorical and numerical features\n",
    "numerical_ix = X.select_dtypes(include=['int64', 'float64']).columns \n",
    "categorical_ix = X.select_dtypes(include=['object', 'bool']).columns \n",
    "\n",
    "# define the data preparation for the columns\n",
    "t = [('cat', OneHotEncoder(), categorical_ix), ('num', MinMaxScaler(), numerical_ix)] \n",
    "col_transform = ColumnTransformer(transformers=t)\n",
    "\n",
    "# define the model\n",
    "model  =  SVR(kernel='rbf',gamma='scale',C=100)\n",
    "\n",
    "# define the data preparation and modeling pipeline\n",
    "pipeline = Pipeline(steps=[('prep',col_transform), ('m', model)]) \n",
    "\n",
    "# define the model cross-validation configuration\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# evaluate the pipeline using cross validation and calculate MAE\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# convert MAE scores to positive values \n",
    "scores = absolute(scores)\n",
    "\n",
    "# summarize the model performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5c03f-de96-4705-a40d-2379b03fd9d9",
   "metadata": {},
   "source": [
    "You now have a template for using the ColumnTransformer on a dataset with mixed data types that you can use and adapt for your own projects in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59a151-98b3-4e5c-b959-490b12d7ecdc",
   "metadata": {},
   "source": [
    "### How to Transform the Target in Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62ec69-d17f-44bf-a3ac-f6f36cb35d27",
   "metadata": {},
   "source": [
    "ðŸ“Œ **Why Target Transformation Matters**\n",
    "- **Critical** for boosting model performance in **regression tasks**.\n",
    "- Transforms like scaling improve models sensitive to value range (e.g., linear models, SVMs).\n",
    "- **Often overlooked**: Target variable (output) needs scaling tooâ€”not just input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a3737-3c3a-4158-a3d7-53e0ae6e73bb",
   "metadata": {},
   "source": [
    "ðŸ“ **Importance of Data Scaling**\n",
    "- Helps algorithms that rely on:<br>\n",
    "o\tWeighted sums (Linear models, Neural Nets)<br>\n",
    "o\tDistance measures (SVM, KNN)\n",
    "- Common techniques:<br>\n",
    "o\tMinMaxScaler â†’ Normalize values to range [0,1]<br>\n",
    "o\tPowerTransformer â†’ Make data more Gaussian-like\n",
    "- Improves both input and target representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5240a-21f9-4e72-8089-a5ca864b091a",
   "metadata": {},
   "source": [
    "âš™ï¸ **Two Ways to Scale Target Variables**<br>\n",
    "1ï¸âƒ£ **Manual Scaling**\n",
    "- Requires multiple steps:\n",
    "1.\tCreate and fit scaler (e.g., MinMaxScaler).\n",
    "2.\tTransform training and test target values.\n",
    "3.\tInvert transforms for predictions.\n",
    "- âš ï¸ Downside:<br>\n",
    "o\tTedious<br>\n",
    "o\tDoesnâ€™t integrate well with cross_val_score() or pipelines.<br>\n",
    "\n",
    "2ï¸âƒ£ **Automatic Scaling with TransformedTargetRegressor**\n",
    "- Scikit-learn wrapper that:<br>\n",
    "o\tAutomatically transforms and inverse-transforms target variable.<br>\n",
    "o\tIntegrates with models and pipelines.<br>\n",
    "- Defined by: **TransformedTargetRegressor(regressor=model, transformer=scaler)**\n",
    "- âœ… Allows use of model evaluation functions like cross_val_score().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08dce1-221c-4f52-b61e-1b6bd15feedf",
   "metadata": {},
   "source": [
    "ðŸ§  **Key Takeaways**\n",
    "- Always consider transforming target variables in regression tasks.\n",
    "- Use **TransformedTargetRegressor** for:<br>\n",
    "o\tCleaner code<br>\n",
    "o\tIntegration with cross-validation<br>\n",
    "o\tAvoiding manual transform/inverse headaches<br>\n",
    "- Experiment with different transforms (MinMaxScaler, PowerTransformer) to boost accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35ef4b-f46d-4a8f-9310-eb0877b6b714",
   "metadata": {},
   "source": [
    "#### Example of Using the TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d8844-6938-4579-a459-503b748dc2e5",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the TransformedTargetRegressor on a real dataset. We will use the Boston housing regression problem that has 13 inputs and one numerical target and requires learning the relationship between suburb characteristics and house prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8288e26-5432-4746-9814-d3c596b45346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset \n",
    "from numpy import loadtxt\n",
    "\n",
    "# load data\n",
    "boston_housing = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv \"\n",
    "dataset = loadtxt(boston_housing, delimiter=\",\") \n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = dataset[:, :-1], dataset[:, -1]\n",
    "\n",
    "# summarize dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c87f1-bf3d-4b54-9967-f28995d84814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of normalizing input and output variables for regression. \n",
    "from numpy import mean\n",
    "from numpy import absolute \n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor \n",
    "\n",
    "# load data\n",
    "boston_housing = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv \"\n",
    "dataset = loadtxt(boston_housing, delimiter=\",\") \n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = dataset[:, :-1], dataset[:, -1] \n",
    "\n",
    "# prepare the model with input scaling\n",
    "pipeline = Pipeline(steps=[('normalize', MinMaxScaler()), ('model', HuberRegressor())]) \n",
    "\n",
    "# prepare the model with target scaling\n",
    "model  =  TransformedTargetRegressor(regressor=pipeline,  transformer=MinMaxScaler())\n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1) \n",
    "\n",
    "# convert scores to positive\n",
    "scores = absolute(scores) \n",
    "\n",
    "# summarize the result \n",
    "s_mean = mean(scores)\n",
    "print('Mean MAE: %.3f' % (s_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaad7e8-18ba-4a8e-9985-261537fc1a90",
   "metadata": {},
   "source": [
    "Running the example evaluates the model with normalization of the input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095710c-2eeb-4e4f-a371-6789a8852d5d",
   "metadata": {},
   "source": [
    "In this case, we achieve a MAE of about 3.2, much better than a naive model that achieved about 6.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268001cb-b92c-412b-9d16-691eb9026477",
   "metadata": {},
   "source": [
    "We are not restricted to using scaling objects; for example, we can also explore using other data transforms on the target variable, such as the PowerTransformer, that can make each variable more-Gaussian-like (using the Yeo-Johnson transform) and improve the performance of linear models (introduced in Chapter 20). By default, the PowerTransformer also performs a standardization of each variable after performing the transform. It can also help to scale the values to the range 0-1 prior to applying a power transform, to avoid problems inverting the transform. We achieve this using the MinMaxScaler and defining a positive range (introduced in Chapter 17). The complete example of using a PowerTransformer on the input and target variables of the housing dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637f6d3-b3f6-4b64-9186-8d039f121db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of power transform input and output variables for regression. \n",
    "from numpy import mean\n",
    "from numpy import absolute \n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor \n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor \n",
    "\n",
    "# load data\n",
    "boston_housing = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv \"\n",
    "dataset = loadtxt(boston_housing, delimiter=\",\") \n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = dataset[:, :-1], dataset[:, -1]\n",
    "\n",
    "# prepare the model with input scaling and power transform \n",
    "steps = list()\n",
    "steps.append(('scale', MinMaxScaler(feature_range=(1e-5,1)))) \n",
    "steps.append(('power', PowerTransformer())) \n",
    "steps.append(('model', HuberRegressor()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# prepare the model with target scaling\n",
    "model  =  TransformedTargetRegressor(regressor=pipeline,  transformer=PowerTransformer()) \n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1) \n",
    "\n",
    "# convert scores to positive\n",
    "scores = absolute(scores) \n",
    "\n",
    "# summarize the result \n",
    "s_mean = mean(scores)\n",
    "print('Mean MAE: %.3f' % (s_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b04a7-77e8-4cc0-bb17-cb1642b73448",
   "metadata": {},
   "source": [
    "Running the example evaluates the model with a power transform of the input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385651ae-d728-4513-9df8-216c61675607",
   "metadata": {},
   "source": [
    "In this case, we see further improvement to a MAE of about 2.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0399b-df40-4d78-b1e6-9805fba7363b",
   "metadata": {},
   "source": [
    "### How to Save and Load Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44856573-be43-4fc5-9c40-6536a8da87ea",
   "metadata": {},
   "source": [
    "ðŸš§ **Why It Matters**\n",
    "- âœ… **Consistency is key**: Any transform applied during training must also be applied to **test or future data.**\n",
    "- âŒ Inconsistent data preparation â†’ **invalid predictions or degraded model performance.**\n",
    "- âœ… Essential for **model deployment** and **real-world inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6ef90-18da-49bb-8d50-36220db8b963",
   "metadata": {},
   "source": [
    "âš ï¸ **Challenge: Preparing New Data**\n",
    "- Input variables can vary in **scale or units** (e.g., inches, days, miles).\n",
    "- **Models sensitive to input scale**:<br>\n",
    "o\tLogistic Regression<br>\n",
    "o\tNeural Networks<br>\n",
    "o\tKNN<br>\n",
    "- **Scaling ensures**:<br>\n",
    "o\tEqual contribution of all features.<br>\n",
    "o\tImproved model stability and convergence.<br>\n",
    "- ðŸŽ¯ Scaling must be:<br>\n",
    "o\tFit on **training data**<br>\n",
    "o\tApplied to both train and test/new data<br>\n",
    "- âœ… Easy in-memory, âŒ Tricky when model is **saved for later use**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8ce0e-688f-45ad-8f1b-776d0d7d119b",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Solution: Save Data Preparation Objects**\n",
    "- Save both:<br>\n",
    "o\tðŸ§  Trained model<br>\n",
    "o\tâš™ï¸ Data preparation object (e.g., MinMaxScaler, StandardScaler)\n",
    "- ðŸ”§ Use **pickle** (Python built-in) for serialization.\n",
    "- **Two options**:\n",
    "1.\tSave **entire object**s (easiest, most common).\n",
    "2.\tSave **parameters only** (for more control, **expert use**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791bdfc-51bf-4157-9210-f5774ecb8a48",
   "metadata": {},
   "source": [
    "ðŸ§  **Key Takeaways**\n",
    "- Always **save the exact transform** used during training.\n",
    "- Use tools like pickle to:<br>\n",
    "o\tSerialize models and scalers.<br>\n",
    "o\tLoad them later for consistent preprocessing and inference.<br>\n",
    "- ðŸ” Reuse this workflow in real-world machine learning projects to ensure **reliable and repeatable results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6deae-fbbf-4fe3-a32c-bf412f34d2d9",
   "metadata": {},
   "source": [
    "#### Define a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf96507-3eb9-4131-bfd8-0919a10907d8",
   "metadata": {},
   "source": [
    "First, we need a dataset. We will use a synthetic dataset, specifically a binary classification problem with two input variables created randomly via the make blobs() function. The example below creates a test dataset with 100 examples, two input features, and two class labels (0 and 1). The dataset is then split into training and test sets and the min and max values of each variable are then reported.\n",
    "Importantly, the random state is set when creating the dataset and when splitting the data so that the same dataset is created and the same split of data is performed each time that the code is run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b1ac9-be4e-43e8-a494-93abfff9985b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example of creating a test dataset and splitting it into train and test sets \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# summarize the scale of each input variable\n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' % (i, X_train[:, i].min(), X_train[:, i].max(),\n",
    "    X_test[:, i].min(), X_test[:, i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3052f24e-038a-4d77-9e89-442a974b0ab5",
   "metadata": {},
   "source": [
    "Running the example reports the min and max values for each variable in both the train and test datasets. We can see that each variable has a different scale, and that the scales differ between the train and test datasets. This is a realistic scenario that we may encounter with a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626478b-2850-4197-bab5-4d4f4864959b",
   "metadata": {},
   "source": [
    "#### Scale the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce80294-7629-4aa8-b273-769fa099d1b2",
   "metadata": {},
   "source": [
    "Next, we can scale the dataset. We will use the MinMaxScaler to scale each input variable to the range 0-1 (introduced in Chapter 17). The best practice use of this scaler is to fit it on the training dataset and then apply the transform to the training dataset, and other datasets: in this case, the test dataset. The complete example of scaling the data and summarizing the effects is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ddf19-203e-407e-80e1-11d0275668d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of scaling the dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit scaler on the training dataset \n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform both datasets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# summarize the scale of each input variable \n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' % (i, X_train_scaled[:, i].min(), X_train_scaled[:, i].max(),\n",
    "    X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed3eb6-49d8-4983-a85d-3f9fe38b72c1",
   "metadata": {},
   "source": [
    "Running the example prints the effect of the scaled data showing the min and max values for each variable in the train and test datasets. We can see that all variables in both datasets now have values in the desired range of 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d56e1-eb90-43af-b6ae-ad9cbe817185",
   "metadata": {},
   "source": [
    "#### Save Model and Data Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae72b22-3c14-4c09-a3c2-95bf57dde6f8",
   "metadata": {},
   "source": [
    "Next, we can fit a model on the training dataset and save both the model and the scaler object to file. We will use a LogisticRegression model because the problem is a simple binary classification task. The training dataset is scaled as before, and in this case, we will assume the test dataset is currently not available. Once scaled, the dataset is used to fit a logistic regression model. We will use the pickle framework to save the LogisticRegression model to one file, and the MinMaxScaler to another file. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21332ddf-3bcc-4d64-b3d3-9381307b06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of fitting a model on the scaled dataset \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from pickle import dump\n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit scaler on the training dataset \n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform the training dataset \n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "\n",
    "# define model\n",
    "model = LogisticRegression(solver='lbfgs') \n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# save the model\n",
    "model_path = \"../data/models/model.pkl\"\n",
    "dump(model, open(model_path, 'wb')) \n",
    "\n",
    "# save the scaler\n",
    "scaler_path = \"../data/scalers/scaler.pkl\"\n",
    "dump(scaler,  open(scaler_path,  'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea71cfe-ed42-4a85-b8e8-74c43fa5565e",
   "metadata": {},
   "source": [
    "Running the example scales the data, fits the model, and saves the model and scaler to files using pickle. <br>You should have two files in your current working directory:\n",
    "- The model object: model.pkl\n",
    "- The scaler object: scaler.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c87365-0391-4f48-aa51-087814ff59af",
   "metadata": {},
   "source": [
    "#### Load Model and Data Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6904686-173c-40df-ba43-64b70db40fb1",
   "metadata": {},
   "source": [
    "Finally, we can load the model and the scaler object and make use of them. In this case, we will assume that the training dataset is not available, and that only new data or the test dataset is available. We will load the model and the scaler, then use the scaler to prepare the new data and use the model to make predictions. Because it is a test dataset, we have the expected target values, so we will compare the predictions to the expected target values and calculate the accuracy of the model. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d47a32-3de6-460e-86f6-1eb62069694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and scaler and make predictions on new data\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from pickle import load \n",
    "\n",
    "# prepare dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) \n",
    "\n",
    "# split data into train and test sets\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.33, random_state=1) \n",
    "\n",
    "# load the model\n",
    "model_path = \"../data/models/model.pkl\"\n",
    "model = load(open(model_path, 'rb'))\n",
    "\n",
    "# load the scaler\n",
    "scaler_path = \"../data/scalers/scaler.pkl\"\n",
    "scaler = load(open(scaler_path, 'rb'))\n",
    "\n",
    "# check scale of the test set before scaling \n",
    "print('Raw test set range')\n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, min=%.3f, max=%.3f' % (i, X_test[:, i].min(), X_test[:, i].max())) \n",
    "\n",
    "# transform the test dataset\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "print('\\nScaled test set range')\n",
    "for i in range(X_test_scaled.shape[1]):\n",
    "    print('>%d, min=%.3f, max=%.3f' % (i, X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))\n",
    "    \n",
    "# make predictions on the test set \n",
    "yhat = model.predict(X_test_scaled) \n",
    "\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(y_test, yhat) \n",
    "\n",
    "print('\\nTest Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c97155-7329-4d52-97b2-3e8687c08980",
   "metadata": {},
   "source": [
    "Running the example loads the model and scaler, then uses the scaler to prepare the test dataset correctly for the model, meeting the expectations of the model when it was trained. To confirm the scaler is having the desired effect, we report the min and max value for each input feature both before and after applying the scaling. The model then makes a prediction for the examples in the test set and the classification accuracy is calculated. In this case, as expected, the data set correctly normalized the model achieved 100 percent accuracy on the test set because the test problem is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7c3e8-f646-41a3-b1ff-c6c91887aa35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50150617-fc58-4b41-994c-0b8b6b98a2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c372a3-1d72-4a76-9611-d51ed287a39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
